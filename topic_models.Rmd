---
title: "Topic models"
author: "Caspar J. van Lissa"
date: "2/10/2021"
output:
  bookdown::github_document2:
    number_sections: false
bibliography      : ["veni_sysrev.bib"]
---

```{r setup, include=FALSE}
library(yaml)
library(stringr)
library(udpipe)
library(igraph)
library(wordcloud)
library(Matrix)
library(yaml)
library(Rmpfr)
library(topicmodels)
library(udpipe)
library(slam)
library(tidytext)
library(ggplot2)
library(textrank)
source("word_functions.R")
knitr::opts_chunk$set(echo = FALSE)

# LDA analysis ------------------------------------------------------------


# Functions:
harmonicMean <- function(logLikelihoods, precision = 2000L) {
  llMed <- median(logLikelihoods)
  as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
                                       prec = precision) + llMed))))
}
BIC <- function(ll, p, n){
  -2 * ll + p * log(n)
}
entropy <- function(post_prob){
  1 + (1/(nrow(post_prob) * 
            log(ncol(post_prob)))) * (sum(rowSums(post_prob * 
                                                    log(post_prob + 1e-12))))
}

# Preprocessing -----------------------------------------------------------
df <- readRDS("study1_df.RData")

# Frequency of word by doc
nounbydoc <- df[, list(freq = .N), by = list(doc_id = doc, term = word_coded)]
# Set frequency to 1; we're not interpreting word frequency, only occurrence
nounbydoc$freq <- 1

df_lda <- nounbydoc #[nounbydoc$term %in% names(dict), ]
df_lda <- df_lda %>%
  bind_tf_idf(term, doc_id, freq)
summary(df_lda$tf_idf)
df_lda <- df_lda[order(df_lda$tf_idf),]

select_these <- df_lda[!duplicated(df_lda$term), ]
select_these <- select_these$term[select_these$tf_idf >= median(select_these$tf_idf)]
df_lda <- df_lda[df_lda$term %in% select_these, ]

dtm <- udpipe::document_term_matrix(document_term_frequencies(df_lda))

# dtm <- as.simple_triplet_matrix(dtm)
# dim(dtm)
# summary(col_sums(dtm))
# term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(dim(dtm)[1]/col_sums(dtm > 0))
# summary(term_tfidf)

yaml::write_yaml(dim(dtm), file = "Study1_lda_dims.txt")

# Build topic models

seqk <- seq(2, 20, 1)
burnin <- 1000
iter <- 1000
keep <- 50
set.seed(44773)
res_lda <- lapply(seqk, function(k) {
  topicmodels::LDA(
    dtm,
    k = k,
    method = "Gibbs",
    control = list(
      burnin = burnin,
      iter = iter,
      keep = keep
    )
  )
})

ll <- sapply(res_lda, function(x){harmonicMean(x@logLiks[-c(1:(burnin/keep))])})

K = seqk
N = nrow(dtm)
M = ncol(dtm)

parameters <- K*(M-1)+N*(K-1)
N <- nrow(dtm)
bics <- BIC(ll, parameters, N)

entropies <- sapply(res_lda, function(x){entropy(x@gamma)})

p <- ggplot(data.frame(K = seqk, Entropy = entropies), aes(x = K, y = Entropy)) + geom_path() +
  xlab('Number of topics') +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw()

ggsave("study1_entropies.png", p, device = "png")
ggsave("study1_entropies.svg", p, device = "svg")

p <- ggplot(data.frame(K = seqk, ll = ll), aes(x = K, y = ll)) + geom_path() +
  geom_vline(xintercept = (which.max(ll)+1), linetype = 2) +
  xlab('Number of topics') +
  geom_smooth(method = "lm", formula = y~log(x), se = FALSE)+
  theme_bw()

ggsave("study1_ll.png", p, device = "png")
ggsave("study1_ll.svg", p, device = "svg")

p <- ggplot(data.frame(K = seqk, BIC = bics), aes(x = K, y = BIC)) + geom_path() +
  geom_vline(xintercept = (which.min(bics)+1), linetype = 2) +
  xlab('Number of topics') +
  geom_smooth(method = "lm", formula = y~x, se = FALSE)+
  theme_bw()

ggsave("study1_BIC.png", p, device = "png")
ggsave("study1_BIC.svg", p, device = "svg")


lda_dims <- read_yaml("Study1_lda_dims.txt")
dtm_top <- read_yaml("study1_dtm_top.yml")
df <- readRDS("study1_df.RData")
number_docs_words <- yaml::read_yaml("study1_number_docs_words.txt")
```

This document describes a supplementary analysis related to the studies
described in the document `manuscript.Rmd`.

## Analysis 1: Author keywords

One important step in reviewing the literature is to examine heterogeneity of the corpus;
to analyze empirically whether there is, for example, a clear divide between psychiatric and developmental texts.
To this end, we conducted topic modeling using latent dirichlet allocation [@bleiLatentDirichletAllocation2003].
This is a clustering method for large sparse matrices.

The corpus for this first analysis consisted of author-provided keywords.
We extracted keywords by document, and applied an exclusion filter of methodological terms and similar non-substantive words.
The resulting corpus consisted of `r number_docs_words[1]` documents with `r number_docs_words[2]` unique terms.

We used the term frequency/inverse document frequency (TF-IDF) to select terms
used frequently in a document, but not used frequently in the corpus,
which could therefore be more diagnostic of subgroup membership.
Selection terms with an TF-IDF greater than the median resulted in a corpus of `r lda_dims[1]` documents and `r lda_dims[2]` terms.

We considered a range from 2-20 topics, evaluating fit based on the BIC, and interpretability based on the entropy of the posterior document/topic probabilities.
As can be seen in Figure \@ref(fig:figbic), the BICs followed a near-perfect linearly increasing trend, and the simplest model had the lowest BIC, indicating that no subcorpora could be identified.

```{r figbic, fig.cap="Analysis 1: Bayesian Information Criteria (BIC) for LDA models with 2-20 clusters."}
knitr::include_graphics("study1_BIC.png")
```
Congruently, all entropies were near-zero, as seen in Figure \@ref(fig:figent). 
Entropy reflects the separability of the extracted clusters.
The low entropies observed in this analysis indicate that the posterior document/topic probabilities were effectively uniformly distributed.
Thus, no subcorpora could be identified,
and we proceeded with an analysis of the whole sample.

```{r figent, fig.cap="Analysis 1: Entropy values for LDA models with 2-20 clusters."}
knitr::include_graphics("study1_entropies.png")
```

## Analysis 2: Abstracts

```{r, warning=FALSE, message=FALSE}
df <- readRDS("study2_df.RData")
# Functions:
harmonicMean <- function(logLikelihoods, precision = 2000L) {
  llMed <- median(logLikelihoods)
  as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
                                       prec = precision) + llMed))))
}
BIC <- function(ll, p, n){
  -2 * ll + p * log(n)
}
entropy <- function(post_prob){
  1 + (1/(nrow(post_prob) * 
            log(ncol(post_prob)))) * (sum(rowSums(post_prob * 
                                                    log(post_prob + 1e-12))))
}

# Preprocessing -----------------------------------------------------------

# Frequency of word by doc
nounbydoc <- df[df$upos %in% c("NOUN", "ADJ"), list(freq = .N), by = list(doc_id = doc_id, term = lemma)]
df_lda <- nounbydoc #[nounbydoc$term %in% names(dict), ]
df_lda <- df_lda %>%
  bind_tf_idf(term, doc_id, freq)
summary(df_lda$tf_idf)

select_these <- df_lda[!duplicated(df_lda$term), ]
select_these <- select_these$term[select_these$tf_idf >= median(select_these$tf_idf)]
df_lda <- df_lda[df_lda$term %in% select_these, ]

dtm <- udpipe::document_term_matrix(document_term_frequencies(df_lda))
yaml::write_yaml(dim(dtm), file = "Study2_lda_dims.txt")

# Build topic models
seqk <- seq(2, 20, 1)
burnin <- 1000
iter <- 1000
keep <- 50
set.seed(44773)
res_lda <- lapply(seqk, function(k) {
  topicmodels::LDA(
    dtm,
    k = k,
    method = "Gibbs",
    control = list(
      burnin = burnin,
      iter = iter,
      keep = keep
    )
  )
})

ll <- sapply(res_lda, function(x){harmonicMean(x@logLiks[-c(1:(burnin/keep))])})

K = seqk
N = nrow(dtm)
M = ncol(dtm)

parameters <- K*(M-1)+N*(K-1)
N <- nrow(dtm)
bics <- BIC(ll, parameters, N)

entropies <- sapply(res_lda, function(x){entropy(x@gamma)})

p <- ggplot(data.frame(K = seqk, Entropy = entropies), aes(x = K, y = Entropy)) + geom_path() +
  xlab('Number of topics') +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw()

ggsave("study2_entropies.png", p, device = "png")
ggsave("study2_entropies.svg", p, device = "svg")

p <- ggplot(data.frame(K = seqk, ll = ll), aes(x = K, y = ll)) + geom_path() +
  geom_vline(xintercept = (which.max(ll)+1), linetype = 2) +
  xlab('Number of topics') +
  geom_smooth(method = "lm", formula = y~log(x), se = FALSE)+
  theme_bw()

ggsave("study2_ll.png", p, device = "png")
ggsave("study2_ll.svg", p, device = "svg")

p <- ggplot(data.frame(K = seqk, BIC = bics), aes(x = K, y = BIC)) + geom_path() +
  geom_vline(xintercept = (which.min(bics)+1), linetype = 2) +
  xlab('Number of topics') +
  geom_smooth(method = "lm", formula = y~x, se = FALSE)+
  theme_bw()

ggsave("study2_BIC.png", p, device = "png")
ggsave("study2_BIC.svg", p, device = "svg")
lda_dims_2 <- read_yaml("Study2_lda_dims.txt")
```
The corpus for this second analysis consisted of the abstracts of the selected articles.
To perform feature extraction,
we first applied the natural language processing technique "part-of-speech tagging" (POS-tagging), which identifies a word's grammatical function within the sentence context.
Because our analysis sought to identify phenomena,
we retained only nouns (to capture terms like "emotion") and adjectives (to capture the "mental" in "mental health").
Retaining nouns and adjectives generally helps derive more interpretable text mining models [@martinMoreEfficientTopic2015].
Finally, we used stemming to reduce the retained terms to their root form.
The resulting corpus consisted of 15587 terms in 4414 documents.

To assess the homogeneity of the corpus of abstracts,
we again conducted topic modeling.
We selected terms with an TF-IDF greater than the median, which resulted in a corpus of `r lda_dims_2[2]` terms in `r lda_dims_2[1]` documents.

We considered a range from 2-20 topics, evaluating fit based on the BIC, and interpretability based on the entropy of the posterior document/topic probabilities.
As can be seen in Figure \@ref(fig:figbic2), the BICs again followed a near-perfect linearly increasing trend, and the simplest model had the lowest BIC, indicating that no subcorpora could be identified.

```{r figbic2, fig.cap="Analysis 2: Bayesian Information Criteria (BIC) for LDA models with 2-20 clusters."}
knitr::include_graphics("study2_BIC.png")
```

Congruently, all entropies were near-zero, as seen in Figure \@ref(fig:figent2). 
Entropy reflects the separability of the extracted clusters.
The low entropies observed in this analysis indicate that the posterior document/topic probabilities were effectively uniformly distributed.
Thus, no subcorpora could be identified,
and we proceeded with an analysis of the whole sample.

```{r figent2, fig.cap="Analysis 2: Entropy values for LDA models with 2-20 clusters."}
knitr::include_graphics("study2_entropies.png")
```

As before, the BICs followed a linearly increasing trend, and entropies were near-zero.
Thus, no subcorpora were identified,
and we proceed with a whole sample analysis.

# References
