---
title: "Compare words from feature extractor with words data frame in manuscript"
Author: "Amy van der Ham"
date: "9/29/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load in the csv with the word vector embeddings obtained from the asreview simulation 

```{r}
# load csv into object 
dict_wordvecemb <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/asreview_simulation/dict_wordvec.csv", header = FALSE)

# adjust the first column name to word
colnames(dict_wordvecemb)[1] <- "word"

# check structure of dataframe
str(dict_wordvecemb)

# remove certain characters from the column V2 which now is column of the type character and contains a string as value. 
library(tidyverse)
# create new data frame that can be used for applying adjustments
df_wordvecemb <- dict_wordvecemb

# remove the [ character from V2 
df_wordvecemb$V2 <-gsub("\\[","",as.character(df_wordvecemb$V2))

# remove the ] character from V2 
df_wordvecemb$V2 <-gsub("\\]","",as.character(df_wordvecemb$V2))

# remove the \n character from V2 
df_wordvecemb$V2 <-gsub("\\\n","",as.character(df_wordvecemb$V2))

# check if removing the characters went correctly 
df_wordvecemb[1,"V2"]
# -> yes there are now only numbers (the vectors)


# split column V2 into multiple columns
library(splitstackshape)
# separate on the space
df_wordvecemb <- cSplit(df_wordvecemb, "V2", " ")

# retain dimensions of data frame
dim(df_wordvecemb)
# -> we have 19,476 words that are defined by 40 dimensions. These dimensions define the context of the words.

# check how many unique words there are
length(unique(df_wordvecemb$word))
# -> as expected there are as many unique words as there are observations. 

# rename the column names of the data frame. 
# First column is named word and the other columns dim1-40
colnames(df_wordvecemb) <- c("word", paste0("dim", 1:40))

# check if there are any missings
summary(df_wordvecemb)
# -> no missing values. 
```

## RUN ONCE
Study 2: code below shows words that are delete from abstracts before applying 
```{r}
# load libraries
library(stringr)
library(udpipe)
library(igraph)
library(wordcloud)
library(Matrix)
library(yaml)
library(Rmpfr)
library(topicmodels)
library(udpipe)
library(slam)
library(tidytext)
library(ggplot2)
library(textrank)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
source("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/word_functions.R")
source("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/circle2.R")

recs <- data.table(read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/recs_final.csv"))
recs[, "doc" := 1:nrow(recs)]

# convert abstract column to lower case
recs$AB <- tolower(recs$AB)

# download English udpipe model
ud_model <- udpipe_download_model(language = "english")
# load the language model - NOTE that this is a more recent version than the one Caspar used. 
ud_model <- udpipe_load_model("english-ewt-ud-2.5-191206.udpipe")
ud_model <- udpipe_load_model(ud_model$file)

# apply to abstract of recs data table
udp_res <- udpipe_annotate(ud_model, x = recs$AB, doc_id = recs$doc)

# convert to data table and save as .Rdata
df <- as.data.table(udp_res)
saveRDS(df, "study2_df.RData")
  
# Keyword extraction ------------------------------------------------------

# Exclude words
# make sure to only include nouns and adjectives. 
df_kw <- df[upos %in% c("NOUN", "ADJ"), ]
# make sure to only include strings that start with a letter. (. -> and end with any character?)
df_kw <- df_kw[grepl("^[a-zA-Z].", df_kw$lemma), ]
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df_kw$lemma)))
df_kw <- df_kw[-exclude_these, ]
saveRDS(df_kw, "study2_df_kw.RData")

# Dit is een check  
# No numeric values
# all(is.na(as.numeric(df_kw$lemma)))
# df_kw$lemma[nchar(df_kw$lemma) == 3]

# textrank is used for identifying more meaningful units of analysis
# DO NOT UNDERSTAND WHY upos %in% c("NOUN", "ADJ") line is applied again here because I thought this was already used to create df_kw. 
kw_tr <- textrank_keywords(x = df_kw$lemma[df_kw$upos %in% c("NOUN", "ADJ")], ngram_max = 3, sep = " ")
  saveRDS(kw_tr, "study2_textrank.RData")

# Merge back with original data
df_kw$keyword <- txt_recode_ngram(df_kw$lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = " ")

df_kw$keyword[!df_kw$keyword %in% kw_tr$keywords$keyword] <- NA
  
# NOTE this time we do not want to apply the dictionary filter. 
df_analyze <- df_kw[!is.na(df_kw$keyword), ]
# save the 'cleaned' dataframe
saveRDS(df_analyze, "study2_analyze_without_dict_filter.RData")

####
# Also run it with applying dictionary filter. So I can look at what happens and what the differences are. 
# load dictionary 
dict <- read_yaml("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/yaml_dict.txt")
# check number of obs when dic is applied
res_cat <- cat_words(df_analyze$keyword, dict, handle_dups = "all")
  # Check coding issues
  #res_cat$dup
  #head(res_cat$unmatched)
  df_analyze_dict <- merge_df(df_analyze, res_cat$words, "word_coded")
  saveRDS(df_analyze_dict, "study2_df_analyze.RData")
  
# check if corpus consist of same number of documents and unique terms as in paper
nounbydoc <- df_analyze_dict[, list(freq = .N), by = list(doc_id = doc_id, term = word_coded)]
number_docs_words2 <- c(docs = length(unique(nounbydoc$doc_id)), words = length(unique(nounbydoc$term)))
```
## START HERE!!
Since I have already once run the code above once, I can now just call the saved R.Data files. 

```{r}
# WITHOUT DICT FILTER APPLIED
# load the pre-processed file (without dict filter)
df_paper <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_analyze_without_dict_filter.RData")

# check number of unique words and doc_id
length(unique(df_paper$keyword))
length(unique(df_paper$doc_id))
# -> 17584 unique words (or more precisely word units)

# create data frame with column doc_id term and add a term frequency per doc column column 
nounbydoc1 <- df_paper[, list(freq = .N), by = list(doc_id = doc_id, term = keyword)]

# check if that went correctly 
count(df_paper$doc_id == 3228 & df_paper$keyword == "conflict mental health")
# this gives 5 and that is also the freq give in the datafram nounbydoc1, so went correctly. 

# which terms occur in more than 6 documents
library(dplyr)
df_morethan6 <-  nounbydoc1 %>% group_by(term) %>% filter(n()>6) 

length(unique(df_morethan6$term))
length(unique(df_morethan6$doc_id))

### WITH DICT FILTER APPLIED
# load the preprocessed file (with dict filter)
df_paperdict <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_analyze.RData")

# create data frame with column doc_id term and add a term frequency per doc column column 
nounbydoc2 <- df_paperdict[, list(freq = .N), by = list(doc_id = doc_id, term = word_coded)]

# check number of unique words and number of unique documents
length(unique(nounbydoc2$doc_id))
length(unique(nounbydoc2$term))
# -> note that it does not concern unique words but unique word units. 

# which terms occur in more than 6 documents
df_mt6 <-  nounbydoc2 %>% group_by(term) %>% filter(n()>6) #
length(unique(df_mt6$term))
length(unique(df_mt6$doc_id))
```

# Create filter that can be applied on dataframe with word embeddings to decide which words to keep. 
Here I work with the dataframe on which the textrank algorithm is already applied which means I have to bring the word backs to unigrams. I have however also created a filter called include_these2 later on in which I use a dataframe on which the textrank algorithme is not applied yet. -> This makes more sense because then I do not have to bring them back to unigrams. 
```{r}
library(splitstackshape)
# Make sure that there are only unigrams
# separate on the space
df_paper2 <- cSplit(df_paper, "keyword", " ")

# create data frame with only doc_id and keyword columns
dat <- df_paper2[ , c("doc_id", "keyword_1", "keyword_2", "keyword_3")]

# create new data frame in which keyword columns are stacked
library(reshape2)
dat2 <- melt(dat, id.vars=1)
dat2

dat2 <- na.omit(dat2)
# create data frame with column doc_id term and add a term frequency per doc column column 
nounbydoc3 <- dat2[, list(freq = .N), by = list(doc_id = doc_id, term = value)]

# check number of unique words and number of unique documents unigrams 
length(unique(nounbydoc3$doc_id)) 
length(unique(nounbydoc3$term)) 

# how many times do terms occur in the dataframe
df3_mt6 <- nounbydoc3 %>% group_by(term) %>% filter(n()>6) #
length(unique(df3_mt6$term))
length(unique(df3_mt6$doc_id))

# create a dataframe with one column including the unique terms
include_these <- unique(df3_mt6$term)
include_these <- as.data.table(include_these)

saveRDS(include_these, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_these.RData")
```

# FILTER FOR CLUSTERING
Do the same as before but now only with the filter that is applied in the official manuscript until textrank. Because textrank concerns the bi - and trigrams. 

Things that are applied: 
- to lower
- udpipe ("english"): (stopwords, POS, stemming, nouns and adjectives)
- include only strings that start with a letter
- exclusion filter (removing methodological terms and similar non-substantive words)

Might want to consider not applying the final filter but fine for now I think. 


```{r}
# WITHOUT TEXTRANK ALGORITHM APPLIED
# load the pre-processed file (without textrank algorithm dict filter having been applied)
df_include <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")

# check number of unique words and doc_id
length(unique(df_include$token))
length(unique(df_include$doc_id))
# -> 16499 unique tokens

# check difference between tokens and lemma's
diff <- df_include[df_include$token != df_include$lemma, ]
# -> looking at the manuscript we want to continue with the lemma column and not the token column. 

# check number of unique words 
length(unique(df_include$lemma))
# -> 15274 unique words

# check if there are only nouns and adj in this dataframe
unique(df_include$upos)
# -> yes.

# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc4<- df_include[, list(freq = .N), by = list(doc_id = doc_id, term = lemma)]

# check number of unique words and documents. 
length(unique(nounbydoc4$doc_id)) 
length(unique(nounbydoc4$term)) 

# create a data frame with one column including the unique terms
include_these2 <- unique(nounbydoc4$term)
include_these2 <- as.data.table(include_these2)

saveRDS(include_these2, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_these2.RData")
```

# Check manuscript
Figuring out what happens with the piece of code under Keyword extraction (Exclude words till textrank) in manuscript.
```{r}
# check data frame before exclusion filter is applied
df_bf_ex <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df.RData")

# check number of unique words
length(unique(df_bf_ex$lemma))
# 25535 words 

# only select nouns and adjectives
df_ex <- df_bf_ex[upos %in% c("NOUN", "ADJ"), ]

# check number of unique words
length(unique(df_ex$lemma))
# -> 16311: 9224 unique words dropped

# makes sure that only strings that start with a letter are included. (. -> and end with any character?)
df_ex1 <- df_ex[grepl("^[a-zA-Z].", df_ex$lemma), ]

# check number of unique words
length(unique(df_ex1$lemma))
# -> 15515: 796 unique words dropped 

# excludes methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df_ex1$lemma)))
df_ex2 <- df_ex1[-exclude_these, ]

# check number of unique words
length(unique(df_ex2$lemma))
# -> 15274: 241 unique words dropped. 
```

