---
title: "Try out GloVe and Word2Vec"
Author: "Amy van der Ham"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Try to apply (https://medium.com/broadhorizon-cmotions/nlp-with-r-part-2-training-word-embedding-models-and-visualize-results-ae444043e234)[tutorial] on GloVe in R on own data -> author keywords. 

```{r}
# Loading required libraries
library(stringr)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(Matrix)
library(ggplot2)
library(dplyr)

# Required library for GloVe
library(text2vec)

# Loading script containing functions
source("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/word_functions.R")

# prepare data 
# this is were line 891 in manuscript file starts
recs <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/recs_final.csv")

# convert object into data table
recs <- as.data.table(recs)

# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]

## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})

# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column. 
df <- merge_df(recs, df, "word")

# make sure that the values in the column word do not contain any capitals.   
df[, word := tolower(word)]
  
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")

## Exclude words
# create object with the terms that should be excluded  
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")

# object with all the row numbers of author keywords that should be excluded from the data frame 
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))

# create new data frame that excludes all the row numbers that have an author keyword that should be excluded 
df <- df[!exclude_these, ]

# check the number of author keywords per doc
df_n_keywords <- df %>% 
  group_by(doc) %>% 
  summarise(n_keywords = n()) # snap nog niet helemaal hoe deze code weet dat die n() dan betrekking heeft op de column word maar gaat wel goed. 

## NOTE: some keywords contain more than one word, e.g. risk factors or follow-up studies and youth smoking behavior. So to now the lenght (number of words) per doc this won't work

# Now that the dataset is cleaned created a df in which each document is a row and the column DEclean contains the author keywords of the specific document. Shape the tokens back to their original form
df_new <- df %>% 
          group_by(word) %>% 
          group_by(doc) %>% 
          summarise(DEclean = str_c(word, collapse = " "))

# second attempt word count
df_new$DEclean_length <- sapply(df_new$DEclean, function(x){str_count(x, " ")+1})
# this gives us the number of words in the column DEclean for each row. NOTE: words that are connected by a - are seen as one word, e.g. follow-up or affect-regulatory. Also, abbreviations are also counted as a word, e.g. INFORMATION AND COMMUNICATION TECHNOLOGY (ICT). (ICT) is counted in this example. 

df_new <- data.table(df_new)

# Max length of author keywords 
max(df_new$DEclean_length)

# investigate with the highest value of number of author keywords
df_new[df_new$doc == 3667, 'DEclean']

# check if this is a result of data cleaning by looking at the original data set
recs[recs$doc == 3667, 'DE']

# -> shows that this is not a result of data cleaning and it seems that this article does really have 524 author keywords.

# check the doi of this article so I can look it up
recs[recs$doc == 3667, c('doi', 'DI', 'title', 'AU', 'PY')]
# -> online this journal contains all the keywords of the articles belonging to this journal in the past 5 years. 

# check article with 77 author keywords
recs[recs$doc == 5234, c('doi', 'DI', 'title', 'AU', 'PY', 'DE')] 
# -> looked up the article online and does not seem to have any author keywords, do not not know where the words come from. 

# check article with 33 author keywords
recs[recs$doc == 2750, c('doi', 'DI', 'title', 'AU', 'PY', 'DE')] 
# -> this is correct looked up the article and does indeed contain 33 words in the author keywords. Same is the case for the article with 30 author keywords (doc = 1186)

# NOTE: document/case/row shown below contains < in author keywords. Does this need to be cleaned? It also includes the word literature whereas this word was part of the terms that had to be excluded.
df_new[df_new$doc == 5234, 'DEclean']

# check why those words with literature were not excluded
df[df$doc == 5234, 'word']

# ->  they were not excluded because the regular expression ^literature$ tells to only delete those values that where start of the string is followed by literature followed by the end of the string. 

# check how many unique documents and how many unique words there are after excluding non substantive words. 
length(unique(df$doc))
length(unique(df$word))
```

### GloVe ###
Try to run the code of the [tutorial] (https://medium.com/broadhorizon-cmotions/nlp-with-r-part-2-training-word-embedding-models-and-visualize-results-ae444043e234) with the cleaned dataset from above. 
```{r}
# We need to tokenize our already tokenized set as input for text2vec, re-use cleaned text in df_new
it <- itoken(df_new$DEclean, 
                   tokenizer = word_tokenizer,
                   ids = df_new$doc,
                   progressbar = TRUE)

# create a vocabulary out of the tokenset (stopword removal and bi-grams are optional)
vocab <- create_vocabulary(it) # use uni-grams

# text2vec has the option to prune the vocabulary of low-frequent words
vocab <- prune_vocabulary(vocab, term_count_min = 5)
# NOTE; term_count_min is set to 5 here.

# What's in the vocabulary?
print(vocab)
```

Next, vectorize input tokens and create a Term-Count-Matrix for GloVe to handle. 
```{r}
# Vectorize word to integers
vectorizer <- vocab_vectorizer(vocab)

# Create a Term-Count-Matrix, by default it will use a skipgram window of 5 (symmetrical)
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)

# maximum number of co-occurrences to use in the weighting function, we choose the entire token set divided by 100
x_max <- length(vocab$doc_count)/100

# set up the embedding matrix and fit model
glove_model <- GloVe$new(rank = 32, x_max = x_max) 
glove_embedding = glove_model$fit_transform(tcm, n_iter = 20, convergence_tol = 0.01, n_threads = 4)
# NOTE: number of dimensions is set to 32

# combine main embedding and context embeddings (sum) into one matrix
glove_embedding = glove_embedding + t(glove_model$components) # the transpose of the context matrix
```

Now check how well Glove is doing on the author keywords
```{r}
# wat ligt er dicht bij 'school'
word <- glove_embedding["school", , drop = FALSE] 
cos_sim = sim2(x = glove_embedding, y = word, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)

# wat ligt er dicht bij 'personality'
word <- glove_embedding["personality", , drop = FALSE] 
cos_sim = sim2(x = glove_embedding, y = word, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
```

### Word2Vec ###
Prep Word2Vec, length of author keywords per article and number of unique words. 
```{r}
## count the number of words per review and plot results
df %>% 
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(1,11,1),Inf))) %>% 
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>% 

## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) + 
geom_bar(stat='identity',fill='green') + 
theme_minimal() + 
theme(axis.text.x = element_text(angle = 45, hjust=1)) + 
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)

# most articles have 2-6 author keywords -> There are also 47 articles that only have 1 author keyword

# check word frequency: shows how many of the articles contain more and less than 5 author keywords. 
df %>% 
group_by(word) %>% summarize(word_freq=n()) %>% 
mutate(min_5_freq = case_when(word_freq<5~'token frequency: <5',TRUE~'token frequency: >=5')) %>% 
group_by(min_5_freq) %>% summarise(n_tokens = n()) %>% mutate(pct_tokens = n_tokens / sum(n_tokens))

# -> more than 90% of the words appear less than 5 times in the corpus. 
```

First, vectorize text, create an index and use padding (add zeros to create equal sizes)
```{r}
# load required libraries used in the tutorial 
library(tidyverse)
library(tidytext)
library(keras)
library(uwot)

# maximum number of words for a review
max_length <- 524

# Vectorize the tokens, each token will receive a unique integer, the index of that token in a dictionary. 
# Remember, we already restricted the corpus to 37.520 unique words.
tokenizer_w2v <- text_tokenizer() %>% fit_text_tokenizer(df_new)


# and put these integers into a sequence
sequences_w2v <- texts_to_sequences(tokenizer_w2v, df_new)

# and make sure that every sequence has the same length (Keras requirement)
input_w2v <- pad_sequences(sequences_w2v, maxlen = max_length)


# show an example from the created index (word and vector)
tokenizer_w2v$word_index[200:204]
```

