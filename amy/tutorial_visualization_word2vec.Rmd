---
title: "visualization word2vec - Rebecca Barter"
Author: "Amy van der Ham"
date: "9/24/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# load in some useful libraries needed to copy tutorial 
library(knitr)
library(dplyr)
library(reshape2)
library(cluster)
library(ggplot2)
library(devtools)
# install the development version of superheat
# devtools::install_github("rlbarter/superheat")
library(superheat)
```

# load data frame with word vector embeddings
```{r}
# load data frame with word vectors into object
df <- readRDS("featurematrix_asreview.RData")

# load data frame with column with selection of words to include in analysis
df_include <- readRDS("include_these.RData")

# filter the df so that only the words that we want to be included are left
df_final <- df[df_include$include_these, ]

# drop all words that were in the include these object but not in the data frame
df_final <- na.omit(df_final)
```

Write a function for computing the pairwise cosine similarity between entries in a matrix. 
```{r}
CosineFun <- function(x, y){
  # calculate the cosine similarity between two vectors: x and y
  c <- sum(x*y) / (sqrt(sum(x * x)) * sqrt(sum(y * y)))
  return(c)
}

CosineSim <- function(X) {
  # calculate the pairwise cosine similarity between columns of the matrix X.
  # initialize similarity matrix
  m <- matrix(NA, 
              nrow = ncol(X),
              ncol = ncol(X),
              dimnames = list(colnames(X), colnames(X)))
  cos <- as.data.frame(m)
  
  # calculate the pairwise cosine similarity
  for(i in 1:ncol(X)) {
    for(j in i:ncol(X)) {
      co_rate_1 <- X[which(X[, i] & X[, j]), i]
      co_rate_2 <- X[which(X[, i] & X[, j]), j]  
      cos[i, j] <- CosineFun(co_rate_1, co_rate_2)
      # fill in the opposite diagonal entry
      cos[j, i] <- cos[i, j]        
    }
  }
  return(cos)
}
```
# Visualising word clusters for the 1231 words that I preselected (to some extend based on Caspar his pre-selection but without using the dictionary)
Define/calculate a new cluster similarity matrix that corresponds to the  1231 words. So that we can cluster words into meaningful groups. 
```{r}
# Note that t() is used to transform the data frame to a matrix
cosine_similarity <- CosineSim(t(df_final))

# check dimensions
dim(cosine_similarity)
```
We now want to cluster this cosine similarity matrix.

In order to select the number of clusters, we will choose the number that not only effectively captures distinct groupings, but also has the highest cluster stability (i.e. the same clusters are generated when using sub-samples of the data).

The function below calculates the silhouette width for every data point provided in the cosine.matrix argument based on the clusters provided by the membership argument.
```{r}
# calculate the cosine silhouette width, which in cosine land is 
# (1) the lowest average dissimilarity of the data point to any other cluster, 
#  minus
# (2) the average dissimilarity of the data point to all other data points in 
#     the same cluster
cosineSilhouette <- function(cosine.matrix, membership) {
  # Args:
  #   cosine.matrix: the cosine similarity matrix for the words
  #   membership: the named membership vector for the rows and columns. 
  #               The entries should be cluster centers and the vector 
  #               names should be the words.
  if (!is.factor(membership)) {
    stop("membership must be a factor")
  }
  # note that there are some floating point issues:
  # (some "1" entires are actually sliiightly larger than 1)
  cosine.dissim <- acos(round(cosine.matrix, 10)) / pi
  widths.list <- lapply(levels(membership), function(clust) {
    # filter rows of the similarity matrix to words in the current cluster
    # filter cols of the similarity matrix to words in the current cluster
    cosine.matrix.inside <- cosine.dissim[membership == clust, 
                                          membership == clust]
    # a: average dissimilarity of i with all other data in the same cluster
    a <- apply(cosine.matrix.inside, 1, mean)
    # filter rows of the similarity matrix to words in the current cluster
    # filter cols of the similarity matrix to words NOT in the current cluster
    other.clusters <- levels(membership)[levels(membership) != clust]
    cosine.matrix.outside <- sapply(other.clusters, function(other.clust) {
      cosine.dissim[membership == clust, membership == other.clust] %>%
        apply(1, mean) # average over clusters
    })
    # b is the lowest average dissimilarity of i to any other cluster of 
    # which i is not a member
    b <- apply(cosine.matrix.outside, 1, min)
    # silhouette width is b - a
    cosine.sil.width <- b - a
    data.frame(word = names(cosine.sil.width), width = cosine.sil.width)
  })
  widths.list <- do.call(rbind, widths.list)
  # join membership onto data.frame
  membership.df <- data.frame(word = names(membership), 
                              membership = membership)
  widths.list <- left_join(widths.list, membership.df, by = "word")
  return(widths.list)
}
```
Using the cosineSilhouette() function to calculating the average cosine silhouette width for each k, we can plot k versus average cosine silhouette width across all observations for each number of clusters, k.

```{r}
set.seed(238942)
# calculate the average silhouette width for k=5, ..., 20
sil.width <- sapply(5:20, function(k) {
  # generate k clusters
  membership <- pam(cosine_similarity, k = k)
  # calcualte the silhouette width for each observation
  width <- cosineSilhouette(cosine_similarity, 
                   membership = factor(membership$clustering))$width
  return(mean(width))
})

# plot k verus silhouette width
data.frame(k = 5:20, width = sil.width) %>%
  ggplot(aes(x = k, y = width)) +
  geom_line() + 
  geom_point() +
  scale_y_continuous(name = "Avergae silhouette width")
```
Based on the plot the best k seems to be 20. 

# Jaccard Similarity
Next, for each set of cluster membership pairs (where each membership vector is calculated based on a 90% sub-sample of the data), we want to calculate the Jaccard similarity of the membership vectors.

Since each membership iteration corresponds to a 90% sub-sample, we ignore words that are missing from either of the iterations.

The generateClusters() function generates k clusters (where we range k over some set of values such as 5 to 20). We do this N times, each time taking a subset of 90% of the data.

Based on these N iterations of clusters we will evaluate both performance and stability and select a number of clusters based on these criterion.
```{r}
library(cluster)

# perform clustering for k in k.range clusters over N 90% sub-samples 
generateClusters <- function(similarity.mat, k.range, N) {
  random.subset.list <- lapply(1:100, function(i) {
    sample(1:nrow(similarity.mat), 0.9 * nrow(similarity.mat))
    })
  lapply(k.range, function(k) {
    print(paste("k =", k))
    lapply(1:N, function(i) {
      # randomly sample 90% of words
      cosine.sample <- similarity.mat[random.subset.list[[i]], random.subset.list[[i]]]
      # perform clustering
      pam.clusters <- pam(1 - cosine.sample, k = k, diss = TRUE)
    })
  })
}
```

We decide to test the range of k=5,...,20 clusters and repeat each of these clusterings across 100 different 90% sub-sample.
```{r}
# generate clusters ranging from 5 to 20 cluster groups for each of 100 subsamples
# This will take a little while to run
cluster.iterations <- generateClusters(cosine_similarity, 
                                       k.range = 5:20, 
                                       N = 100)
```

We next need to clean the results into a nice format. The outer list of join.cluster.iterations below corresponds to each k value. Each list entry is a data frame for a single subsample in which the first column corresponds to the word and the remaining columns correspond to the word cluster for each value of k.
```{r}
# clean the simulation structure
join.cluster.iterations <- lapply(cluster.iterations, function(list) {
  # for each list of iterations (for a specific k), 
  # full-join the membership vectors into a data frame 
  # (there will be missing values in each column)
  Reduce(function(x, y) full_join(x, y, by = "words"), 
    lapply(list, function(cluster.obj) {
      df <- data.frame(words = names(cluster.obj$clustering), 
                 clusters = cluster.obj$clustering)
      }))
  })
# clean column names 
join.cluster.iterations <- lapply(join.cluster.iterations, function(x) {
  colnames(x) <- c("words", paste0("membership", 1:100))
  return(x)
  })
```

Below we print the first six entries of the membership vectors for 7 of the 100 iterations when k=5. Notice that there are NA values: these correspond to words that were omitted in the 90% subsample. There are actually 100 membership rows in the full data frame, each corresponding to an iteration of PAM with k=5 on a 90% subsample.
```{r}
# view the first 8 columns of the first data frame (correpsonding to k=5)
kable(head(join.cluster.iterations[[1]][, 1:8]))
```


Next for each pair of these cluster iterations, we can calculate the Jaccard similarity. To speed things up, we wrote a Jaccard function in C++. Note also that to avoid correlations, we take independent pairs: e.g. we calculate the Jaccard similarity between the membership vector from iterations 1 and 2, and then from 3 and 4, then from 5 and 6, etc. This means that for each value of k we calculate 50 Jaccard similarity values.
```{r}
# calculate the pairwise jaccard similarity between each of the cluster 
# memberships accross the common words
# to avoid correlation, we do this pairwise between simulations 1 and 2, 
# and then between simulations 3 and 4, and so on
library(Rcpp)
library(reshape2)
# use Rcpp to speed up the computation
sourceCpp('Rcpp_similarity.cpp')
jaccard.similarity <- sapply(join.cluster.iterations, 
       function(cluster.iteration) {
        sapply(seq(2, ncol(cluster.iteration) - 1, by = 2), 
             function(i) {
               # calculate the Jaccard similarity between each pair of columns
               cluster.iteration.pair <- cluster.iteration[ , c(i, i + 1)]
               colnames(cluster.iteration.pair) <- c("cluster1", "cluster2")
               # remove words that do not appear in both 90% sub-samples
               cluster.iteration.pair <- cluster.iteration.pair %>%
                 filter(!is.na(cluster1), !is.na(cluster2))
               # Calcualte the Jaccard similarity between the two cluster vectors
               RcppSimilarity(cluster.iteration.pair[ , 1], 
                              cluster.iteration.pair[ , 2])
             })
  })
```
We next want to melt jaccard.similarity into a long-form data frame that is easy to manipulate for visualisation.
```{r}
# average similarity over simulations
jaccard.similarity.long <- melt(jaccard.similarity)
colnames(jaccard.similarity.long) <- c("iter", "k", "similarity")
# k is the number of clusters
jaccard.similarity.long$k <- jaccard.similarity.long$k + 4
jaccard.similarity.long <- jaccard.similarity.long %>% 
  filter(k <= 20)
# average over iterations
jaccard.similarity.avg <- jaccard.similarity.long  %>%
  group_by(k) %>%
  summarise_at(.vars = names(.)[3],.funs = c(similarity ="mean"))
```

Plotting the Jaccard similarity for each k. 
```{r}
# plot number of clusters versus Jaccard similarity
ggplot(jaccard.similarity.long) + 
  geom_boxplot(aes(x = k, y = similarity, group = k)) +
  geom_line(aes(x = k, y = similarity), 
            linetype = "dashed",
            data = jaccard.similarity.avg) +
  ggtitle("Jaccard similarity versus k")
```

# Clustering with 20 clusters
Having decided that k=20 is an appropriate number of clusters to generate, we use PAM on the full cosine distance matrix (it is easier to provide a dissimilarity matrix than it is to provide a similarity matrix when clustering). Later we will compare the clustering we obtain when we set k=13 so we will compute this too.
```{r}
# note that there are some floating point issues in the similarity matrix:
# some "1" entires are actually sliiightly larger than 1, so we round to 
# the nearest 10 dp when calcualting the distance matrix
word.clusters <- pam(acos(round(cosine_similarity, 10)) / pi, k = 20, diss = TRUE)
word.clusters.13 <- pam(acos(round(cosine_similarity, 10)) / pi, k = 13, diss = TRUE)
```
We then define the cluster labels to be the medoid (center) word for that cluster (recall that PAM is like k-means but requires that the center of the cluster is a data point).
```{r}
# print the cluster medoids
word.clusters$medoids
## [1] "various"       "practitioner"  "emotions"      "valuable"     
## [5] "aggressive"    "rumination"    "spain"         "multivariable"
## [9] "relate"        "chd"           "strong"        "family"       
## [13] "session"       "maladjustment" "parietal"      "common"       
## [17] "presence"      "greater"       "cronbach"      "neural"    

# convert the membership vector to a factor
word.membership <- factor(word.clusters$clustering)

# print the cluster medoids
word.clusters.13$medoids
## [1] "various"         "practitioner"    "parietal"        "software"       
## [5] "victimization"   "spain"           "chd"             "strong"         
## [9] "rumination"      "effective"       "deviation"       "contribution"   
## [13] "parasympathetic"

# convert the membership vector to a factor
word.membership.13 <- factor(word.clusters.13$clustering)

# replace integer membership by medoid membership
levels(word.membership) <- word.clusters$medoids
# replace integer membership by medoid membership
levels(word.membership.13) <- word.clusters.13$medoids
```

Next, we compare an example of clustering with k=20 and k=13. For the most part, we are curious about which clusters would be forced to split into two or more clusters.Below we plot a superheatmap displaying the proportion of words in each cluster that are the same.

NOTE: IN THE EXAMPLE THE USE 11 AND 12 AS K NOT SURE IF BIGGER DISTANT BETWEEN THE 2 K VALUES INFLUENCE WHAT I HAVE DONE BELOW.
```{r}
# compare the membership vectors with 20 and 13 clusters
word.membership.split <- split(word.membership, word.membership)
word.membership.split.13 <- split(word.membership.13, word.membership.13)
compare.20.13 <- sapply(word.membership.split, function(i) {
  sapply(word.membership.split.13, function(j) {
    sum(names(i) %in% names(j)) / length(i)
  })
})

superheat(compare.20.13, 
          heat.pal = c("white", "grey", "black"),
          heat.pal.values = c(0, 0.1, 1),
          column.title = "20 clusters",
          row.title = "13 clusters",
          bottom.label.text.angle = 90,
          bottom.label.size = 0.4)
```
# Plotting a clustered superheatmap with silhouette plot

Next, we can calculate the cosine silhouette width for each word. We will plot this above the columns of our superheatmap.
```{r}
# calculate the cosine silhouette width
cosine.silhouette <- 
  cosineSilhouette(cosine_similarity, word.membership)
# arrange the words in the same order as the original matrix
rownames(cosine.silhouette) <- cosine.silhouette$word
cosine.silhouette <- cosine.silhouette[rownames(cosine_similarity), ]
```
Next, we want to order the clusters in order of average silhouette width.
```{r}
# calculate the average width for each cluster
avg.sil.width <- cosine.silhouette %>% 
  group_by(membership) %>% 
  summarise(avg.width = mean(width)) %>% 
  arrange(avg.width)
# add a blank space after each word (for aesthetic purposes)
word.membership.padded <- paste0(word.membership, " ")
# reorder levels based on increasing separation
word.membership.padded <- factor(word.membership.padded, 
                          levels = paste0(avg.sil.width$membership, " "))
```
We are now ready to plot a clustered superheatmap:

THIS CODE IS NOT WORKING
```{r}
superheat(cosine_similarity, 
          
          # row and column clustering
          membership.rows = word.membership.padded,
          membership.cols = word.membership.padded,
          
          # top plot: silhouette
          yt = cosine.silhouette$width,
          yt.axis.name = "Cosine\nsilhouette\nwidth",
          yt.plot.type = "bar",
          yt.bar.col = "grey35",
          
          # order of rows and columns within clusters
          order.rows = order(cosine.silhouette$width),
          order.cols = order(cosine.silhouette$width),
          
          # bottom labels
          bottom.label.col = c("grey95", "grey80"),
          bottom.label.text.angle = 90,
          bottom.label.text.alignment = "right",
          bottom.label.size = 0.28,
          
          # left labels
          left.label.col = c("grey95", "grey80"),
          left.label.text.alignment = "right",
          left.label.size = 0.26,
          
          # smooth heatmap within clusters
          smooth.heat = T,
          
          # title
          title = "(b)")
```
# Cluster word clouds
Lastly, we produce some word clouds to identify the members of each cluster. The function below produces a word cloud for a specific word cluster.
```{r}
library(RColorBrewer)
library(wordcloud)
# define a function that takes the cluster name and the membership vector 
# and returns a word cloud
makeWordCloud <- function(cluster, word.membership, words.freq) {
  words <- names(word.membership[word.membership == cluster])
  words.freq <- words.freq[words]
  # make all words black except for the cluster center
  words.col <- rep("black", length = length(words.freq))
  words.col[words == cluster] <- "red"
  # the size of the words will be the frequency from the NY Times headlines
  wordcloud(words, words.freq, colors = words.col, 
            ordered.colors = TRUE, random.order = FALSE, max.words = 80)
}
```
In each word cloud, the cluster center is in red, and the size corresponds to the word frequency from the NY Times headlines. It is thus interesting to notice that oftentimes the cluster center has fairly low frequency.
```{r}
# plot word clouds
set.seed(52545)
for (word in levels(word.membership)) {
  makeWordCloud(word, word.membership, words.freq = freq)
}
```

DIT LAATSTE WERKT NIET WANT WORD.MEMBERSHIP is gewoon alleen een factor dus bevat niet de column freq of iets. CODE nog even goed checken. Heeft misschien met de . te maken? 