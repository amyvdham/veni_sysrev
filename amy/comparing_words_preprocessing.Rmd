---
title: "Compare words from feature extractor with words data frame in manuscript"
Author: "Amy van der Ham"
date: "9/29/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load in the csv with the word vector embeddings obtained from the asreview simulation 

```{r}
# load csv into object 
dict_wordvecemb <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/asreview_simulation/dict_wordvec.csv", header = FALSE)

# adjust the first column name to word
colnames(dict_wordvecemb)[1] <- "word"

# check structure of dataframe
str(dict_wordvecemb)

# remove certain characters from the column V2 which now is column of the type character and contains a string as value. 
library(tidyverse)
# create new data frame that can be used for applying adjustments
df_wordvecemb <- dict_wordvecemb

# remove the [ character from V2 
df_wordvecemb$V2 <-gsub("\\[","",as.character(df_wordvecemb$V2))

# remove the ] character from V2 
df_wordvecemb$V2 <-gsub("\\]","",as.character(df_wordvecemb$V2))

# remove the \n character from V2 
df_wordvecemb$V2 <-gsub("\\\n","",as.character(df_wordvecemb$V2))

# check if removing the characters went correctly 
df_wordvecemb[1,"V2"]
# -> yes there are now only numbers (the vectors)


# split column V2 into multiple columns
library(splitstackshape)
# separate on the space
df_wordvecemb <- cSplit(df_wordvecemb, "V2", " ")

# retain dimensions of data frame
dim(df_wordvecemb)
# -> we have 19,476 words that are defined by 40 dimensions. These dimensions define the context of the words.

# check how many unique words there are
length(unique(df_wordvecemb$word))
# -> as expected there are as many unique words as there are observations. 

# rename the column names of the data frame. 
# First column is named word and the other columns dim1-40
colnames(df_wordvecemb) <- c("word", paste0("dim", 1:40))

# check if there are any missings
summary(df_wordvecemb)
# -> no missing values. 
```

## RUN ONCE
Study 2: code below shows words that are delete from abstracts before applying 
```{r}
# load libraries
library(stringr)
library(udpipe)
library(igraph)
library(wordcloud)
library(Matrix)
library(yaml)
library(Rmpfr)
library(topicmodels)
library(udpipe)
library(slam)
library(tidytext)
library(ggplot2)
library(textrank)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
source("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/word_functions.R")
source("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/circle2.R")

recs <- data.table(read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/recs_final.csv"))
recs[, "doc" := 1:nrow(recs)]

# convert abstract column to lower case
recs$AB <- tolower(recs$AB)

# download English udpipe model
ud_model <- udpipe_download_model(language = "english")
# load the language model - NOTE that this is a more recent version than the one Caspar used. 
ud_model <- udpipe_load_model("english-ewt-ud-2.5-191206.udpipe")
ud_model <- udpipe_load_model(ud_model$file)

# apply to abstract of recs data table
udp_res <- udpipe_annotate(ud_model, x = recs$AB, doc_id = recs$doc)

# convert to data table and save as .Rdata
df <- as.data.table(udp_res)
saveRDS(df, "study2_df.RData")
  
# Keyword extraction ------------------------------------------------------

# Exclude words
# make sure to only include nouns and adjectives. 
df_kw <- df[upos %in% c("NOUN", "ADJ"), ]
# make sure to only include strings that start with a letter. (. -> and end with any character?)
df_kw <- df_kw[grepl("^[a-zA-Z].", df_kw$lemma), ]
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df_kw$lemma)))
df_kw <- df_kw[-exclude_these, ]
saveRDS(df_kw, "study2_df_kw.RData")

# Dit is een check  
# No numeric values
# all(is.na(as.numeric(df_kw$lemma)))
# df_kw$lemma[nchar(df_kw$lemma) == 3]

# textrank is used for identifying more meaningful units of analysis
# DO NOT UNDERSTAND WHY upos %in% c("NOUN", "ADJ") line is applied again here because I thought this was already used to create df_kw. 
kw_tr <- textrank_keywords(x = df_kw$lemma[df_kw$upos %in% c("NOUN", "ADJ")], ngram_max = 3, sep = " ")
  saveRDS(kw_tr, "study2_textrank.RData")

# Merge back with original data
df_kw$keyword <- txt_recode_ngram(df_kw$lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = " ")

df_kw$keyword[!df_kw$keyword %in% kw_tr$keywords$keyword] <- NA
  
# NOTE this time we do not want to apply the dictionary filter. 
df_analyze <- df_kw[!is.na(df_kw$keyword), ]
# save the 'cleaned' dataframe
saveRDS(df_analyze, "study2_analyze_without_dict_filter.RData")

####
# Also run it with applying dictionary filter. So I can look at what happens and what the differences are. 
# load dictionary 
dict <- read_yaml("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/yaml_dict.txt")
# check number of obs when dic is applied
res_cat <- cat_words(df_analyze$keyword, dict, handle_dups = "all")
  # Check coding issues
  #res_cat$dup
  #head(res_cat$unmatched)
  df_analyze_dict <- merge_df(df_analyze, res_cat$words, "word_coded")
  saveRDS(df_analyze_dict, "study2_df_analyze.RData")
  
# check if corpus consist of same number of documents and unique terms as in paper
nounbydoc <- df_analyze_dict[, list(freq = .N), by = list(doc_id = doc_id, term = word_coded)]
number_docs_words2 <- c(docs = length(unique(nounbydoc$doc_id)), words = length(unique(nounbydoc$term)))
```
## START HERE!!
Since I have already once run the code above once, I can now just call the saved R.Data files. 

```{r}
# WITHOUT DICT FILTER APPLIED
# load the pre-processed file (without dict filter)
df_paper <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_analyze_without_dict_filter.RData")

# check number of unique words and doc_id
length(unique(df_paper$keyword))
length(unique(df_paper$doc_id))
# -> 17584 unique words (or more precisely word units)

# create data frame with column doc_id term and add a term frequency per doc column column 
nounbydoc1 <- df_paper[, list(freq = .N), by = list(doc_id = doc_id, term = keyword)]

# check if that went correctly 
count(df_paper$doc_id == 3228 & df_paper$keyword == "conflict mental health")
# this gives 5 and that is also the freq give in the datafram nounbydoc1, so went correctly. 

# which terms occur in more than 6 documents
library(dplyr)
df_morethan6 <-  nounbydoc1 %>% group_by(term) %>% filter(n()>6) 

length(unique(df_morethan6$term))
length(unique(df_morethan6$doc_id))

### WITH DICT FILTER APPLIED
# load the preprocessed file (with dict filter)
df_paperdict <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_analyze.RData")

# create data frame with column doc_id term and add a term frequency per doc column column 
nounbydoc2 <- df_paperdict[, list(freq = .N), by = list(doc_id = doc_id, term = word_coded)]

# check number of unique words and number of unique documents
length(unique(nounbydoc2$doc_id))
length(unique(nounbydoc2$term))
# -> note that it does not concern unique words but unique word units. 

# which terms occur in more than 6 documents
df_mt6 <-  nounbydoc2 %>% group_by(term) %>% filter(n()>6) #
length(unique(df_mt6$term))
length(unique(df_mt6$doc_id))
```

# Create filter that can be applied on dataframe with word embeddings to decide which words to keep. 
Here I work with the dataframe on which the textrank algorithm is already applied which means I have to bring the word backs to unigrams. I have however also created a filter called include_these2 later on in which I use a dataframe on which the textrank algorithme is not applied yet. -> This makes more sense because then I do not have to bring them back to unigrams. 
```{r}
library(splitstackshape)
# Make sure that there are only unigrams
# separate on the space
df_paper2 <- cSplit(df_paper, "keyword", " ")

# create data frame with only doc_id and keyword columns
dat <- df_paper2[ , c("doc_id", "keyword_1", "keyword_2", "keyword_3")]

# create new data frame in which keyword columns are stacked
library(reshape2)
dat2 <- melt(dat, id.vars=1)
dat2

dat2 <- na.omit(dat2)
# create data frame with column doc_id term and add a term frequency per doc column column 
nounbydoc3 <- dat2[, list(freq = .N), by = list(doc_id = doc_id, term = value)]

# check number of unique words and number of unique documents unigrams 
length(unique(nounbydoc3$doc_id)) 
length(unique(nounbydoc3$term)) 

# how many times do terms occur in the dataframe
df3_mt6 <- nounbydoc3 %>% group_by(term) %>% filter(n()>6) #
length(unique(df3_mt6$term))
length(unique(df3_mt6$doc_id))

# create a dataframe with one column including the unique terms
include_these <- unique(df3_mt6$term)
include_these <- as.data.table(include_these)

saveRDS(include_these, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_these.RData")
```

# FILTER FOR CLUSTERING
Do the same as before but now only with the filter that is applied in the official manuscript until textrank. Because textrank concerns the bi - and trigrams. 

Things that are applied: 
- to lower
- udpipe ("english"): (stopwords, POS, stemming, nouns and adjectives)
- include only strings that start with a letter
- exclusion filter (removing methodological terms and similar non-substantive words)

Might want to consider not applying the final filter but fine for now I think. 


```{r}
# WITHOUT TEXTRANK ALGORITHM APPLIED
# load the pre-processed file (without textrank algorithm dict filter having been applied)
df_include <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")

# check number of unique words and doc_id
length(unique(df_include$token))
length(unique(df_include$doc_id))
# -> 16499 unique tokens

# check number of unique words 
length(unique(df_include$lemma))
# -> 15274 unique words


# check difference between tokens and lemma's
diff <- df_include[df_include$token != df_include$lemma, ]
# -> looking at the manuscript we want to continue with the lemma column and not the token column. 

length(unique(diff$token))

# check if there are only nouns and adj in this dataframe
unique(df_include$upos)
# -> yes.

# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc4<- df_include[, list(freq = .N), by = list(doc_id = doc_id, term = lemma)]

# check number of unique words and documents. 
length(unique(nounbydoc4$doc_id)) 
length(unique(nounbydoc4$term)) 

# create a data frame with one column including the unique terms
include_these2 <- unique(nounbydoc4$term)
include_these2 <- as.data.table(include_these2)

saveRDS(include_these2, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_these2.RData")
```

# Apply more filters.
Stopwords removal + removal of dots, numbers and maybe also removal of - or splitting words on -. Finally I might also want to consider to apply stemming after using the filter on the feature matrix. But this is something I will look into later. 

# LEMMA
```{r}
# first I want to load in a file that still contains the upos column to see if the stopwords have been assigned NOUN or ADJ. On this file all filters have been applied until textrank algorith. So this is the file that has been used to create include_these2. On this file we want to apply more filters (mentioned above) before saving it as include_these3.
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")

# check which value is assigned to the word "and" in the upos column. 
df_check[df_check$lemma == "and", c("lemma", "upos")]
# we see that the word "and" is assigned NOUN where as you would expect this to be a conjunction. 

# -> because there are also other stopwords in there that might have been assigned noun or adj in the upos column and are therefore still included I will have to apply an extra filter that removes the stopwords. This makes sense because despite the word "and" is not some stopwords can indeed be a noun or adj. 

# check number of unique words before applying more filters
length(unique(df_check$lemma))
# 15274

# REMOVING STOPWORDS.
# use stopwords from tidytext. 
library(tidyverse)
library(tidytext)
clean_df <- df_check %>%
  anti_join(stop_words, by= c("lemma" = "word"))

# check number of unique words after removing stop words
length(unique(clean_df$lemma))
# -> 15002: 272 unique words dropped 

# double check if the word "and" is removed from the file
clean_df[clean_df$lemma == "and", c("token", "lemma", "doc_id")]
# -> it is. 

# REMOVE TERMS THAT HAVE A NUMBER
# check if term with number is on the df
clean_df[clean_df$lemma == "rmsea=.046", c("token", "lemma", "doc_id")]
# -> yes it is. 

# apply filter that removes terms that contain a digit. 
clean_df <- clean_df %>%
  filter(!str_detect(lemma, "[:digit:]"))

# check if filter went correctly 
clean_df[clean_df$lemma == "rmsea=.046", c("token", "lemma", "doc_id")]
# this term is not in the dataframe anymore, so filter went correctly. 

# check number of unique words after removing words with digits
length(unique(clean_df$lemma))
# -> 14691: 311 unique words dropped.

# REMOVE DOTS FROM STRING VALUES IN LEMMA AND TOKEN COLUMN
# check if the word "data." is in the file.
clean_df[clean_df$lemma == "data.", c("token", "lemma", "doc_id")]
# -> yes it is. 

# check number of unique tokens before applying the next filter
length(unique(clean_df$token))
# -> 15877

# Check number of observations before applying filter on terms. We want the number of observations to stay the same because we are not removing words, we are only removing part parts of the words.
nrow(clean_df)
# -> 353958

# remove dots from string values in lemma column 
clean_df$lemma <- gsub("\\.","",clean_df$lemma)

# remove dots from string values in token column 
clean_df$token <- gsub("\\.","",clean_df$token)
          
# check if filter went correctly 
clean_df[clean_df$lemma == "data.", c("token", "lemma", "doc_id")]

# check if filter went correctly 
clean_df[clean_df$lemma == "data", c("token", "lemma", "doc_id")]
# "data." is not in the dataframe anymore and "data" is so filter went correctly 

# check number of unique words after removing dots. Should be the same as before. 
length(unique(clean_df$lemma))
# -> 14471: 220 unique (lemma) words dropped. It could be that some of the words were in there with a . and without, so this will reduce the number of unique words if we remove dots. 

length(unique(clean_df$token))

# also check unique number of tokens
length(unique(clean_df$token))
# -> 15632: 254 unique (token) words dropped.

# check if number of observations is still the same 
nrow(clean_df)
# -> 353958: this is still the same as before, which shows it went correctly. 

# REMOVE - FROM STRING VALUES IN LEMMA and TOKEN COLUMN
# check if the word "non-clinical" is in the file.
clean_df[clean_df$lemma == "non-clinical", c("token", "lemma", "doc_id")]
# -> yes it is. 

# Check number of observations before applying filter on terms. We want the number of observations to stay the same because we are not removing words, we are only removing part parts of the words.
nrow(clean_df)
# -> 353958

# remove - from string values in lemma column 
clean_df$lemma <- gsub("\\-","",clean_df$lemma)

# remove - from string values in token column 
clean_df$token <- gsub("\\-","",clean_df$token)
          
# check if filter went correctly 
clean_df[clean_df$lemma == "non-clinical", c("token", "lemma", "doc_id")]

# check if filter went correctly 
clean_df[clean_df$lemma == "nonclinical", c("token", "lemma", "doc_id")]
# "non-clinical" is not in the dataframe anymore and "nonclinical" is so filter went correctly 

# check number of unique words after removing -. Should be the same as before. 
length(unique(clean_df$lemma))
# -> 14195: 276 unique (lemma) words dropped. It could be that some of the words were in there with a - and without, so this will reduce the number of unique words if we remove -. 

# also check unique number of tokens
length(unique(clean_df$token))
# -> 15343: 289 unique (token) words dropped

# check if number of observations is still the same 
nrow(clean_df)
# -> 353958: this is still the same as before, which shows it went correctly. 
```


Note that the needed filters have been applied create a file with terms to include. Note that I make one with the tokens (so before stemming/lemmatization) and with the lemma column (so before stemming/lemmatization). 

*NOTE*: I have that I have removed - signs in the terms. Could also have chosen to split terms on - sign and have to separate terms for this. 


```{r}
# SAVE INCLUDE_TOKEN
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc5<- clean_df[, list(freq = .N), by = list(doc_id = doc_id, term = token)]

# check number of unique words and documents. 
length(unique(nounbydoc5$doc_id)) 
length(unique(nounbydoc5$term)) 

# create a data frame with one column including the unique terms
include_token <- unique(nounbydoc5$term)
include_token <- as.data.table(include_token)

saveRDS(include_token, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_token.RData")


# SAVE INCLUDE_LEMMA
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc6<- clean_df[, list(freq = .N), by = list(doc_id = doc_id, term = lemma)]

# check number of unique words and documents. 
length(unique(nounbydoc6$doc_id)) 
length(unique(nounbydoc6$term)) 

# create a data frame with one column including the unique terms
include_lemma <- unique(nounbydoc6$term)
include_lemma <- as.data.table(include_lemma)

saveRDS(include_lemma, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_lemma.RData")
```

# Check manuscript
Figuring out what happens with the piece of code under Keyword extraction (Exclude words till textrank) in manuscript.
```{r}
# check data frame before exclusion filter is applied
df_bf_ex <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df.RData")

# check number of unique words
length(unique(df_bf_ex$lemma))
# 25535 words 

# check different part of speech values 
unique(df_bf_ex$upos)

# only select nouns and adjectives
df_ex <- df_bf_ex[upos %in% c("NOUN", "ADJ"), ]

# check number of unique words
length(unique(df_ex$lemma))
# -> 16311: 9224 unique words dropped

# makes sure that only strings that start with a letter are included. (. -> and end with any character?)
df_ex1 <- df_ex[grepl("^[a-zA-Z].", df_ex$lemma), ]

# check number of unique words
length(unique(df_ex1$lemma))
# -> 15515: 796 unique words dropped 

# excludes methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df_ex1$lemma)))
df_ex2 <- df_ex1[-exclude_these, ]

# check number of unique words
length(unique(df_ex2$lemma))
# -> 15274: 241 unique words dropped. 

# CHECK WHAT HAPPENS WHEN DICTIONARY FILTER IS APPLIED. 
# first apply text rank code 
kw_tr <- textrank_keywords(x = df_ex2$lemma[df_ex2$upos %in% c("NOUN", "ADJ")], ngram_max = 3, sep = " ")

# Merge back with original data
df_ex2$keyword <- txt_recode_ngram(df_ex2$lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = " ")

df_ex2$keyword[!df_ex2$keyword %in% kw_tr$keywords$keyword] <- NA

# from here the dictionary filter is applied. Check what happens 
df_analyze <- df_ex2[!is.na(df_ex2$keyword), ]

# check df_analyze on some words that should be there before dicitionary filter and should have a different word after filter is applied
df_analyze[df_analyze$token == "binge", ]
df_analyze[df_analyze$keyword == "binge drinking related", ]

# I think this is a keywords that should fall under substance once the dictionary filter is applied. 
df_analyze[df_analyze$keyword == "substance use disorder", ]

# apply dictionary filter
dict <- read_yaml("yaml_dict.txt")
res_cat <- cat_words(df_analyze$keyword, dict, handle_dups = "all")
# Check coding issues
res_cat$dup
head(res_cat$unmatched)
df_analyze <- merge_df(df_analyze, res_cat$words, "word_coded")

# check what happened 
df_analyze[df_analyze$keyword == "substance use disorder", c("token", "lemma", "keyword", "word_coded")]
# as expected this word is now coded as substance. 

# check which words fall under the word_coded "mothers"
df_analyze[df_analyze$word_coded == "mothers", c("token", "lemma", "keyword", "word_coded")]

df_analyze[df_analyze$lemma == "inequality", c("token", "lemma", "keyword", "word_coded")]
# -> here we see that this lemma has different values for the word_coded column. 

# check which words fall under the word_coded "negative_emotionality"
df_analyze[df_analyze$word_coded == "negative_emotionality", c("token", "lemma", "keyword", "word_coded")]


# check if the word "mothers" in token column falls under different word_coded values
df_analyze[df_analyze$token == "mothers", c("token", "lemma", "keyword", "word_coded")]


# check the keyword "mother online well"
df_analyze[df_analyze$keyword == "mother online well", ]

# -> here we see that this concerns the same sentence, it is in there twice because it has two different values for word_coded. 
```
