---
title: "Try - out K-means"
author: "Amy van der Ham"
date: "10/22/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Try out k-means clustering on the embeddings on which the pre-processing lemma filter is applied

```{r}
# load libraries
library(dplyr)
library(cluster)
library(ggplot2)
library(devtools)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
```

# set.seed prior to running the k-means clustering 
```{r}
set.seed(88)
```

# Load needed datafiles
```{r}
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="") 

# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))

# load data frame with column with selection of words to include in analysis. For now I will only 
df_incllemma <- readRDS("include_lemma.RData")

# create df in which only the words that we want to be included are kept
lemma_embedding <- subset(vectors_glove, word %in% df_incllemma$include_lemma)

# check number of unique words
length(unique((lemma_embedding$word)))

# non-GloVe: check which words are in the included filter but are not in the feature matrix and are therefore lost (unwanted).
lost_lemma <- subset(df_incllemma, !(include_lemma %in% lemma_embedding $word))
# 2768 words lost

length(unique((lemma_embedding$word))) + length(unique((lost_lemma$include_lemma)))
# ->  14195: dit is gelijk aan:
length(unique((df_incllemma$include_lemma)))

```

# Make final GloVE embedding ready for analysis
```{r}
# convert first column, word, to row index
library(tidyverse)
glove_embedding <- lemma_embedding %>%
     remove_rownames() %>%
     column_to_rownames(var = 'word')

# convert dataframe to a matrix
glove_embedding <- as.matrix(glove_embedding)
str(glove_embedding)
```

# Look at wich words are close to mother
```{r}
library(text2vec)
find_similar_words <- function(word, embedding_matrix, n = 5) {
  similarities <- embedding_matrix[word, , drop = FALSE] %>%
    sim2(embedding_matrix, y = ., method = "cosine")
  
  similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}

find_similar_words("health",glove_embedding,25)
```

# Clustering analysis
- https://rstudio-pubs-static.s3.amazonaws.com/33876_1d7794d9a86647ca90c4f182df93f0e8.html

- https://www.r-bloggers.com/2013/09/clustering-search-keywords-using-k-means-clustering/

```{r}
# fit the k-means clustering with 100 clusters
k_means_fit <- kmeans(glove_embedding, 100, iter.max = 30)
# -> default iteration is 10 but this gave the following warning: did not converge in 10 iterations so set the iter.max to 30. 

# obtain the centroids
k_means_fit$centers

# look at the size of the clusters
k_means_fit$size

# find the cluster to which each word belongs
k_means_fit$cluster

# Create data frame in which the merge cluster assignment back to rows/word. 
kw_with_cluster <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit$cluster))
# add column names
names(kw_with_cluster) <- c("word", "kmeans100")

# make a df for the first 5 cluster results, quickly "eyeball" results
cluster1 <- subset(kw_with_cluster, subset=kmeans100 == 1)
cluster2 <- subset(kw_with_cluster, subset=kmeans100 == 2)
cluster3 <- subset(kw_with_cluster, subset=kmeans100 == 3)
cluster4 <- subset(kw_with_cluster, subset=kmeans100 == 4)
cluster5 <- subset(kw_with_cluster, subset=kmeans100 == 5)

# Mother 
# find out in which cluster the word mother is assigned
kw_with_cluster[kw_with_cluster$word == "mother", ]

# make a df of cluster 26
cluster6 <- subset(kw_with_cluster, subset=kmeans100 == 6)

# I did not setseeded the cluster analysis yet so want to compare the difference between the two clusters that contain the word mother. -> Can not rerun this code now that I have set.seeded the analysis but it is important to be aware that every time you run the analysis and do not set.seed, the clusters will be different. The 3 times I have now run the analyis the cluster containing mother had either 71, 70 or 86 observations. 
diff_setseed <- subset(cluster6, !(word %in% cluster26$word))

# Health
# find out in which cluster the word health is assigned
kw_with_cluster[kw_with_cluster$word == "health", ]

# make a df of cluster 55
cluster55 <- subset(kw_with_cluster, subset=kmeans100 == 55)

# Environment
# find out in which cluster the word environment is assigned
kw_with_cluster[kw_with_cluster$word == "environment", ]

# make a df of cluster 75
cluster75 <- subset(kw_with_cluster, subset=kmeans100 == 75)

# Depression
# find out in which cluster the word depression is assigned so that I can check if all the forms of depression are in there. 
kw_with_cluster[kw_with_cluster$word == "depression", ]

# make a df of cluster 35
cluster35 <- subset(kw_with_cluster, subset=kmeans100 == 35)
# -> only the words depressed, depression, depressive are in this cluster. Might be that these are the only depression words in the embedding need to check this. -> If I look at the lemma_embedding dataframe and type depre in the filter within the word column I get the following words: nondepressed, antidepressant depressed, depression, depressive. It makes sense that the other two words concerning depression can belong to different clusters.

# Kelly
# find out in which cluster the word Kelly is assigned so that I can check if all names are put together for example. 
kw_with_cluster[kw_with_cluster$word == "kelly", ]

# make a df of cluster 80
cluster80 <- subset(kw_with_cluster, subset=kmeans100 == 80)

# cortisol
# find out in which cluster the word cortisol is assigned so that I can check if all names are put together for example. 
kw_with_cluster[kw_with_cluster$word == "cortisol", ]

# make a df of cluster 90
cluster90 <- subset(kw_with_cluster, subset=kmeans100 == 90)

# empathy
# find out in which cluster the word empathy is assigned so that I can check if all names are put together for example. 
kw_with_cluster[kw_with_cluster$word == "empathy", ]

# make a df of cluster 33
cluster33 <- subset(kw_with_cluster, subset=kmeans100 == 33)

## Selecting ‘k’ Using ‘Elbow Method’ ##
# accumulator for cost results
cost_df <- data.frame()

# run kmeans for all clusters up to 100
for(i in 1:100){
  # Run kmeans for each level of i, allowing up to 100 iterations for convergence
  kmeans<- kmeans(x=glove_embedding, centers=i, iter.max=100)
  
  # Combine cluster number and cost together, write to df
  cost_df<- rbind(cost_df, cbind(i, kmeans$tot.withinss))

}

names(cost_df) <- c("cluster", "cost")

# create the elbow plot -> this is not working. 
#Calculate lm's for emphasis
lm(cost_df$cost[1:10] ~ cost_df$cluster[1:10])
lm(cost_df$cost[10:19] ~ cost_df$cluster[10:19])
lm(cost_df$cost[20:100] ~ cost_df$cluster[20:100])

cost_df$fitted <- ifelse(cost_df$cluster <10, (489521  - 7126*cost_df$cluster), 
                         ifelse(cost_df$cluster <20, ( 443476 - 1610*cost_df$cluster),
                         (415803.5 - 497.2  *cost_df$cluster)))

#Cost plot
ggplot(data=cost_df, aes(x=cluster, y=cost, group=1)) + 
theme_bw(base_family="Garamond") + 
geom_line(colour = "darkgreen") +
theme(text = element_text(size=20)) +
ggtitle("Reduction In Cost For Values of 'k'\n") +
xlab("\nClusters") + 
ylab("Within-Cluster Sum of Squares\n") +
scale_x_continuous(breaks=seq(from=0, to=200, by= 10)) +
geom_line(aes(y= fitted), linetype=2)

```

## Apply cluster analysis within a cluster (cluster 33) ##
```{r}
# create new subset data frame
embedding_cluster33 <- subset(glove_embedding, rownames(glove_embedding) %in% cluster33$word)

# fit k-means
k_means_fit_cluster33 <- kmeans(embedding_cluster33, 3, iter.max = 30)

# look at the results
# obtain the centroids
k_means_fit_cluster33$centers

# look at the size of the clusters
k_means_fit_cluster33$size

# find the cluster to which each word belongs
k_means_fit_cluster33$cluster

# Create data frame in which the merge cluster assignment back to rows/word. 
token_within_clusters <- as.data.frame(cbind(row.names(embedding_cluster33), k_means_fit_cluster33$cluster))
# add column names
names(token_within_clusters) <- c("word", "kmeans3")

# make a df for the 3 cluster results
cluster33_1 <- subset(token_within_clusters, subset=kmeans3 == 1)
cluster33_2 <- subset(token_within_clusters, subset=kmeans3 == 2)
cluster33_3 <- subset(token_within_clusters, subset=kmeans3 == 3)

##  Determine the value of K 
library(factoextra)
# Elbow method
fviz_nbclust(embedding_cluster33, kmeans, method = "wss") + labs(subtitle = "Elbow method") # add subtitle

# geom_vline(xintercept = 4, linetype = 2) + # add line for better visualisation. I do not see a clear knee location in the plot. 

# Silhouette method
fviz_nbclust(embedding_cluster33, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")

# Gap statistic
fviz_nbclust(embedding_cluster33, kmeans,
  nstart = 25,
  method = "gap_stat",
  nboot = 500 # reduce it for lower computation time (but less precise results)
) +
  labs(subtitle = "Gap statistic method")


# try to determine number of cluster with NbClust package
library(NbClust)
nbclust_out <- NbClust(
  data = embedding_cluster33,
  distance = "euclidean",
  min.nc = 2, # minimum number of clusters
  max.nc = 10, # maximum number of clusters
  method = "kmeans", # one of: "ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid", "kmeans"
  index = "silhouette"
)

nbclust_out

# create plot of results
factoextra::fviz_nbclust(nbclust_out) + theme_minimal() + ggtitle("NbClust's optimal number of clusters")

# Another package for determine number of clusters
library(ClusterR) 

Optimal_Clusters_KMeans(embedding_cluster33, 
              max_clusters = 15, 
              criterion = "distortion_fK",
              fK_threshold = 0.85, 
              num_init = 3, 
              max_iters = 50,
              initializer = "kmeans++",
              tol = 1e-04, 
              plot_clusters = TRUE,
              verbose = T, 
              tol_optimal_init = 0.3, 
              seed = 1)

# Run analysis with 10 clusters
km_res <- kmeans(embedding_cluster33, centers = 10, nstart = 20)

sil <- silhouette(km_res$cluster, dist(embedding_cluster33))
fviz_silhouette(sil)
# low average and negative values. 

# check cluster with highest silhouette index
resulting_clusters <- as.data.frame(cbind(row.names(embedding_cluster33), km_res$cluster))
# add column names
names(resulting_clusters) <- c("word", "kmeans10")

cluster5_highestsil <- subset(resulting_clusters, subset=kmeans10 == 5)

# visualize the k-means (2) clusters
fviz_cluster(k_means_fit_cluster33, data = embedding_cluster33,
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
ellipse.type = "euclid", # Concentration ellipse
star.plot = TRUE, # Add segments from centroids to items
repel = TRUE, # Avoid label overplotting (slow)
ggtheme = theme_minimal()
)
```


# Determining the value of the parameter K
This gives warning about the number of iterations being set to 10 is not enough to converge. So will need to adjust this. Next to this the resulting plot does not give a clear elbow point. 
```{r}
wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i, iter.max = 30)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}

wssplot(glove_embedding, nc=200) 
```

# Represent the cluster solution into 2 dimensions
```{r}
library(cluster)
clusplot(glove_embedding, k_means_fit$cluster, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,
         labels=2, lines=0)
```

# Other tutorial on nonhierarchical k-means clustering of words/tweets. Is zelfde manier maar kijken even welke stappen hij hierna nog doet. Note that hij volgensmij wel term document matrix gebruikt voor het clusteren. 
https://www.youtube.com/watch?v=v2hip37YRgg, minute 9:44
```{r}
k <- 50
kc <- kmeans(glove_embedding, k)
kc
k_means_fit
# this one shows beter results for between_SS / total_SS. 

# try to make a plot
clusplot(glove_embedding, 
         kc$cluster,
         color = TRUE,
         shade = TRUE,
         labels = 2,
         lines = 0)
```

# Gaussian Mixture Model Clustering
gmm: Gaussian Mixture Model clustering 
https://github.com/teramonagi/scdv
```{r}
library(scdv)
gmm(glove_embedding, 100, args = list())
```

## CLUSTER ANALYSIS DETERMINING THE NUMBER OF CLUSTER FOR FULL DATASET##
https://statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/#silhouette-method
```{r}
# Elbow method
# toevoegen aan kmeans, iter.max = 30 en andere bins (stapjes 1-10, 10-20) op x as. 
fviz_nbclust(glove_embedding, kmeans, method = "wss", k.max = 150) + labs(subtitle = "Elbow method") # add subtitle

# geom_vline(xintercept = 4, linetype = 2) + # add line for better visualisation. I do not see a clear knee location in the plot. 

# Silhouette method
fviz_nbclust(glove_embedding, kmeans, method = "silhouette", k.max = 150) +
  labs(subtitle = "Silhouette method")

silplot

# Dit duurt heel lang
# Gap statistic
fviz_nbclust(glove_embedding, kmeans,
  nstart = 25,
  method = "gap_stat",
  nboot = 500 # reduce it for lower computation time (but less precise results)
) +
  labs(subtitle = "Gap statistic method")


# try to determine number of cluster with NbClust package
library(NbClust)
nbclust_out <- NbClust(
  data = glove_embedding,
  distance = "euclidean",
  min.nc = 2, # minimum number of clusters
  max.nc = 150, # maximum number of clusters
  method = "kmeans", # one of: "ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid", "kmeans"
  index = "silhouette"
)
# duurt heel lang en krijg de converge warning weer. 

nbclust_out
```

