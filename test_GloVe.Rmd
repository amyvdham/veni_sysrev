---
title: "Try out GloVe and Word2Vec"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Try to apply tutorial on GloVe in R on own data. 

```{r}
# Loading required libraries
library(stringr)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(Matrix)
library(ggplot2)
library(dplyr)

# Required library for GloVe
library(text2vec)

# Loading script containing functions
source("word_functions.R")

# prepare data 
# this is were line 891 in manuscript file starts
recs <- read.csv("recs_final.csv")

# convert object into data table
recs <- as.data.table(recs)

# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]

## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})

# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column. 
df <- merge_df(recs, df, "word")

# make sure that the values in the column word do not contain any capitals.   
df[, word := tolower(word)]
  
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")

## Exclude words
# create object with the terms that should be excluded  
exclude_terms <- readLines("exclude_terms.txt")

# object with all the row numbers of author keywords that should be excluded from the data frame 
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))

# create new data frame that excludes all the row numbers that have an author keyword that should be excluded 
df <- df[!exclude_these, ]

# Now that the dataset is cleaned created a df in which each document is a row and the column DEclean contains the author keywords of the specific document. Shape the tokens back to their original form
df_new <- df %>% 
          group_by(word) %>% 
          group_by(doc) %>% 
          summarise(DEclean = str_c(word, collapse = " "))
```

Try to run the code of the [tutorial] (https://medium.com/broadhorizon-cmotions/nlp-with-r-part-2-training-word-embedding-models-and-visualize-results-ae444043e234) with the cleaned dataset from above. 
```{r}
# We need to tokenize our already tokenized set as input for text2vec, re-use cleaned text in df_new
it <- itoken(df_new$DEclean, 
                   tokenizer = word_tokenizer,
                   ids = df_new$doc,
                   progressbar = TRUE)

# create a vocabulary out of the tokenset (stopword removal and bi-grams are optional)
vocab <- create_vocabulary(it) # use uni-grams

# text2vec has the option to prune the vocabulary of low-frequent words
vocab <- prune_vocabulary(vocab, term_count_min = 5)

# What's in the vocabulary?
print(vocab)
```

Next, vectorize input tokens and create a Term-Count-Matrix for GloVe to handle. 
```{r}
# Vectorize word to integers
vectorizer <- vocab_vectorizer(vocab)

# Create a Term-Count-Matrix, by default it will use a skipgram window of 5 (symmetrical)
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)

# maximum number of co-occurrences to use in the weighting function, we choose the entire token set divided by 100
x_max <- length(vocab$doc_count)/100

# set up the embedding matrix and fit model
glove_model <- GloVe$new(rank = 32, x_max = x_max) 
glove_embedding = glove_model$fit_transform(tcm, n_iter = 20, convergence_tol = 0.01, n_threads = 4)

# combine main embedding and context embeddings (sum) into one matrix
glove_embedding = glove_embedding + t(glove_model$components) # the transpose of the context matrix
```

Now check how well Glove is doing on the author keywords
```{r}
# wat ligt er dicht bij 'school'
word <- glove_embedding["school", , drop = FALSE] 
cos_sim = sim2(x = glove_embedding, y = word, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)

# wat ligt er dicht bij 'personality'
word <- glove_embedding["personality", , drop = FALSE] 
cos_sim = sim2(x = glove_embedding, y = word, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
```


