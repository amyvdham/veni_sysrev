View(bigrams_data)
# SAVE FILTER_LEMMA
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_lemma <- lemma_clean[, list(freq = .N), by = list(doc_id = doc_id, term = filter_lemma)]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id, term = value)]
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id, term = value)]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id,
term = value)]
# load libraries
library(stringr)
library(udpipe)
library(Matrix)
library(yaml)
library(Rmpfr)
library(topicmodels)
library(udpipe)
library(slam)
library(tidytext)
library(textrank)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# create data frame on which preprocessing will be applied
lemma_clean <- df_check
# add lemma_filter column to dataframe so we keep the original column and filter can be applied on the new column.
lemma_clean$filter_lemma <- df_check$lemma
# SPLIT AT -
library(splitstackshape)
# Split words in lemma column on the -
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "-", direction = "long")
# check results by looking at a term that contains - in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "non-risk", c("lemma", "filter_lemma", "doc_id")]
# SPLIT AT /
# Split words in lemma column on the /
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "/", direction = "long")
# check results by looking at term that contains / in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "police/judicial", c("lemma", "filter_lemma", "doc_id")]
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# check results by looking at term that contains a non-ASCII character in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# remove all the digit characters from words in lemma column
lemma_clean$filter_lemma <- gsub("[0-9]+" ,"", lemma_clean$filter_lemma)
# check results by looking at term that contains a number in the original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# check number of unique words after applying more filters
length(unique(lemma_clean$filter_lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
lemma_clean <- lemma_clean %>%
anti_join(stop_words, by= c("filter_lemma" = "word"))
# check number of unique words after removing stop words
length(unique(lemma_clean$filter_lemma))
exclude_these2 <- unique(unlist(lapply(exclude_terms, grep, x = lemma_clean$filter_lemma)))
lemma_clean <- lemma_clean[-exclude_these2, ]
# check number of unique words after excluding terms
length(unique(lemma_clean$filter_lemma))
# SAVE FILTER_LEMMA
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_lemma <- lemma_clean[, list(freq = .N), by = list(doc_id = doc_id, term = filter_lemma)]
# check number of unique words and documents.
length(unique(nounbydoc_lemma$doc_id))
length(unique(nounbydoc_lemma$term))
# check
# create a data frame with one column including the unique terms
filter_lemma <- unique(nounbydoc_lemma$term)
filter_lemma <- as.data.table(filter_lemma)
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "_")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "_")
# investigating text rank results
stats <- subset(kw_tr$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
library(reshape2)
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id,
term = value)]
# check number of unique words and documents.
length(unique(nounbydoc_bigrams$doc_id))
length(unique(nounbydoc_bigrams$term))
# check
# create a data frame with one column including the unique terms
filter_bigrams <- unique(nounbydoc_bigrams$term)
filter_bigrams <- as.data.table(filter_bigrams)
View(filter_bigrams)
saveRDS(filter_bigrams, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/bigrams_filter.RData")
worcs::git_update()
# load bigrams filter
bigrams_filter <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/bigrams_filter.RData")
# save as .csv
write.csv(bigrams_filter,"/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/bigrams_filter.csv", row.names = FALSE)
worcs::git_update()
# load pretrained word2vec
wrd2vec_embedding <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/pretrained_w2v_filtered.csv", header = FALSE)
# adjust the first column name to word
colnames(wrd2vec_embedding)[1] <- "word"
# check structure of dataframe
str(wrd2vec_embedding)
# remove certain characters from the column V2 which now is column of the type character and contains a string as value.
library(tidyverse)
# create new data frame that can be used for applying adjustments
df_w2vemb <- wrd2vec_embedding
# remove the [ character from V2
df_w2vemb$V2 <-gsub("\\[","",as.character(df_w2vemb$V2))
# remove the ] character from V2
df_w2vemb$V2 <-gsub("\\]","",as.character(df_w2vemb$V2))
# remove the \n character from V2
df_w2vemb$V2 <-gsub("\\\n","",as.character(df_w2vemb$V2))
# check if removing the characters went correctly
df_w2vemb[1,c("word", "V2")]
# split column V2 into multiple columns
library(splitstackshape)
# separate on the space
df_w2vemb <- cSplit(df_w2vemb, "V2", " ")
# retain dimensions of data frame
dim(df_w2vemb)
# check values after splitting
df_w2vemb[1,]
# check how many unique words there are
length(unique(df_w2vemb$word))
# rename the column names of the data frame.
# First column is named word and the other columns dim1-40
colnames(df_w2vemb) <- c("word", paste0("dim", 1:300))
# check if there are any missings
summary(df_w2vemb)
w2v_embedding <- df_w2vemb
glove_embedding <-  readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_final.RData")
glv_embedding <- as.data.frame(glove_embedding)
# check words that are in the glove embedding that are also in the w2v embedding
words_same <- subset(glv_embedding, rownames(glv_embedding) %in% wrd2vec_embedding$word)
# check words that are in the glove embedding but not in the w2vec embedding
words_diff <- subset(glv_embedding, !(rownames(glv_embedding) %in% wrd2vec_embedding$word))
View(words_same)
View(words_diff)
View(wrd2vec_embedding)
View(words_diff)
# check words that are in the w2v embedding but not in the glove embedding
words_in_w2v_notglve <- subset(w2v_embedding, !( word %in% rownames(glv_embedding)))
View(words_in_w2v_notglve)
# retain dimensions of data frame
dim(df_w2vemb)
# load pretrained word2vec with bigrams included
w2v_embedding_bigram <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/pretrained_w2v_filtered_bigrams.csv", header = FALSE)
# adjust the first column name to word
colnames(w2v_embedding_bigram)[1] <- "word"
# check structure of dataframe
str(w2v_embedding_bigram)
# remove certain characters from the column V2 which now is column of the type character and contains a string as value.
library(tidyverse)
# create new data frame that can be used for applying adjustments
df_bigrams <- w2v_embedding_bigram
# remove the [ character from V2
df_bigrams$V2 <-gsub("\\[","",as.character(df_bigrams$V2))
# remove the ] character from V2
df_bigrams$V2 <-gsub("\\]","",as.character(df_bigrams$V2))
# remove the \n character from V2
df_bigrams$V2 <-gsub("\\\n","",as.character(df_bigrams$V2))
# check if removing the characters went correctly
df_bigrams[1,c("word", "V2")]
# split column V2 into multiple columns
library(splitstackshape)
# separate on the space
df_bigrams <- cSplit(df_bigrams, "V2", " ")
# retain dimensions of data frame
dim(df_bigrams)
# check values after splitting
df_bigrams[1,]
# check how many unique words there are
length(unique(df_bigrams$word))
# rename the column names of the data frame.
# First column is named word and the other columns dim1-40
colnames(df_bigrams) <- c("word", paste0("dim", 1:300))
# check if there are any missings
summary(df_bigrams)
# check bigrams embedding
bigrams_embedding <- df_bigrams
View(bigrams_embedding)
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
View(vectors_glove)
vectors_glove[vectors_glove$word == "mental_health", c("word", "dim1")]
View(vectors_glove)
vectors_glove[vectors_glove$word == "and", c("word", "dim_1")]
vectors_glove[vectors_glove$word == "mental", c("word", "dim_1")]
vectors_glove[vectors_glove$word == "mental_health", c("word", "dim_1")]
vectors_glove[vectors_glove$word == "emotional_dysregulation", c("word", "dim_1")]
View(wrd2vec_embedding)
# compare the w2v embeddings in which bigrams are and are not included
# check words that are in the w2v bigrams embedding but not in the w2c embedding
words_diff_w2v <- subset(bigrams_embedding, !(rownames(bigrams_embedding) %in% wrd2vec_embedding$word))
View(words_diff_w2v)
# compare the w2v embeddings in which bigrams are and are not included
# check words that are in the w2v bigrams embedding but not in the w2c embedding
words_diff_w2v <- subset(bigrams_embedding, !(rownames(bigrams_embedding) %in% wrd2vec_embedding$word))
View(words_diff_w2v)
View(wrd2vec_embedding)
View(bigrams_embedding)
# compare the w2v embeddings in which bigrams are and are not included
# check words that are in the w2v bigrams embedding but not in the w2c embedding
words_diff_w2v <- subset(bigrams_embedding, !(word %in% wrd2vec_embedding$word))
View(words_diff_w2v)
10068-209
View(words_diff)
w2v_embedding <- df_w2vemb %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
View(w2v_embedding)
# convert dataframe to a matrix
w2v_embedding <- as.matrix(w2v_embedding)
str(glove_embedding)
str(w2v_embedding)
# save the glove embedding to which the filter is applied so this can be easily loaded into other scripts
saveRDS(w2v_embedding, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/w2v_embedding.RData")
w2v_bigrams_embedding <- df_bigrams %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
View(w2v_bigrams_embedding)
# convert dataframe to a matrix
w2v_bigrams_embedding <- as.matrix(w2v_bigrams_embedding)
str(w2v_bigrams_embedding)
# save the glove embedding to which the filter is applied so this can be easily loaded into other scripts
saveRDS(w2v_bigrams_embedding, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/w2v_bigrams_embedding.RData")
# load glove word embedding file
glove_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_final.RData")
str(glove_embedding)
# load glove word embedding file
w2v_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/w2v_embedding.RData")
# load glove word embedding file
w2v_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/w2v_embedding.RData")
str(glove_embedding)
str(w2v_embedding)
# load glove word embedding file
w2v_bigrams_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/w2v_bigrams_embedding.RData")
# load glove word embedding file
w2v_bigrams_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/w2v_bigrams_embedding.RData")
str(w2v_bigrams_embedding)
library(text2vec)
# create function to find similar words based on cosine distance
find_similar_words <- function(word, embedding_matrix, n = 5) {
similarities <- embedding_matrix[word, , drop = FALSE] %>%
sim2(embedding_matrix, y = ., method = "cosine")
similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}
# Run function on the word health and see the 25 closest words based on cosine similarity.
find_similar_words("white",glove_embedding,50)
find_similar_words("white",w2v_embedding,50)
find_similar_words("white",w2v_bigrams_embedding,50)
View(glove_embedding)
glove_embedding["monocultural", ]
find_similar_words("mental_health",w2v_bigrams_embedding,50)
# fit the k-means clustering with 75 clusters, glove
kmeans_fit75 <- kmeans(glove_embedding, 75, iter.max = 30, nstart = 25)
# fit the k-means clustering with 75 clusters, w2v
kmeans_fit75_w2v <- kmeans(w2v_embedding, 75, iter.max = 30, nstart = 25)
# fit the k-means clustering with 75 clusters, w2v bigrams
kmeans_fit75_bigr <- kmeans(w2v_bigrams_embedding, 75, iter.max = 30, nstart = 25)
# results
kmeans_fit75
# 25.2%
kmeans_fit75_w2v
# 20.2%
k_means_fit75_bigr
# 20.2%
kmeans_fit75_bigr
# Create data frame in which the cluster assignment is merged back to rows/word.
words_with_cluster75 <- as.data.frame(cbind(row.names(glove_embedding), kmeans_fit75$cluster))
# add column names
names(words_with_cluster) <- c("word", "kmeans75")
# add column names
names(words_with_cluster75) <- c("word", "kmeans75")
# Create data frame in which the cluster assignment is merged back to rows/word.
words_with_cluster75_w2v <- as.data.frame(cbind(row.names(w2v_embedding), kmeans_fit75_w2v$cluster))
# add column names
names(words_with_cluster75_w2v) <- c("word", "kmeans75")
# Create data frame in which the cluster assignment is merged back to rows/word.
words_with_cluster75_w2v_bigr <- as.data.frame(cbind(row.names(w2v_bigrams_embedding), kmeans_fit75_bigr$cluster))
# add column names
names(words_with_cluster_w2v_bigr) <- c("word", "kmeans75")
# add column names
names(words_with_cluster75_w2v_bigr) <- c("word", "kmeans75")
# Mother
# find out in which cluster the word mother is assigned
words_with_cluster[words_with_cluster75$word == "mental", ]
# Mother
# find out in which cluster the word mother is assigned
words_with_cluster75[words_with_cluster75$word == "mental", ]
words_with_cluster75_w2v[words_with_cluster75_w2v$word == "mental", ]
words_with_cluster75_w2v_bigr[words_with_cluster75_w2v_bigr$word == "mental", ]
cluster_mental_glv <- subset(words_with_cluster75, subset=kmeans75 == 9)
cluster2_mental_w2v <- subset(words_with_cluster75_w2v, subset=kmeans75 == 8)
cluster3_mental_w2v_bigr <- subset(words_with_cluster75_w2v_bigr, subset=kmeans75 == 34)
View(cluster_mental_glv)
View(cluster2_mental_w2v)
View(cluster3_mental_w2v_bigr)
# find out in which cluster the word mother is assigned
words_with_cluster75[words_with_cluster75$word == "mother", ]
words_with_cluster75_w2v[words_with_cluster75_w2v$word == "mother", ]
words_with_cluster75_w2v_bigr[words_with_cluster75_w2v_bigr$word == "mother", ]
cluster_mental_glv <- subset(words_with_cluster75, subset=kmeans75 == 74)
cluster2_mental_w2v <- subset(words_with_cluster75_w2v, subset=kmeans75 == 19)
cluster3_mental_w2v_bigr <- subset(words_with_cluster75_w2v_bigr, subset=kmeans75 == 43)
cluster_mother_glv <- subset(words_with_cluster75, subset=kmeans75 == 74)
cluster_mother_w2v <- subset(words_with_cluster75_w2v, subset=kmeans75 == 19)
cluster_mother_w2v_bigr <- subset(words_with_cluster75_w2v_bigr, subset=kmeans75 == 43)
View(cluster_mental_glv)
View(cluster_mother_glv)
View(cluster_mother_w2v)
View(cluster_mother_w2v_bigr)
cluster_mental_w2v_bigr <- subset(words_with_cluster75_w2v_bigr, subset=kmeans75 == 34)
View(cluster_mental_w2v_bigr)
# load glove vectors into R
vectors <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
vectors[vectors$word == "mental-health", ]
View(vectors)
# rename the columns
colnames(vectors) <- c('word',paste('dim',1:300,sep = '_'))
vectors[vectors$word == "risk-factor", ]
vectors[vectors$word == "mental-health", ]
# mental-health
vectors[142239, ]
vectors[vectors$word == "emotional-dysregulation", ]
vectors[vectors$word == "emotional-dysregulation", ]
# load libraries
library(stringr)
library(udpipe)
library(Matrix)
library(yaml)
library(Rmpfr)
library(topicmodels)
library(udpipe)
library(slam)
library(tidytext)
library(textrank)
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# create data frame on which preprocessing will be applied
lemma_clean <- df_check
# add lemma_filter column to dataframe so we keep the original column and filter can be applied on the new column.
lemma_clean$filter_lemma <- df_check$lemma
# SPLIT AT -
library(splitstackshape)
# Split words in lemma column on the -
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "-", direction = "long")
# check results by looking at a term that contains - in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "non-risk", c("lemma", "filter_lemma", "doc_id")]
# SPLIT AT /
# Split words in lemma column on the /
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "/", direction = "long")
# check results by looking at term that contains / in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "police/judicial", c("lemma", "filter_lemma", "doc_id")]
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# check results by looking at term that contains a non-ASCII character in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# remove all the digit characters from words in lemma column
lemma_clean$filter_lemma <- gsub("[0-9]+" ,"", lemma_clean$filter_lemma)
# check results by looking at term that contains a number in the original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# check number of unique words after applying more filters
length(unique(lemma_clean$filter_lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
lemma_clean <- lemma_clean %>%
anti_join(stop_words, by= c("filter_lemma" = "word"))
# check number of unique words after removing stop words
length(unique(lemma_clean$filter_lemma))
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
exclude_these2 <- unique(unlist(lapply(exclude_terms, grep, x = lemma_clean$filter_lemma)))
lemma_clean <- lemma_clean[-exclude_these2, ]
# check number of unique words after excluding terms
length(unique(lemma_clean$filter_lemma))
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "_")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "_")
# investigating text rank results
stats <- subset(kw_tr$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
library(reshape2)
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
kw_tr_glove <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "-")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr_glove$keywords$keyword, ngram = kw_tr_glove$keywords$ngram, sep = "_")
# investigating text rank results
stats <- subset(kw_tr_glove$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
View(bigrams_data)
View(bigrams_data)
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr_glove$keywords$keyword, ngram = kw_tr_glove$keywords$ngram, sep = "-")
# investigating text rank results
stats <- subset(kw_tr_glove$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
View(bigrams_data)
# SAVE filter with bigrams seperated with - instead of _ to apply on glove embedding
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams_glove <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id,
term = value)]
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# create data frame on which preprocessing will be applied
lemma_clean <- df_check
# add lemma_filter column to dataframe so we keep the original column and filter can be applied on the new column.
lemma_clean$filter_lemma <- df_check$lemma
# Split words in lemma column on the -
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "-", direction = "long")
# check results by looking at a term that contains - in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "non-risk", c("lemma", "filter_lemma", "doc_id")]
# SPLIT AT /
# Split words in lemma column on the /
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "/", direction = "long")
# check results by looking at term that contains / in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "police/judicial", c("lemma", "filter_lemma", "doc_id")]
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# check results by looking at term that contains a non-ASCII character in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# remove all the digit characters from words in lemma column
lemma_clean$filter_lemma <- gsub("[0-9]+" ,"", lemma_clean$filter_lemma)
# check results by looking at term that contains a number in the original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# check number of unique words after applying more filters
length(unique(lemma_clean$filter_lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
lemma_clean <- lemma_clean %>%
anti_join(stop_words, by= c("filter_lemma" = "word"))
# check number of unique words after removing stop words
length(unique(lemma_clean$filter_lemma))
exclude_these2 <- unique(unlist(lapply(exclude_terms, grep, x = lemma_clean$filter_lemma)))
lemma_clean <- lemma_clean[-exclude_these2, ]
# check number of unique words after excluding terms
length(unique(lemma_clean$filter_lemma))
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "_")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "_")
# investigating text rank results
stats <- subset(kw_tr$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id,
term = value)]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id,
term = value)]
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id,
term = value)]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id,
term = value)]
