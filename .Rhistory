df_plot$Word <- ordered(df_plot$Word, levels = df_plot$Word[order(df_plot$Frequency)])
cat_cols <- c(Outcome = "gray50", Indicator = "tomato", Cause = "gold", Protective = "forestgreen")
df_plot$cat <- ordered(df_plot$cat, levels = c("Outcome", "Indicator", "Cause", "Protective"))
# p <- ggplot(df_plot, aes(y = Word, x = Frequency)) +
#   geom_segment(aes(x = 0, xend = Frequency,
#                           y = Word, yend = Word), colour = "grey50",
#                linetype = 2) + geom_vline(xintercept = 0, colour = "grey50",
#                                           linetype = 1) + xlab("Word frequency") +
#   geom_point(aes(fill = cat), shape = 21, size = 2) +
#   geom_text(aes(label = Word), x = -850, hjust=0, vjust= 0, size = 2) +
#   scale_fill_manual(values = c(Outcome = "gray50", Indicator = "tomato", Cause = "gold", Protective = "forestgreen")) +
#   scale_x_continuous(limits = c(-850, (max(df_plot$Frequency)+1)))+
#   scale_y_discrete(expand = c(.015,.01))+
#   theme_bw() + theme(panel.grid.major.x = element_blank(),
#                      panel.grid.minor.x = element_blank(), axis.title.y = element_blank(),
#                      legend.position = c(.70,.125),
#                      legend.title = element_blank(),
#                      axis.text.y = element_blank(),
#                      axis.ticks.y = element_blank())
df_plot$Word
write_yaml(df_plot$Word, "s1_words.yml")
library(data.table)
#library(bibliometrix)
#library(yaml)
library(stringr)
#library(lattice)
#library(topicmodels)
library(udpipe)
library(igraph)
library(wordcloud)
library(Matrix)
library(yaml)
library(Rmpfr)
library(topicmodels)
library(udpipe)
library(slam)
library(tidytext)
library(ggplot2)
library(textrank)
source("word_functions.R")
run_everything <- FALSE
## Look at POS tags?
recs <- data.table(read.csv("recs_final.csv"))
recs[, "doc" := 1:nrow(recs)]
recs$AB <- tolower(recs$AB)
# Detect languages not necessary; only one abstract is (partly) in Spanish.
#library(cld3)
#languages <- cld3::detect_language(recs$AB)
#table(languages)
#recs$AB[languages == "es"]
if(!file.exists("english-ewt-ud-2.4-190531.udpipe")) {
ud_model <- udpipe_download_model(language = "english")
} else {
ud_model <- udpipe_load_model("english-ewt-ud-2.4-190531.udpipe")
ud_model <- udpipe_load_model(ud_model$file)
}
if(run_everything){
udp_res <- udpipe_annotate(ud_model, x = recs$AB, doc_id = recs$doc)
df <- as.data.table(udp_res)
saveRDS(df, "study2_df.RData")
} else {
df <- readRDS("study2_df.RData")
}
# LDA analysis ------------------------------------------------------------
if(run_everything){
# Functions:
harmonicMean <- function(logLikelihoods, precision = 2000L) {
llMed <- median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
BIC <- function(ll, p, n){
-2 * ll + p * log(n)
}
entropy <- function(post_prob){
1 + (1/(nrow(post_prob) *
log(ncol(post_prob)))) * (sum(rowSums(post_prob *
log(post_prob + 1e-12))))
}
# Preprocessing -----------------------------------------------------------
# Frequency of word by doc
nounbydoc <- df[df$upos %in% c("NOUN", "ADJ"), list(freq = .N), by = list(doc_id = doc_id, term = lemma)]
df_lda <- nounbydoc #[nounbydoc$term %in% names(dict), ]
df_lda <- df_lda %>%
bind_tf_idf(term, doc_id, freq)
summary(df_lda$tf_idf)
select_words <- df_lda[!duplicated(df_lda$term), ]
select_words <- select_words$term[select_words$tf_idf >= median(select_words$tf_idf)]
df_lda <- df_lda[df_lda$term %in% select_words, ]
dtm <- udpipe::document_term_matrix(document_term_frequencies(df_lda))
yaml::write_yaml(dim(dtm), file = "Study2_lda_dims.txt")
# Build topic models
seqk <- seq(2, 20, 1)
burnin <- 1000
iter <- 1000
keep <- 50
set.seed(44773)
res_lda <- lapply(seqk, function(k) {
topicmodels::LDA(
dtm,
k = k,
method = "Gibbs",
control = list(
burnin = burnin,
iter = iter,
keep = keep
)
)
})
ll <- sapply(res_lda, function(x){harmonicMean(x@logLiks[-c(1:(burnin/keep))])})
K = seqk
N = nrow(dtm)
M = ncol(dtm)
parameters <- K*(M-1)+N*(K-1)
N <- nrow(dtm)
bics <- BIC(ll, parameters, N)
entropies <- sapply(res_lda, function(x){entropy(x@gamma)})
p <- ggplot(data.frame(K = seqk, Entropy = entropies), aes(x = K, y = Entropy)) + geom_path() +
xlab('Number of topics') +
scale_y_continuous(limits = c(0,1)) +
theme_bw()
ggsave("study2_entropies.png", p, device = "png")
ggsave("study2_entropies.svg", p, device = "svg")
p <- ggplot(data.frame(K = seqk, ll = ll), aes(x = K, y = ll)) + geom_path() +
geom_vline(xintercept = (which.max(ll)+1), linetype = 2) +
xlab('Number of topics') +
geom_smooth(method = "lm", formula = y~log(x), se = FALSE)+
theme_bw()
ggsave("study2_ll.png", p, device = "png")
ggsave("study2_ll.svg", p, device = "svg")
p <- ggplot(data.frame(K = seqk, BIC = bics), aes(x = K, y = BIC)) + geom_path() +
geom_vline(xintercept = (which.min(bics)+1), linetype = 2) +
xlab('Number of topics') +
geom_smooth(method = "lm", formula = y~x, se = FALSE)+
theme_bw()
ggsave("study2_BIC.png", p, device = "png")
ggsave("study2_BIC.svg", p, device = "svg")
}
# Keyword extraction ------------------------------------------------------
# Exclude words
if(run_everything){
df_kw <- df[upos %in% c("NOUN", "ADJ"), ]
df_kw <- df_kw[grepl("^[a-zA-Z].", df_kw$lemma), ]
exclude_terms <- readLines("exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df_kw$lemma)))
df_kw <- df_kw[-exclude_these, ]
saveRDS(df_kw, "study2_df_kw.RData")
} else {
df_kw <- readRDS("study2_df_kw.RData")
}
# No numeric values
# all(is.na(as.numeric(df_kw$lemma)))
#df_kw$lemma[nchar(df_kw$lemma) == 3]
if(run_everything){
kw_tr <- textrank_keywords(x = df_kw$lemma[df_kw$upos %in% c("NOUN", "ADJ")],
ngram_max = 3, sep = " ")
saveRDS(kw_tr, "study2_textrank.RData")
} else {
kw_tr <- readRDS("study2_textrank.RData")
}
# Merge back with original data
df_kw$keyword <- txt_recode_ngram(df_kw$lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = " ")
df_kw$keyword[!df_kw$keyword %in% kw_tr$keywords$keyword] <- NA
df_analyze <- df_kw[!is.na(df_kw$keyword), ]
if(run_everything){
dict <- read_yaml("yaml_dict.txt")
res_cat <- cat_words(df_analyze$keyword, dict, handle_dups = "all")
# Check coding issues
#res_cat$dup
#head(res_cat$unmatched)
df_analyze <- merge_df(df_analyze, res_cat$words, "word_coded")
saveRDS(df_analyze, "study2_df_analyze.RData")
} else {
df_analyze <- readRDS("study2_df_analyze.RData")
}
# Wordcloud ---------------------------------------------------------------
# Frequency of word by doc
nounbydoc <- df_analyze[, list(freq = .N), by = list(doc_id = doc_id, term = word_coded)]
nounbydoc$freq <- 1
dtm <- udpipe::document_term_matrix(document_term_frequencies(nounbydoc))
topterms <- colSums(dtm)
topterms <- sort(topterms, decreasing = TRUE)
# Select most common terms ------------------------------------------------
set.seed(720)
dtm_top <- dtm[, select_words(dtm, .975)]
dtm_top <- dtm_top[rowSums(dtm_top) > 0, ]
dim(dtm_top)
#topterms <- topterms[topterms > .005*nrow(recs)]
#which_topterms <- head(topterms, 250)
#which_topterms <- names(which_topterms)
#dtm_top <- dtm[, which_topterms]
#dtm_top <- dtm_top[rowSums(dtm_top) > 0, ]
#dim(dtm_top)
# Wordcloud ---------------------------------------------------------------
## Word frequencies
topterms <- colSums(dtm_top)
topterms <- sort(topterms, decreasing = TRUE)
word_freq <- data.frame(Word = names(topterms), Frequency = topterms, row.names = NULL)
write.csv(word_freq, "study2_word_freq.csv", row.names = FALSE)
df_plot <- word_freq
df_plot$Word <- pretty_words(df_plot$Word)
df_plot$Frequency <- sqrt(df_plot$Frequency)
## Visualise them with wordclouds
p <- quote({
set.seed(46)
wordcloud(words = df_plot$Word, freq = df_plot$Frequency, scale = c(4,.4), max.words = 150, rot.per = 0,  random.order = FALSE, colors = brewer.pal(8, "Dark2"))
})
svg("study2_wordcloud.svg")
eval(p)
dev.off()
png("study2_wordcloud.png")
eval(p)
dev.off()
# Feature importance ------------------------------------------------------
topterms <- colSums(dtm_top)
word_freq <- data.frame(Word = names(topterms), Frequency = topterms, row.names = NULL)
df_plot <- word_freq
categ <- read.csv("study1_categorization.csv", stringsAsFactors = FALSE)
df_plot$cat <- categ$category[match(df_plot$Word, categ$name)]
df_plot$Word <- pretty_words(df_plot$Word)
df_plot <- df_plot[order(df_plot$Frequency, decreasing = TRUE), ]
df_plot$Word <- ordered(df_plot$Word, levels = df_plot$Word[order(df_plot$Frequency)])
cat_cols <- c(Outcome = "gray50", Indicator = "tomato", Cause = "gold", Protective = "forestgreen")
df_plot$cat <- ordered(df_plot$cat, levels = c("Outcome", "Indicator", "Cause", "Protective"))
write_yaml(df_plot$Word, "s2_words.yml")
s1 <- read_yaml("s1_words.yml")
s2 <- read_yaml("s2_words.yml")
adist(s1, s2)
s1
s1 <- read_yaml("s1_words.yml")
s2 <- read_yaml("s2_words.yml")
library(yaml)
s1 <- read_yaml("s1_words.yml")
s2 <- read_yaml("s2_words.yml")
all(s2 %in% s1)
length(s2[!s2 %in% s1])
length(s1[!s1 %in% s2])
length(s1[!s1 %in% s2])
length(s2[!s2 %in% s1])
length(s1[!s1 %in% s2])/length(s1)
length(s2[!s2 %in% s1])length(s2)
length(s2[!s2 %in% s1])/length(s2)
length(unique(c(s1, s2)))
length(s1[!s1 %in% s2])/length(s1)
length(s2[!s2 %in% s1])/length(s2)
length(s1[s1 %in% s2])/length(s1)
length(s2[s2 %in% s1])/length(s2)
(3557 x 2700)/8
(3557 * 2700)/8
(3557 * 3100)/8
(3557 * 3100)
(3557 * 3100)/8000
(3557 * 2700)/8000
(2000 * 2700)/8000
P = 3000
L = 2700
M <- (P * L)/8
M
E <- 210000 # Voor staal
#M = Moment [Nm]
#f = Doorbuiging [mm]
P = 3000
Q = Verdeelde belasting [N/m]
L = 2700
# E = Elasticiteitsmodulus [N/mm2 of MPa]
E <- 210000 # Voor staal
#Oppervlaktetraagheidsmoment [mm4]
I = 31681.93554822
# Maximaal moment
M <- (P * L)/8
# Maximale doorbuiging
f <- (P * L^3)/(192 * E * I)
f
#M = Moment [Nm]
#f = Doorbuiging [mm]
P = 1500
#Q = Verdeelde belasting [N/m]
L = 2700
# E = Elasticiteitsmodulus [N/mm2 of MPa]
E <- 210000 # Voor staal
#Oppervlaktetraagheidsmoment [mm4]
I = 31681.93554822
# Maximaal moment
M <- (P * L)/8
# Maximale doorbuiging
f <- (P * L^3)/(192 * E * I)
f
#M = Moment [Nm]
#f = Doorbuiging [mm]
P = 750
#Q = Verdeelde belasting [N/m]
L = 2700
# E = Elasticiteitsmodulus [N/mm2 of MPa]
E <- 210000 # Voor staal
#Oppervlaktetraagheidsmoment [mm4]
I = 31681.93554822
# Maximaal moment
M <- (P * L)/8
# Maximale doorbuiging
f <- (P * L^3)/(192 * E * I)
f
P = 3000
#Q = Verdeelde belasting [N/m]
L = 2700
# E = Elasticiteitsmodulus [N/mm2 of MPa]
E <- 210000 # Voor staal
#Oppervlaktetraagheidsmoment [mm4]
I = 31681.93554822 # 33mm
I = 65651.26212062 # 42mm
# Maximaal moment
M <- (P * L)/8
# Maximale doorbuiging
f <- (P * L^3)/(192 * E * I)
f
#M = Moment [Nm]
#f = Doorbuiging [mm]
P = 750
#Q = Verdeelde belasting [N/m]
L = 2700
# E = Elasticiteitsmodulus [N/mm2 of MPa]
E <- 210000 # Voor staal
#Oppervlaktetraagheidsmoment [mm4]
I = 31681.93554822 # 33mm
I = 65651.26212062 # 42mm
# Maximaal moment
M <- (P * L)/8
# Maximale doorbuiging
f <- (P * L^3)/(192 * E * I)
f
s1 <- read_yaml("s1_words.yml")
s2 <- read_yaml("s2_words.yml")
length(unique(c(s1, s2)))
length(s1[s1 %in% s2])/length(s1)
length(s2[s2 %in% s1])/length(s2)
library("papaja")
library(bibliometrix)
library(revtools)
library(bib2df)
library(knitr)
library(kableExtra)
library(cowplot)
run_everything = FALSE
s1 <- read_yaml("s1_words.yml")
s2 <- read_yaml("s2_words.yml")
length(unique(c(s1, s2)))
oneintwo <- length(s1[s1 %in% s2])/length(s1)
twoinone <- length(s2[s2 %in% s1])/length(s2)
oneintwo
twoinone
oneintwo <- round(length(s1[s1 %in% s2])/length(s1)*100)
twoinone <- round(length(s2[s2 %in% s1])/length(s2)*100)
oneintwo
library(cowplot)
fig_svg <- cowplot::ggdraw()+cowplot::draw_image("study1_network1.svg")
plot(fig_svg)
library("papaja")
library(bibliometrix)
library(revtools)
library(bib2df)
library(knitr)
library(kableExtra)
library(cowplot)
run_everything = FALSE
knitr::include_graphics("s2_varimp.png")
cowplot::ggdraw()+cowplot::draw_image("s2_varimp.png")
cowplot::ggdraw()+cowplot::draw_image("s1_varimp.png")
table(read.table("clipboard", sep = "\n"))
6/100
1000/6
160/30
(1/3)*240
(1/3)*80
(1/3)*188
library("papaja")
library(bibliometrix)
library(revtools)
library(bib2df)
library(knitr)
library(kableExtra)
library(cowplot)
run_everything = FALSE
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
filenam <- "recs_6653.RData"
if(!file.exists(filenam)){
f <- list.files("./recs/18-3-2020/", full.names = TRUE)
f <- readFiles(f)
recs <- convert2df(f)
saveRDS(recs, filenam)
} else {
recs <- readRDS(filenam)
}
filenam <- "unique_recs.RData"
if(!file.exists(filenam)){
recs$doi <- recs$DI
recs$doi[is.na(recs$doi)] <- paste0("fake", 1:sum(is.na(recs$doi)))
if(any(duplicated(recs$doi))){
recs$doi[duplicated(recs$doi)|duplicated(recs$doi, fromLast = T)]
dup_doi <- duplicated(recs$doi)
recs <- recs[!dup_doi, ]
}
recs$title <- recs$TI
# locate and extract unique references
potential_dups <- find_duplicates(recs, match_variable = "title", match_function = "stringdist", threshold = 5, to_lower = TRUE, remove_punctuation = TRUE)
unique_recs <- extract_unique_references(recs, matches = potential_dups)
saveRDS(unique_recs, filenam)
write.csv(data.frame(dup_doi = sum(dup_doi), dup_title = (nrow(recs)-nrow(unique_recs))-sum(dup_doi)), "dups.csv", row.names = FALSE)
} else {
unique_recs <- readRDS(filenam)
dups <- read.csv("dups.csv")
}
unique_recs$id_num <- 1:nrow(unique_recs)
if(FALSE){
# df_screen <- unique_recs[, c("id_num", "TI", "AB")]
# names(df_screen) <- c("id_num", "title", "abstract")
# write.csv(df_screen, file = "asreview.csv", row.names = FALSE, fileEncoding = "UTF-8")
df_screen <- unique_recs[, c("id_num", "TI", "AU", "SO", "SN", "VL", "IS", "BP", "PY", "PU", "DI", "AB")]
names(df_screen) <- c("key", "title", "authors", "journal", "issn", "volume", "issue", "pages", "year", "publisher", "url", "abstract")
write.csv(df_screen, file = "rayyan.csv", row.names = FALSE)
}
rayyan_res <- read.csv("rayyan_exports/articles.csv")
rayyan_dups <- nrow(unique_recs) - nrow(rayyan_res)
screen <- read.csv("asreview_result_sysrevemotprob.csv")
screen <- screen[screen$key %in% rayyan_res$key, ]
screen$drop_these <- FALSE
screen$drop_these[which(screen$included == 1 | screen$rayyan == "False")] <- TRUE
if(!file.exists("recs_final.csv")){
recs_final <- unique_recs[screen$key[!screen$drop_these], ]
write.csv(recs_final, "recs_final.csv", row.names = FALSE)
} else {
recs_final <- read.csv("recs_final.csv")
}
library(tidySEM)
lo <- matrix(c("start", "dedup", "rayyan", "asreview", "included",
"", "dedup2", "excluded", "", ""), nrow = 5, ncol = 2)
nodes <- data.frame(
name = c("start", "dedup", "dedup2", "rayyan", "asreview", "included", "excluded"),
label = c(
paste0("Records identified through\nWeb of Science (n = ", nrow(recs), ")"),
paste0("After duplicates removed\n(n = ", nrow(unique_recs), ")"),
paste0("Duplicates\n(n = ", nrow(recs)-nrow(screen), ")"),
paste0("Articles screened for eligibility\nusing Rayyan (n = ", nrow(screen), ")"),
paste0("Articles screened for eligibility\nusing ASReview (n = ", nrow(screen), ")"),
paste0("Studies included in\nqualitative synthesis (n = ", nrow(recs_final), ")"),
paste0("Excluded studies (n = ", nrow(screen)-nrow(recs_final), ")"))
)
edges <- data.frame(
from = c("start", "dedup", "rayyan", "asreview",
"start", "rayyan", "rayyan", "asreview"),
to = c("dedup", "rayyan", "asreview", "included",
"dedup2", "dedup2", "excluded", "excluded"),
label = c("",   "", paste0("n = ", sum(screen$rayyan == "True")), paste0("n = ", sum(screen$included == 0, na.rm = TRUE)-sum(screen$rayyan == "True")),
paste0("n = ", sum(dups)),
paste0("n = ", rayyan_dups),
paste0("n = ", sum(screen$rayyan == "False")),
paste0("n = ", sum(screen$included == 1, na.rm = TRUE)-sum(screen$rayyan == "False"))
)
)
p <- graph_sem(layout = lo, nodes = nodes, edges = edges, angle = 1, rect_height = 1.2, rect_width = 1.3)
ggsave("prismachart.png", p, device = "png")
ggsave("prismachart.svg", p, device = "svg")
knitr::include_graphics("prismachart.png")
source("study1.R")
if(run_everything | !file.exists("Study1_lda_dims.txt")) source("LDA_analysis.R")
lda_dims <- read_yaml("Study1_lda_dims.txt")
knitr::include_graphics("s1_varimp.png")
#fig_svg <- cowplot::ggdraw()+cowplot::draw_image("s1_varimp.svg")
#plot(fig_svg)
#knitr::include_graphics("study1_network1.svg")
library(cowplot)
fig_svg <- cowplot::ggdraw()+cowplot::draw_image("study1_network1.svg")
plot(fig_svg)
notingraph <- names(topterms)[!names(topterms) %in% unique(c(edg$term1, edg$term2))]
notingraph <- categ[categ$name %in% notingraph, ]
notingraph <- tapply(tolower(pretty_words(notingraph$name)), notingraph$category, function(i){paste0(paste(paste0("*", i[-length(i)], "*"), collapse = ", "), ", and *", i[length(i)], "*")})
source("study2.R")
if(run_everything | !file.exists("Study2_lda_dims.txt")) source("LDA_analysis.R")
lda_dims_2 <- read_yaml("Study2_lda_dims.txt")
knitr::include_graphics("s2_varimp.png")
# fig_svg <- cowplot::ggdraw()+cowplot::draw_image("s1_varimp.svg")
# plot(fig_svg)
#knitr::include_graphics("study2_network1.svg")
library(cowplot)
fig_svg <- cowplot::ggdraw()+cowplot::draw_image("study2_network1.svg")
plot(fig_svg)
notingraph <- names(topterms)[!names(topterms) %in% unique(c(edg$term1, edg$term2))]
notingraph <- categ[categ$name %in% notingraph, ]
notingraph <- tapply(tolower(pretty_words(notingraph$name)), notingraph$category, function(i){paste0(paste(paste0("*", i[-length(i)], "*"), collapse = ", "), ", and *", i[length(i)], "*")})
notingraph <- names(topterms)[!names(topterms) %in% unique(c(edg$term1, edg$term2))]
notingraph <- categ[categ$name %in% notingraph, ]
notingraph <- tapply(tolower(pretty_words(notingraph$name)), notingraph$category, function(i){paste0(paste(paste0("*", i[-length(i)], "*"), collapse = ", "), ", and *", i[length(i)], "*")})
notingraph["Cause"]
topterms
tt <- sort(topterms, decreasing = TRUE)
tt
tt <- sort(topterms, decreasing = TRUE)
notingraph <- names(tt)[!names(tt) %in% unique(c(edg$term1, edg$term2))]
notingraph <- categ[categ$name %in% notingraph, ]
notingraph <- tapply(tolower(pretty_words(notingraph$name)), notingraph$category, function(i){paste0(paste(paste0("*", i[-length(i)], "*"), collapse = ", "), ", and *", i[length(i)], "*")})
notingraph
tt
notingraph["Cause"]
git_update("Rewrite theoretical section")
worcs::git_update("Rewrite theoretical section")
1900/17000000
install.packages("crosstalk")
install.packages("carData")
install.packages("car")
9*50
.09*50
((.09*50)*5*4)*11
