"
worcs:::cat(cleancitations(string))
string <- "Optional reference: @@reference2020
```{r}
myiris <- read.csv('iris.csv')
```
"
worcs:::cleancitations(string)
library(data.table)
library(bibliometrix)
install.packages("bibliometrix")
source("word_functions.R")
exclude_terms <- readLines("exclude_terms.txt")
dict_synon <- dget("dictionary_synonyms.txt")
recs <- readRDS("recs_6653.RData")
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
df <- data.table(word = tolower(unlist(df)), doc = rep(1:length(df), times = sapply(df, length)))
table(grep("disease", df$word, value = T))
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
df <- df[!exclude_these, ]
res_cat <- cat_words(df$word, dict_synon)
head(res_cat$unmatched)
res_cat$dup
res_cat <- cat_words(df$word, dict_synon)
df[, word := res_cat$words]
df[, id := paste0(word, doc)]
df <- df[!duplicated(id), ]
df[, id := NULL]
library(lattice)
# What are these texts about?
library(topicmodels)
library(udpipe)
library(slam)
install.packages("topicmodels", dependencies = TRUE)
library(lattice)
# What are these texts about?
library(topicmodels)
library(udpipe)
library(slam)
install.packages("modeltools", dependencies = TRUE)
library(lattice)
# What are these texts about?
library(topicmodels)
library(udpipe)
library(slam)
install.packages("udpip3", dependencies = TRUE)
install.packages("udpipe", dependencies = TRUE)
install.packages("slam", dependencies = TRUE)
library(lattice)
# What are these texts about?
library(topicmodels)
library(udpipe)
library(slam)
nounbydoc <- df[, list(freq = .N), by = list(document = doc, term = word)]
dtm <- udpipe::document_term_matrix(document_term_frequencies(nounbydoc)) # document_frequency_matrix(x)
topterms <- col_sums(dtm)
topterms <- sort(topterms, decreasing = TRUE)
hist(topterms, breaks = 100)
tt <- table(topterms)
plot(1:10, tt[1:10], type = "l")
topterms <- topterms[topterms > .005*nrow(recs)]
topterms <- head(topterms, 250)
topterms <- names(topterms)
topterms
dtm_top <- dtm[, topterms]
dtm_top <- dtm_top[row_sums(dtm_top) > 0, ]
set.seed(43892)
topics <- LDA(x = dtm_top, k = 5, method = "VEM", control = list(alpha = 0.1, estimate.alpha = TRUE, seed = as.integer(10:1), verbose = FALSE, nstart = 10, save = 0, best = TRUE))
# What topics are there?
topic_terms <- predict(topics, type = "terms", min_posterior = 0.01)
topic_terms
scores <- predict(topics, newdata = dtm[, topterms], type = "topics")
# How many articles about each topic?
table(scores$topic)
word_freq <- df[, list(n = .N), by = list(word)]
word_freq <- word_freq[order(word_freq$n, decreasing = TRUE), ]
word_freq <- as.data.frame(word_freq)
## Visualise them with wordclouds
library(wordcloud)
wordcloud(words = word_freq$word, freq = word_freq$n, max.words = 150, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
install.packages("RColorBrewer")
library(wordcloud)
wordcloud(words = word_freq$word, freq = word_freq$n, max.words = 150, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
install.packages("wordcloud")
library(wordcloud)
wordcloud(words = word_freq$word, freq = word_freq$n, max.words = 150, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
library(ggraph)
library(ggforce)
library(igraph)
library(tidytext)
install.package("tidytext")
install.packages("tidytext")
library(remotes)
install_github("juliasilge/tidytext")
install_github("juliasilge/tidytext")
install_github("juliasilge/tidytext", upgrade = "never")
worcs::git_update("Synchronize remote")
worcs::worcs_project("c:/git_repositories/veni_sysrev", manuscript = "none", preregistration = "none")
install.packages("topicmodels")
renv::snapshot()
(file.exists(file.path(path, "README.md")))
path = normalizePath(".")
path
(file.exists(file.path(path, "README.md")))
cont <- readLines(file.path(path, "README.md"), encoding = "UTF-8")
f <- list.files(path)
tab <- matrix(c("File", "Description", "Usage",
"README.md", "Description of project", "Human editable"), nrow = 2, byrow = TRUE)
rproj_name <- paste0(gsub("^.+\\\\b(.+)$", "\\\\1", path), ".Rproj")
cont[grep("You can load this project in Rstudio by opening the file called ", cont)] <- paste0(grep("You can load this project in Rstudio by opening the file called ", cont, value = TRUE), "'", rproj_name, "'.")
tab <- rbind(tab, c(rproj_name, "Project file", "Loads project"))
tab <- describe_file("LICENSE", "User permissions", "Read only", tab, path)
tab <- describe_file(".worcs", "WORCS metadata YAML", "Read only", tab, path)
tab <- describe_file("preregistration.rmd", "Preregistered hypotheses", "Human editable", tab, path)
tab <- describe_file("prepare_data.R", "Script to process raw data", "Human editable", tab, path)
tab <- describe_file("manuscript/manuscript.rmd", "Source code for paper", "Human editable", tab, path)
tab <- describe_file("manuscript/references.bib", "BibTex references for manuscript", "Human editable", tab, path)
tab <- describe_file("renv.lock", "Reproducible R environment", "Read only", tab, path)
tab <- nice_tab(tab)
cont <- append(cont, tab, after = grep("You can add rows to this table", cont))
write_as_utf(cont, file.path(path, "README.md"))
tab <- matrix(c("File", "Description", "Usage",
"README.md", "Description of project", "Human editable"), nrow = 2, byrow = TRUE)
rproj_name <- paste0(gsub("^.+\\\\b(.+)$", "\\\\1", path), ".Rproj")
cont[grep("You can load this project in Rstudio by opening the file called ", cont)] <- paste0(grep("You can load this project in Rstudio by opening the file called ", cont, value = TRUE), "'", rproj_name, "'.")
tab <- rbind(tab, c(rproj_name, "Project file", "Loads project"))
tab <- describe_file("LICENSE", "User permissions", "Read only", tab, path)
describe_file <- worcs:::describe_file
tab <- describe_file("LICENSE", "User permissions", "Read only", tab, path)
tab <- describe_file(".worcs", "WORCS metadata YAML", "Read only", tab, path)
tab <- describe_file("preregistration.rmd", "Preregistered hypotheses", "Human editable", tab, path)
tab <- describe_file("prepare_data.R", "Script to process raw data", "Human editable", tab, path)
tab <- describe_file("manuscript/manuscript.rmd", "Source code for paper", "Human editable", tab, path)
tab <- describe_file("manuscript/references.bib", "BibTex references for manuscript", "Human editable", tab, path)
tab <- describe_file("renv.lock", "Reproducible R environment", "Read only", tab, path)
tab <- nice_tab(tab)
nice_tab <- worcs:::nice_tab
tab <- nice_tab(tab)
cont <- append(cont, tab, after = grep("You can add rows to this table", cont))
write_as_utf(cont, file.path(path, "README.md"))
write_as_utf <- worcs:::write_as_utf
write_as_utf(cont, file.path(path, "README.md"))
tab
worcs::git_credentials("cjvanlissa", "c.j.vanlissa@uu.nl")
worcs::git_update("Update worcs version")
rayyan <- read.csv("rayyan_exports/articles.csv")
rayyan <- rayyan[, c("key", "notes")]
head(rayyan)
(all(rayyan$key == unique_recs$id_num))
library(reticulate)
install.packages("reticulate")
library(reticulate)
use_python()
use_condaenv("veni_sysrev") #C:\Users\lissa102\AppData\Local\Continuum\miniconda3\envs\veni_sysrev
asreview <- import("asreview")
py_run_string("asreview oracle")
recs <- readRDS("recs_6653.RData")
recs$id
recs$ID
rayyan <- read.csv("rayyan_exports/articles.csv")
recs$IS
recs$DI
i=1
this_paper <- rayyan[i,]
this_paper
this_paper["url"]
(tolower(this_paper["url"]) %in% recs$DI)
recs$rayyan <- NA
this_paper["notes"]
table(rayyan$notes)
nrow(recs)-nrow(rayyan)
rayyan$merge_id <- tolower(rayyan$url)
anyNA(rayyan$merge_id)
head(rayyan[is.na(rayyan$merge_id),])
rayyan$merge_id[is.na(rayyan$merge_id)] <- tolower(rayyan$issn)[is.na(rayyan$merge_id)]
anyNA(rayyan$merge_id)
head(rayyan[is.na(rayyan$merge_id),])
max(rayyan$key)
load("recs_18-3-2020.RData")
tmp <- readRDS("recs_18-3-2020.RData")
tmp <- readRDS("unique_recs.RData")
unique_recs <- readRDS("unique_recs.RData")
rayyan <- read.csv("rayyan_exports/articles.csv")
nrow(recs)-nrow(rayyan)
nrow(unique_recs)-nrow(rayyan)
tmp1 <- head(unique_recs)
tmp2 <- head(rayyan)
View(tmp2)
View(tmp1)
any(names(rayyan) %in% names(unique_recs))
names(rayyan)[which(names(rayyan) %in% names(unique_recs))]
names(rayyan)[which(names(rayyan) %in% names(unique_recs))]
names(rayyan)[which(names(rayyan) %in% names(unique_recs))] <- paste0(names(rayyan)[which(names(rayyan) %in% names(unique_recs))], "ray")
which(names(rayyan) %in% names(unique_recs))
unique_recs[names(rayyan)] <- NA
unique_recs[names(rayyan)][rayyan$key, ] <- rayyan
View(unique_recs)
unique_recs$rayyan
unique_recs$rayyan <- NA
unique_recs$rayyan[unique_recs$notes == " RAYYAN-INCLUSION: {\"Caspar\"=>\"Included\"}"] <- TRUE
unique_recs$rayyan[unique_recs$notes == " RAYYAN-INCLUSION: {\"Caspar\"=>\"Excluded\"}"] <- FALSE
table(unique_recs$rayyan, useNA = "always")
unique_recs$excluded_rayyan <- is.na(unique_recs$key)
table(unique_recs$excluded_rayyan, useNA = "always")
table(unique_recs$excluded_rayyan, useNA = "always")["TRUE"]
sum(unique_recs$excluded_rayyan, useNA == TRUE)
sum(unique_recs$excluded_rayyan)
table(unique_recs$rayyan, useNA = "always")
(!(sum(unique_recs$rayyan) == 367 & sum(!unique_recs$rayyan) == 192))
sum(unique_recs$rayyan)
unique_recs$rayyan
sum(unique_recs$rayyan, na.rm = TRUE) == 367
(!(sum(unique_recs$rayyan, na.rm = TRUE) == 367 & sum(!unique_recs$rayyan, na.rm = TRUE) == 192))
sum(!unique_recs$rayyan, na.rm = TRUE)
(sum(unique_recs$rayyan, na.rm = TRUE) == 367 & sum(!unique_recs$rayyan, na.rm = TRUE) == 192)
(!(sum(unique_recs$rayyan, na.rm = TRUE) == 367 & sum(!unique_recs$rayyan, na.rm = TRUE) == 192))
unique_recs$id_num
unique_recs$key[which(unique_recs$rayyan)]
[index_from_0+1]
index_from_0+1
index_from_0 <- TRUE
index_from_0+1
c(0, -1)[index_from_0+1]
write.csv(unique_recs, "unique_recs.csv", rrow.names = FALSE)
write.csv(unique_recs, "unique_recs.csv", row.names = FALSE)
md5sum("unique_recs.csv")
tools::md5sum("unique_recs.csv")
library(data.table)
library(bibliometrix)
source("word_functions.R")
exclude_terms <- readLines("exclude_terms.txt")
dict_synon <- dget("dictionary_synonyms.txt")
#recs <- convert2df(bibliometrix::readFiles("savedrecs_full.txt"))
#saveRDS(recs, "savedrecs.RData")
recs <- readRDS("recs_6653.RData")
#recs <- unique_recs
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
df <- data.table(word = tolower(unlist(df)), doc = rep(1:length(df), times = sapply(df, length)))
#table(grep("disease", df$word, value = T))
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
df <- df[!exclude_these, ]
res_cat <- cat_words(df$word, dict_synon)
head(res_cat$unmatched)
words = head(df$word, 100)
dict = dict_synon
rexes <- unlist(dict)
dict_matches <- sapply(rexes, function(this_reg){
grepl(this_reg, words, perl = TRUE)})
matches <- matrix(FALSE, nrow = length(words), ncol = length(dict))
reps <- c(0, sapply(dict, length))
for(this_word in 1:length(dict)){
sum_cols <- (sum(reps[1:this_word])+1):sum(reps[1:(this_word+1)])
matches[, this_word] <- apply(dict_matches[, sum_cols, drop = FALSE], 1, any)
}
num_matches <- rowSums(matches)
num_matches
words = df$word[1000:1100]
dict_matches <- sapply(rexes, function(this_reg){
grepl(this_reg, words, perl = TRUE)})
matches <- matrix(FALSE, nrow = length(words), ncol = length(dict))
reps <- c(0, sapply(dict, length))
for(this_word in 1:length(dict)){
sum_cols <- (sum(reps[1:this_word])+1):sum(reps[1:(this_word+1)])
matches[, this_word] <- apply(dict_matches[, sum_cols, drop = FALSE], 1, any)
}
num_matches <- rowSums(matches)
num_matches
words = df$word[1100:1200]
#new_name <- rep(names(dict), times = reps)
dict_matches <- sapply(rexes, function(this_reg){
grepl(this_reg, words, perl = TRUE)})
matches <- matrix(FALSE, nrow = length(words), ncol = length(dict))
reps <- c(0, sapply(dict, length))
for(this_word in 1:length(dict)){
sum_cols <- (sum(reps[1:this_word])+1):sum(reps[1:(this_word+1)])
matches[, this_word] <- apply(dict_matches[, sum_cols, drop = FALSE], 1, any)
}
num_matches <- rowSums(matches)
num_matches
words = df$word[1100:1150]
dict_matches <- sapply(rexes, function(this_reg){
grepl(this_reg, words, perl = TRUE)})
matches <- matrix(FALSE, nrow = length(words), ncol = length(dict))
reps <- c(0, sapply(dict, length))
for(this_word in 1:length(dict)){
sum_cols <- (sum(reps[1:this_word])+1):sum(reps[1:(this_word+1)])
matches[, this_word] <- apply(dict_matches[, sum_cols, drop = FALSE], 1, any)
}
num_matches <- rowSums(matches)
num_matches
words
words[num_matches>1]
names(dict)[apply(matches[which_matches, ], 1, function(lv){min(which(lv))})]
num_matches <- rowSums(matches)
has_matches <- !num_matches == 0
which_matches <- which(has_matches)
names(dict)[apply(matches[which_matches, ], 1, function(lv){min(which(lv))})]
names(dict)[apply(matches[which_matches, ], 1, function(lv){min(which(lv))})]
names(dict)[apply(matches[which_matches, ], 1, function(lv){which(lv)})]
apply(matches[which_matches, ], 1, function(lv){which(lv)})
apply(matches[which_matches, ], 1, function(lv){names(dict)[min(which(lv))]})
apply(matches[which_matches, ], 1, function(lv){names(dict)[which(lv)]})
outwords <- vector("list", length = length(words))
outwords[[has_matches]] <- apply(matches[which_matches, ], 1, function(lv){names(dict)[which(lv)]})
outwords <- vector("list", length = length(words))
outwords
apply(matches[which_matches, ], 1, function(lv){names(dict)[which(lv)]})
class(apply(matches[which_matches, ], 1, function(lv){names(dict)[which(lv)]}))
outwords <- apply(matches[which_matches, ], 1, function(lv){names(dict)[which(lv)]})
outwords
sample(c(12, 5))
sample(c(12, 5), 1)
handle_dups = c("first", "all", "random")
outwords <- switch(handle_dups[1],
first = apply(matches[which_matches, ], 1, function(lv){names(dict)[min(which(lv))]}),
random = apply(matches[which_matches, ], 1, function(lv){names(dict)[sample(which(lv), 1)]}),
apply(matches[which_matches, ], 1, function(lv){names(dict)[which(lv)]})
)
outwords
out_dups <- lapply(dups, function(this_dup){
these_matches <- which(words == this_dup & multimatch)[1]
unname(rexes[dict_matches[these_matches, ]])
})
out <- list(
words = outwords
)
multimatch <- num_matches > 1
(any(multimatch))
message("Duplicate matches found; see the '$dup' element of the output.", call. = FALSE)
dups <- unique(words[multimatch])
dups
this_dup = "cortisol"
words == this_dup
multimatch
which(words == this_dup & multimatch)
these_matches <- which(words == this_dup & multimatch)[1]
unname(rexes[dict_matches[these_matches, ]])
unname(rexes[dict_matches[words == this_dup, ][1,]])
words == this_dup
dict_matches[words == this_dup, ]
dict_matches[words == this_dup, ]
dict_matches[words == this_dup, ]
out_dups <- lapply(dups, function(this_dup){
these_matches <- which(words == this_dup & multimatch)[1]
unname(rexes[dict_matches[these_matches, ]])
})
names(out_dups) <- dups
out$dup <- out_dups
out$dup
message("Unmatched words found; see the '$unmatched' element of the output.", call. = FALSE)
nomatch <- which(!has_matches)
tab <- table(words[nomatch])
tab
out$unmatched <- sort(tab, decreasing = TRUE)
out$unmatched
head(df)
install.packages("kbenoit/quanteda.dictionaries")
install.packages("stopwords")
install.packages("kbenoit/quanteda.dictionaries")
library(quanteda.dictionaries)
head(quanteda.corpora::data_corpus_movies)
install.packages("kbenoit/quanteda.corpora")
install.packages("quanteda")
install.packages("quanteda.corpora")
install.packages("quanteda/quanteda.corpora")
head(quanteda.corpora::data_corpus_movies)
head(quanteda.corpora::data_corpus_amicus)
head(dict)
yaml::as.yaml(head(dict))
cat(yaml::as.yaml(head(dict)))
yaml::write_yaml(head(dict), "test.dic")
tmp <- yaml::read_yaml("test.dic")
tmp
class(tmp)
yaml::write_yaml(head(dict), "dictionary.txt")
yaml::write_yaml(head(dict), "yaml_dictionary.txt")
txt <- c("The red-shirted lawyer gave her yellow-haired, red nose ex-boyfriend $300
out of pity:(.")
dict <- quanteda::dictionary(list(people = c("lawyer", "boyfriend"),
color_fixed = "red",
color_glob = c("red*", "yellow*", "green*"),
mwe = "out of"))
liwcalike(txt, dict, what = "word")
txt <- c("The red-shirted lawyer gave her yellow-haired, red nose ex-boyfriend $300
out of pity:(.")
dict <- quanteda::dictionary(list(law = c("\\blawyer"),
color_fixed = "red",
color_glob = c("red*", "yellow*", "green*"),
mwe = "out of"))
liwcalike(txt, dict, what = "word")
dict <- quanteda::dictionary(list(law = c("\blawyer"),
color_fixed = "red",
color_glob = c("red*", "yellow*", "green*"),
mwe = "out of"))
liwcalike(txt, dict, what = "word")
dict <- quanteda::dictionary(list(law = c("lawyer"),
color_fixed = "red",
color_glob = c("red*", "yellow*", "green*"),
mwe = "out of"))
liwcalike(txt, dict, what = "word")
kwic(data_corpus_inaugural, pattern = "terror", valuetype = "regex")
library(quanteda)
kwic(data_corpus_inaugural, pattern = "terror", valuetype = "regex")
kwic(data_corpus_inaugural, pattern = "terror\\b", valuetype = "regex")
txt <- c("The red-shirted lawyer gave her yellow-haired, red nose ex-boyfriend $300
out of pity:(.")
dict <- quanteda::dictionary(list(lawyer = c("\\blawyer\\b"),
color_fixed = "red",
color_glob = c("red*", "yellow*", "green*"),
mwe = "out of"))
liwcalike(txt, dict, what = "word")
dict <- quanteda::dictionary(list(lawyer = c("\\blawyer\\b"))
liwcalike(txt, dict, what = "word")
txt <- c("The red-shirted lawyer gave her yellow-haired, red nose ex-boyfriend $300
out of pity:(.")
dict <- quanteda::dictionary(list(lawyer = c("\\blawyer\\b"))
liwcalike(txt, dict, what = "word")
dict <- quanteda::dictionary(list(lawyer = c("\\blawyer\\b")))
liwcalike(txt, dict, what = "word")
liwcalike(txt, dict, what = "word", valuetype = "regex")
txt <- c("The red-shirted lawyer gave her yellow-haired, red nose ex-boyfriend $300
out of pity:(.")
dict <- quanteda::dictionary(list(lawyer = c("\\blawyer\\b")))
liwcalike(txt, dict, what = "word", valuetype = "regex")
dict <- quanteda::dictionary(list(lawyer = c("\\blawyer\\b", "law.er")))
liwcalike(txt, dict, what = "word", valuetype = "regex")
exclude_terms <- readLines("exclude_terms.txt")
exclude_terms
yaml::as.yaml(exclude_terms)
cat(yaml::as.yaml(exclude_terms))
library(yaml)
dict_synon <- read_yaml("yaml_dictionary.txt")
dict_synon
dict_synon <- dget("dictionary_synonyms.txt")
dict_synon
write_yaml(dict_synon, "yaml_dict.txt")
dict_synon <- read_yaml("yaml_dict.txt")
source("word_functions.R")
exclude_terms <- readLines("exclude_terms.txt")
dict_synon <- read_yaml("yaml_dict.txt")
recs <- readRDS("recs_6653.RData")
class(recs)
screen <- read.csv("asreview_result_sysrevemotprob.csv")
recs <- readRDS("unique_recs.RData")
screen <- read.csv("asreview_result_sysrevemotprob.csv")
View(screen)
table(screen$included)
tail(screen$included)
head(screen$title)
tail(screen$title)
diff(screen$asreview_ranking)
all(diff(screen$asreview_ranking)==1)
tail(screen$title)
tail(screen$rayyan)
tail(screen$abstract)
library("papaja")
library(bibliometrix)
library(revtools)
library(bib2df)
library(tidySEM)
dups <- read.csv("dups.csv")
dups
6653-69
6653-56
filenam <- "recs_6653.RData"
recs <- readRDS(filenam)
recs$doi <- recs$DI
recs$doi[is.na(recs$doi)] <- paste0("fake", 1:sum(is.na(recs$doi)))
if(any(duplicated(recs$doi))){
recs$doi[duplicated(recs$doi)|duplicated(recs$doi, fromLast = T)]
dup_doi <- duplicated(recs$doi)
recs <- recs[!dup_doi, ]
}
recs$title <- recs$TI
potential_dups <- find_duplicates(recs, match_variable = "title", match_function = "stringdist", threshold = 5, to_lower = TRUE, remove_punctuation = TRUE)
unique_recs <- extract_unique_references(recs, matches = potential_dups)
sum(dup_doi)
(nrow(recs)-nrow(unique_recs))-sum(dup_doi))
(nrow(recs)-nrow(unique_recs))-sum(dup_doi)
dups[["dup_title"]]
6651-6595
dups[["dup_doi"]]
dups[["dup_title"]]
tmp <- read.csv("rayyan.csv")
tmp <- read.csv("rayyan_exports/articles.csv")
table(tmp$notes)
rayyan_res <- read.csv("rayyan_exports/articles.csv")
table(rayyan_res$notes)
install.packages("PRISMAstatement")
library(PRISMAstatement)
all(diff(rayyan_res$key) == 1)
nrow(df_screen) - nrow(rayyan_res)
nrow(unique_recs) - nrow(rayyan_res)
screen$record_id[!screen$record_id %in% rayyan_res$key]
names(screen
)
head(screen[, c("key", "record_id")])
head(screen[, c("key", "record_id")])
tail(screen[, c("key", "record_id")])
screen[!screen$key %in% rayyan_res$key, c("included")]
screen <- screen[screen$key %in% rayyan_res$key, ]
screen$included
screen$abstract[min(which(screen$included == 0))-1]
screen$abstract[min(which(screen$included == 0))-2]
min(which(screen$included == 0))-2
min(which(screen$included == 0))
screen$abstract
table(screen$included)
table(screen$included, screen$excluded_rayyan)
table(screen$included, screen$excluded_rayyan)
table(screen$included, screen$rayyan)
367+192
