apply(1, mean) # average over clusters
})
# b is the lowest average dissimilarity of i to any other cluster of
# which i is not a member
b <- apply(cosine.matrix.outside, 1, min)
# silhouette width is b - a
cosine.sil.width <- b - a
data.frame(word = names(cosine.sil.width), width = cosine.sil.width)
})
widths.list <- do.call(rbind, widths.list)
# join membership onto data.frame
membership.df <- data.frame(word = names(membership),
membership = membership)
widths.list <- left_join(widths.list, membership.df, by = "word")
return(widths.list)
}
set.seed(238942)
# calculate the average silhouette width for k=5, ..., 20
sil.width <- sapply(5:20, function(k) {
# generate k clusters
membership <- pam(cosine_similarity, k = k)
# calcualte the silhouette width for each observation
width <- cosineSilhouette(cosine_similarity,
membership = factor(membership$clustering))$width
return(mean(width))
})
library(knitr)
library(dplyr)
library(reshape2)
library(cluster)
library(ggplot2)
library(devtools)
# install the development version of superheat
devtools::install_github("rlbarter/superheat")
library(superheat)
# load csv into object
dict_wordvecemb <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/asreview_simulation/dict_wordvec.csv", header = FALSE)
# adjust the first column name to word
colnames(dict_wordvecemb)[1] <- "word"
# load csv into object
dict_wordvecemb <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/asreview_simulation/dict_wordvec.csv", header = FALSE)
# load csv into object
dict_wordvecemb <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/asreview_simulation/dict_wordvec.csv", header = FALSE)
# adjust the first column name to word
colnames(dict_wordvecemb)[1] <- "word"
# check structure of dataframe
str(dict_wordvecemb)
# remove certain characters from the column V2 which now is column of the type
# character and contains a string as value.
library(tidyverse)
# create new data frame that can be used for applying adjustments
df_wordvecemb <- dict_wordvecemb
# remove the [ character from V2
df_wordvecemb$V2 <-gsub("\\[","",as.character(df_wordvecemb$V2))
# remove the ] character from V2
df_wordvecemb$V2 <-gsub("\\]","",as.character(df_wordvecemb$V2))
# remove the \n character from V2
df_wordvecemb$V2 <-gsub("\\\n","",as.character(df_wordvecemb$V2))
# check if removing the characters went correctly
df_wordvecemb[1,"V2"]
# split column V2 into multiple columns
library(splitstackshape)
# separate on the space
df_wordvecemb <- cSplit(df_wordvecemb, "V2", " ")
# retain dimensions of data frame
dim(df_wordvecemb)
# rename the column names of the data frame.
# First column is named word and the other columns dim1-40
colnames(df_wordvecemb) <- c("word", paste0("dim", 1:40))
# check if there are any missings
summary(df_wordvecemb)
# for same reason rename the rownames does not work on the data frame so first convert to matrix
#  transform data frame to matrix
mat_wordvecemb <- as.matrix(df_wordvecemb)
# the first column contains the words so we want to set the row names accordingly
rownames(mat_wordvecemb) <- mat_wordvecemb[,1]
# and then remove the first column
mat_wordvecemb <- mat_wordvecemb[,-1]
# check str of the matrix
str(mat_wordvecemb)
# convert matrix back to dataframe
df <- as.data.frame(mat_wordvecemb)
# check structure of this data frame
str(df)
# change all columns to numeric
df[,1:40] <- sapply(df[,1:40],as.numeric)
# see what function t() does when applied on this data frame
testmat <- t(df)
str(testmat)
CosineFun <- function(x, y){
# calculate the cosine similarity between two vectors: x and y
c <- sum(x*y) / (sqrt(sum(x * x)) * sqrt(sum(y * y)))
return(c)
}
CosineSim <- function(X) {
# calculate the pairwise cosine similarity between columns of the matrix X.
# initialize similarity matrix
m <- matrix(NA,
nrow = ncol(X),
ncol = ncol(X),
dimnames = list(colnames(X), colnames(X)))
cos <- as.data.frame(m)
# calculate the pairwise cosine similarity
for(i in 1:ncol(X)) {
for(j in i:ncol(X)) {
co_rate_1 <- X[which(X[, i] & X[, j]), i]
co_rate_2 <- X[which(X[, i] & X[, j]), j]
cos[i, j] <- CosineFun(co_rate_1, co_rate_2)
# fill in the opposite diagonal entry
cos[j, i] <- cos[i, j]
}
}
return(cos)
}
# takes a really long time because I am running it on 19467 words for testing I will only run it on 1000 words
cosine_similarity <- CosineSim(t(df[c(1:50, 300:350, 600:700, 845:945, 2000:2100, 18550:18700, 5000:5250, 13840:14000, 8000:8032), ]))
cosineSilhouette <- function(cosine.matrix, membership) {
# Args:
#   cosine.matrix: the cosine similarity matrix for the words
#   membership: the named membership vector for the rows and columns.
#               The entries should be cluster centers and the vector
#               names should be the words.
if (!is.factor(membership)) {
stop("membership must be a factor")
}
# note that there are some floating point issues:
# (some "1" entires are actually sliiightly larger than 1)
cosine.dissim <- acos(round(cosine.matrix, 10)) / pi
widths.list <- lapply(levels(membership), function(clust) {
# filter rows of the similarity matrix to words in the current cluster
# filter cols of the similarity matrix to words in the current cluster
cosine.matrix.inside <- cosine.dissim[membership == clust,
membership == clust]
# a: average dissimilarity of i with all other data in the same cluster
a <- apply(cosine.matrix.inside, 1, mean)
# filter rows of the similarity matrix to words in the current cluster
# filter cols of the similarity matrix to words NOT in the current cluster
other.clusters <- levels(membership)[levels(membership) != clust]
cosine.matrix.outside <- sapply(other.clusters, function(other.clust) {
cosine.dissim[membership == clust, membership == other.clust] %>%
apply(1, mean) # average over clusters
})
# b is the lowest average dissimilarity of i to any other cluster of
# which i is not a member
b <- apply(cosine.matrix.outside, 1, min)
# silhouette width is b - a
cosine.sil.width <- b - a
data.frame(word = names(cosine.sil.width), width = cosine.sil.width)
})
widths.list <- do.call(rbind, widths.list)
# join membership onto data.frame
membership.df <- data.frame(word = names(membership),
membership = membership)
widths.list <- left_join(widths.list, membership.df, by = "word")
return(widths.list)
}
set.seed(238942)
sil.width <- sapply(5:20, function(k) {
# generate k clusters
membership <- pam(cosine_similarity, k = k)
# calcualte the silhouette width for each observation
width <- cosineSilhouette(cosine_similarity,
membership = factor(membership$clustering))$width
return(mean(width))
})
data.frame(k = 5:20, width = sil.width) %>%
ggplot(aes(x = k, y = width)) +
geom_line() +
geom_point() +
scale_y_continuous(name = "Avergae silhouette width")
library(cluster)
generateClusters <- function(similarity.mat, k.range, N) {
random.subset.list <- lapply(1:100, function(i) {
sample(1:nrow(similarity.mat), 0.9 * nrow(similarity.mat))
})
lapply(k.range, function(k) {
print(paste("k =", k))
lapply(1:N, function(i) {
# randomly sample 90% of words
cosine.sample <- similarity.mat[random.subset.list[[i]], random.subset.list[[i]]]
# perform clustering
pam.clusters <- pam(1 - cosine.sample, k = k, diss = TRUE)
})
})
}
# generate clusters ranging from 5 to 20 cluster groups for each of 100 subsamples
# This will take a little while to run
cluster.iterations <- generateClusters(cosine_similarity,
k.range = 5:20,
N = 100)
join.cluster.iterations <- lapply(cluster.iterations, function(list) {
# for each list of iterations (for a specific k),
# full-join the membership vectors into a data frame
# (there will be missing values in each column)
Reduce(function(x, y) full_join(x, y, by = "words"),
lapply(list, function(cluster.obj) {
df <- data.frame(words = names(cluster.obj$clustering),
clusters = cluster.obj$clustering)
}))
})
join.cluster.iterations <- lapply(join.cluster.iterations, function(x) {
colnames(x) <- c("words", paste0("membership", 1:100))
return(x)
})
# view the first 8 columns of the first data frame (correpsonding to k=5)
kable(head(join.cluster.iterations[[1]][, 1:8]))
# calculate the pairwise jaccard similarity between each of the cluster
# memberships accross the common words
# to avoid correlation, we do this pairwise between simulations 1 and 2,
# and then between simulations 3 and 4, and so on
library(Rcpp)
install.packages("Rcpp")
# calculate the pairwise jaccard similarity between each of the cluster
# memberships accross the common words
# to avoid correlation, we do this pairwise between simulations 1 and 2,
# and then between simulations 3 and 4, and so on
library(Rcpp)
find_similar_words <- function(word, embedding_matrix, n = 5) {
similarities <- embedding_matrix[word, , drop = FALSE] %>%
sim2(embedding_matrix, y = ., method = "cosine")
similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}
find_similar_words("friends", testmat,10)
# load csv into object
dict_wordvecemb <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/asreview_simulation/dict_wordvec.csv", header = FALSE)
# adjust the first column name to word
colnames(dict_wordvecemb)[1] <- "word"
# check structure of dataframe
str(dict_wordvecemb)
# remove certain characters from the column V2 which now is column of the type
# character and contains a string as value.
library(tidyverse)
# create new data frame that can be used for applying adjustments
df_wordvecemb <- dict_wordvecemb
# remove the [ character from V2
df_wordvecemb$V2 <-gsub("\\[","",as.character(df_wordvecemb$V2))
# remove the ] character from V2
df_wordvecemb$V2 <-gsub("\\]","",as.character(df_wordvecemb$V2))
# load in some useful libraries need to copy tutorial
library(knitr)
library(dplyr)
library(reshape2)
library(cluster)
library(ggplot2)
library(devtools)
# install the development version of superheat
# devtools::install_github("rlbarter/superheat")
library(superheat)
# load csv into object
dict_wordvecemb <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/asreview_simulation/dict_wordvec.csv", header = FALSE)
# adjust the first column name to word
colnames(dict_wordvecemb)[1] <- "word"
# check structure of dataframe
str(dict_wordvecemb)
# remove certain characters from the column V2 which now is column of the type
# character and contains a string as value.
library(tidyverse)
# create new data frame that can be used for applying adjustments
df_wordvecemb <- dict_wordvecemb
# remove the [ character from V2
df_wordvecemb$V2 <-gsub("\\[","",as.character(df_wordvecemb$V2))
# remove the ] character from V2
df_wordvecemb$V2 <-gsub("\\]","",as.character(df_wordvecemb$V2))
# remove the \n character from V2
df_wordvecemb$V2 <-gsub("\\\n","",as.character(df_wordvecemb$V2))
# check if removing the characters went correctly
df_wordvecemb[1,"V2"]
# split column V2 into multiple columns
library(splitstackshape)
# separate on the space
df_wordvecemb <- cSplit(df_wordvecemb, "V2", " ")
# retain dimensions of data frame
dim(df_wordvecemb)
# rename the column names of the data frame.
# First column is named word and the other columns dim1-40
colnames(df_wordvecemb) <- c("word", paste0("dim", 1:40))
# for same reason rename the rownames does not work on the data frame so first convert to matrix
#  transform data frame to matrix
mat_wordvecemb <- as.matrix(df_wordvecemb)
# the first column contains the words so we want to set the row names accordingly
rownames(mat_wordvecemb) <- mat_wordvecemb[,1]
# and then remove the first column
mat_wordvecemb <- mat_wordvecemb[,-1]
# check str of the matrix
str(mat_wordvecemb)
# convert matrix back to dataframe
df <- as.data.frame(mat_wordvecemb)
# check structure of this data frame
str(df)
# change all columns to numeric
df[,1:40] <- sapply(df[,1:40],as.numeric)
# check structure of this data frame
str(df)
# see what function t() does when applied on this data frame
testmat <- t(df)
str(testmat)
library(text2vec)
install.packages("text2vec")
worcs::git_update()
renv::status()
renv::status()
?renv::dependencies
library("papaja")
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(knitr)
library(kableExtra)
library(cowplot)
run_everything <- TRUE
notingraph <- list()
keyw <- function(x){
x
}
cat_shapes <- c(Outcome = "square2", Indicator = "circle2", Cause = "quad2", Protective = "tri2")
legend_shapes <- c("circle2" = 21, "square2" = 22, "quad2" = 23, "tri2" = 24)[cat_shapes]
scale_shapes <- legend_shapes
names(scale_shapes) <- names(cat_shapes)[match(names(legend_shapes), cat_shapes)]
library(stringr)
library(udpipe)
library(igraph)
library(wordcloud)
library(Matrix)
library(yaml)
library(Rmpfr)
library(topicmodels)
library(udpipe)
library(slam)
library(tidytext)
library(ggplot2)
library(textrank)
source("word_functions.R")
source("circle2.R")
run_intensive = FALSE
if(!run_intensive){
df <- readRDS("study2_df.RData")
} else {
stop("Note: Study 2 file is out of date. Please run this code manually.")
recs <- data.table(read.csv("recs_final.csv"))
if(!is.data.table(recs)){
browser()
}
recs[, "doc" := 1:nrow(recs)]
recs$AB <- tolower(recs$AB)
if(!file.exists("english-ewt-ud-2.4-190531.udpipe")) {
ud_model <- udpipe_download_model(language = "english")
} else {
ud_model <- udpipe_load_model("english-ewt-ud-2.4-190531.udpipe")
ud_model <- udpipe_load_model(ud_model$file)
}
udp_res <- udpipe_annotate(ud_model, x = recs$AB, doc_id = recs$doc)
df <- as.data.table(udp_res)
saveRDS(df, "study2_df.RData")
}
recs <- data.table(read.csv("recs_final.csv"))
recs[, "doc" := 1:nrow(recs)]
# convert abstract column to lower case
recs$AB <- tolower(recs$AB)
# download English udpipe model
ud_model <- udpipe_download_model(language = "english")
View(ud_model)
ud_model <- udpipe_load_model("english-ewt-ud-2.4-190531.udpipe")
ud_model <- udpipe_load_model("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/english-ewt-ud-2.5-191206.udpipe")
View(ud_model)
ud_model <- udpipe_load_model(ud_model$file)
# apply to abstract of recs data table
udp_res <- udpipe_annotate(ud_model, x = recs$AB, doc_id = recs$doc)
# convert to data table and save as .Rdata
df <- as.data.table(udp_res)
saveRDS(df, "study2_df.RData")
# Exclude words
df_kw <- df[upos %in% c("NOUN", "ADJ"), ]
df_kw <- df_kw[grepl("^[a-zA-Z].", df_kw$lemma), ]
exclude_terms <- readLines("exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df_kw$lemma)))
df_kw <- df_kw[-exclude_these, ]
saveRDS(df_kw, "study2_df_kw.RData")
# dit is denk een check
# No numeric values
all(is.na(as.numeric(df_kw$lemma)))
df_kw$lemma[nchar(df_kw$lemma) == 3]
kw_tr <- textrank_keywords(x = df_kw$lemma[df_kw$upos %in% c("NOUN", "ADJ")], ngram_max = 3, sep = " ")
saveRDS(kw_tr, "study2_textrank.RData")
# Merge back with original data
df_kw$keyword <- txt_recode_ngram(df_kw$lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = " ")
df_kw$keyword[!df_kw$keyword %in% kw_tr$keywords$keyword] <- NA
# note this time we do not want to apply the dictionary filter.
df_analyze <- df_kw[!is.na(df_kw$keyword), ]
# save the 'cleaned' dataframe
saveRDS(df_analyze, "study2_analyze_without_dict_filter.RData")
View(df_analyze)
# load dictionary
dict <- read_yaml("yaml_dict.txt")
# check number of obs when dic is applied
res_cat <- cat_words(df_analyze$keyword, dict, handle_dups = "all")
# Check coding issues
res_cat$dup
head(res_cat$unmatched)
# check number of obs when dic is applied
res_cat <- cat_words(df_analyze$keyword, dict, handle_dups = "all")
# Check coding issues
#res_cat$dup
#head(res_cat$unmatched)
df_analyze_dict <- merge_df(df_analyze, res_cat$words, "word_coded")
saveRDS(df_analyze_dict, "study2_df_analyze.RData")
View(df_analyze_dict)
unique(df_analyze_dict$word_coded)
length(unique(df_analyze_dict$word_coded))
length(unique(df_analyze$word_coded))
View(df_analyze)
length(unique(df_analyze$token))
length(unique(df_analyze$lemma))
length(unique(df_analyze_dict$token))
View(df_analyze)
# check if corpus consist of same number of documents and unique terms as in paper
nounbydoc <- df_analyze_dict[, list(freq = .N), by = list(doc_id = doc_id, term = word_coded)]
number_docs_words2 <- c(docs = length(unique(nounbydoc$doc_id)), words = length(unique(nounbydoc$term)))
View(nounbydoc)
length(unique(df_analyze_dict$word_coded)))
length(unique(df_analyze_dict$word_coded))
# load csv into object
dict_wordvecemb <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/asreview_simulation/dict_wordvec.csv", header = FALSE)
# load csv into object
dict_wordvecemb <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/asreview_simulation/dict_wordvec.csv", header = FALSE)
# adjust the first column name to word
colnames(dict_wordvecemb)[1] <- "word"
# check structure of dataframe
str(dict_wordvecemb)
# remove certain characters from the column V2 which now is column of the type
# character and contains a string as value.
library(tidyverse)
# create new data frame that can be used for applying adjustments
df_wordvecemb <- dict_wordvecemb
# remove the [ character from V2
df_wordvecemb$V2 <-gsub("\\[","",as.character(df_wordvecemb$V2))
# remove the ] character from V2
df_wordvecemb$V2 <-gsub("\\]","",as.character(df_wordvecemb$V2))
# remove the \n character from V2
df_wordvecemb$V2 <-gsub("\\\n","",as.character(df_wordvecemb$V2))
# check if removing the characters went correctly
df_wordvecemb[1,"V2"]
# split column V2 into multiple columns
library(splitstackshape)
# separate on the space
df_wordvecemb <- cSplit(df_wordvecemb, "V2", " ")
# retain dimensions of data frame
dim(df_wordvecemb)
# rename the column names of the data frame.
# First column is named word and the other columns dim1-40
colnames(df_wordvecemb) <- c("word", paste0("dim", 1:40))
unique(df_wordvecemb$word)
length(unique(df_wordvecemb$word))
worcs::git_update()
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# load csv into object
dict_wordvecemb <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/asreview_simulation/dict_wordvec.csv", header = FALSE)
# adjust the first column name to word
colnames(dict_wordvecemb)[1] <- "word"
# check structure of dataframe
str(dict_wordvecemb)
# remove certain characters from the column V2 which now is column of the type character and contains a string as value.
library(tidyverse)
# create new data frame that can be used for applying adjustments
df_wordvecemb <- dict_wordvecemb
# remove the [ character from V2
df_wordvecemb$V2 <-gsub("\\[","",as.character(df_wordvecemb$V2))
# remove the ] character from V2
df_wordvecemb$V2 <-gsub("\\]","",as.character(df_wordvecemb$V2))
# remove the \n character from V2
df_wordvecemb$V2 <-gsub("\\\n","",as.character(df_wordvecemb$V2))
# check if removing the characters went correctly
df_wordvecemb[1,"V2"]
# -> yes there are now only numbers (the vectors)
# split column V2 into multiple columns
library(splitstackshape)
# separate on the space
df_wordvecemb <- cSplit(df_wordvecemb, "V2", " ")
# retain dimensions of data frame
dim(df_wordvecemb)
# -> we have 19,476 words that are defined by 40 dimensions. These dimensions define the context of the words.
# check how many unique words there are
length(unique(df_wordvecemb$word))
# -> as expected there are as many unique words as there are observations.
# rename the column names of the data frame.
# First column is named word and the other columns dim1-40
colnames(df_wordvecemb) <- c("word", paste0("dim", 1:40))
# check if there are any missings
summary(df_wordvecemb)
# -> no missing values.
# load libraries
library(stringr)
library(udpipe)
library(igraph)
library(wordcloud)
library(Matrix)
library(yaml)
library(Rmpfr)
library(topicmodels)
library(udpipe)
library(slam)
library(tidytext)
library(ggplot2)
library(textrank)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
source("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/word_functions.R")
source("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/circle2.R")
recs <- data.table(read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/recs_final.csv"))
recs[, "doc" := 1:nrow(recs)]
# convert abstract column to lower case
recs$AB <- tolower(recs$AB)
View(df_wordvecemb)
View(df_wordvecemb)
View(df_wordvecemb)
worcs::git_update()
