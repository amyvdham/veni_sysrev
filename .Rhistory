# note that there are some floating point issues in the similarity matrix:
# some "1" entires are actually sliiightly larger than 1, so we round to
# the nearest 10 dp when calcualting the distance matrix
word.clusters <- pam(acos(round(cosine.similarity.full, 10)) / pi, k = 20, diss = TRUE)
# note that there are some floating point issues in the similarity matrix:
# some "1" entires are actually sliiightly larger than 1, so we round to
# the nearest 10 dp when calcualting the distance matrix
word.clusters <- pam(acos(round(cosine_similarity, 10)) / pi, k = 20, diss = TRUE)
word.clusters.13 <- pam(acos(round(cosine_similarity, 10)) / pi, k = 13, diss = TRUE)
# print the cluster medoids
word.clusters$medoids
# convert the membership vector to a factor
word.membership <- factor(word.clusters$clustering)
# print the cluster medoids
word.clusters.13$medoids
# convert the membership vector to a factor
word.membership.13 <- factor(word.clusters.13$clustering)
# replace integer membership by medoid membership
levels(word.membership) <- word.clusters$medoids
# replace integer membership by medoid membership
levels(word.membership.13) <- word.clusters.13$medoids
# compare the membership vectors with 20 and 13 clusters
word.membership.split <- split(word.membership, word.membership)
word.membership.split.13 <- split(word.membership.13, word.membership.13)
compare.20.13 <- sapply(word.membership.split, function(i) {
sapply(word.membership.split.13, function(j) {
sum(names(i) %in% names(j)) / length(i)
})
})
superheat(compare.20.13,
heat.pal = c("white", "grey", "black"),
heat.pal.values = c(0, 0.1, 1),
column.title = "11 clusters",
row.title = "12 clusters",
bottom.label.text.angle = 90,
bottom.label.size = 0.4)
superheat(compare.20.13,
heat.pal = c("white", "grey", "black"),
heat.pal.values = c(0, 0.1, 1),
column.title = "20 clusters",
row.title = "13 clusters",
bottom.label.text.angle = 90,
bottom.label.size = 0.4)
# calculate the cosine silhouette width
cosine.silhouette <-
cosineSilhouette(cosine.similarity.full, word.membership)
# calculate the cosine silhouette width
cosine.silhouette <-
cosineSilhouette(cosine_similarity, word.membership)
# arrange the words in the same order as the original matrix
rownames(cosine.silhouette) <- cosine.silhouette$word
cosine.silhouette <- cosine.silhouette[rownames(cosine_similarity), ]
# calculate the average width for each cluster
avg.sil.width <- cosine.silhouette %>%
group_by(membership) %>%
summarise(avg.width = mean(width)) %>%
arrange(avg.width)
# add a blank space after each word (for aesthetic purposes)
word.membership.padded <- paste0(word.membership, " ")
# reorder levels based on increasing separation
word.membership.padded <- factor(word.membership.padded,
levels = paste0(avg.sil.width$membership, " "))
superheat(cosine_similarity,
# row and column clustering
membership.rows = word.membership.padded,
membership.cols = word.membership.padded,
# top plot: silhouette
yt = cosine.silhouette$width,
yt.axis.name = "Cosine\nsilhouette\nwidth",
yt.plot.type = "bar",
yt.bar.col = "grey35",
# order of rows and columns within clusters
order.rows = order(cosine.silhouette$width),
order.cols = order(cosine.silhouette$width),
# bottom labels
bottom.label.col = c("grey95", "grey80"),
bottom.label.text.angle = 90,
bottom.label.text.alignment = "right",
bottom.label.size = 0.28,
# left labels
left.label.col = c("grey95", "grey80"),
left.label.text.alignment = "right",
left.label.size = 0.26,
# title
title = "(a)")
superheat(cosine_similarity,
# row and column clustering
membership.rows = word.membership.padded,
membership.cols = word.membership.padded,
# top plot: silhouette
yt = cosine.silhouette$width,
yt.axis.name = "Cosine\nsilhouette\nwidth",
yt.plot.type = "bar",
yt.bar.col = "grey35",
# order of rows and columns within clusters
order.rows = order(cosine.silhouette$width),
order.cols = order(cosine.silhouette$width),
# bottom labels
bottom.label.col = c("grey95", "grey80"),
bottom.label.text.angle = 90,
bottom.label.text.alignment = "right",
bottom.label.size = 0.28,
# left labels
left.label.col = c("grey95", "grey80"),
left.label.text.alignment = "right",
left.label.size = 0.26,
# smooth heatmap within clusters
smooth.heat = T,
# title
title = "(b)")
library(RColorBrewer)
library(wordcloud)
# define a function that takes the cluster name and the membership vector
# and returns a word cloud
makeWordCloud <- function(cluster, word.membership, words.freq) {
words <- names(word.membership[word.membership == cluster])
words.freq <- words.freq[words]
# make all words black except for the cluster center
words.col <- rep("black", length = length(words.freq))
words.col[words == cluster] <- "red"
# the size of the words will be the frequency from the NY Times headlines
wordcloud(words, words.freq, colors = words.col,
ordered.colors = TRUE, random.order = FALSE, max.words = 80)
}
# plot word clouds
set.seed(52545)
for (word in levels(word.membership)) {
makeWordCloud(word, word.membership, words.freq = freq)
}
# convert the membership vector to a factor
word.membership <- factor(word.clusters$clustering)
for (word in levels(word.membership)) {
makeWordCloud(word, word.membership, words.freq = freq())
}
for (word in levels(word.membership)) {
makeWordCloud(word, word.membership, words.freq = freq)
}
# convert the membership vector to a factor
word.membership$membership <- factor(word.clusters$clustering)
View(word.membership)
View(word.clusters)
word <- df_final["empathy", , drop = FALSE]
cos_sim = sim2(x = df_final, y = word, method = "cosine", norm = "l2")
# Loading required libraries
library(stringr)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(Matrix)
library(ggplot2)
library(dplyr)
# Required library for GloVe
library(text2vec)
# Loading script containing functions
source("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/word_functions.R")
# prepare data
# this is were line 891 in manuscript file starts
recs <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]
## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column.
df <- merge_df(recs, df, "word")
# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
# check the number of author keywords per doc
df_n_keywords <- df %>%
group_by(doc) %>%
summarise(n_keywords = n()) # snap nog niet helemaal hoe deze code weet dat die n() dan betrekking heeft op de column word maar gaat wel goed.
## NOTE: some keywords contain more than one word, e.g. risk factors or follow-up studies and youth smoking behavior. So to now the lenght (number of words) per doc this won't work
# Now that the dataset is cleaned created a df in which each document is a row and the column DEclean contains the author keywords of the specific document. Shape the tokens back to their original form
df_new <- df %>%
group_by(word) %>%
group_by(doc) %>%
summarise(DEclean = str_c(word, collapse = " "))
# second attempt word count
df_new$DEclean_length <- sapply(df_new$DEclean, function(x){str_count(x, " ")+1})
# this gives us the number of words in the column DEclean for each row. NOTE: words that are connected by a - are seen as one word, e.g. follow-up or affect-regulatory. Also, abbreviations are also counted as a word, e.g. INFORMATION AND COMMUNICATION TECHNOLOGY (ICT). (ICT) is counted in this example.
df_new <- data.table(df_new)
# Max length of author keywords
max(df_new$DEclean_length)
# investigate with the highest value of number of author keywords
df_new[df_new$doc == 3667, 'DEclean']
# check if this is a result of data cleaning by looking at the original data set
recs[recs$doc == 3667, 'DE']
# -> shows that this is not a result of data cleaning and it seems that this article does really have 524 author keywords.
# check the doi of this article so I can look it up
recs[recs$doc == 3667, c('doi', 'DI', 'title', 'AU', 'PY')]
# -> online this journal contains all the keywords of the articles belonging to this journal in the past 5 years.
# check article with 77 author keywords
recs[recs$doc == 5234, c('doi', 'DI', 'title', 'AU', 'PY', 'DE')]
# -> looked up the article online and does not seem to have any author keywords, do not not know where the words come from.
# check article with 33 author keywords
recs[recs$doc == 2750, c('doi', 'DI', 'title', 'AU', 'PY', 'DE')]
# -> this is correct looked up the article and does indeed contain 33 words in the author keywords. Same is the case for the article with 30 author keywords (doc = 1186)
# NOTE: document/case/row shown below contains < in author keywords. Does this need to be cleaned? It also includes the word literature whereas this word was part of the terms that had to be excluded.
df_new[df_new$doc == 5234, 'DEclean']
# check why those words with literature were not excluded
df[df$doc == 5234, 'word']
# ->  they were not excluded because the regular expression ^literature$ tells to only delete those values that where start of the string is followed by literature followed by the end of the string.
# check how many unique documents and how many unique words there are after excluding non substantive words.
length(unique(df$doc))
length(unique(df$word))
# wat ligt er dicht bij 'school'
word <- glove_embedding["school", , drop = FALSE]
word <- df_final["empathy", , drop = FALSE]
cos_sim = sim2(x = df_final, y = word, method = "cosine", norm = "l2")
cos_sim = sim2(x = t(df_final), y = word, method = "cosine", norm = "l2")
df_final <- t(df_final)
word <- df_final["empathy", , drop = FALSE]
cos_sim = sim2(x = t(df_final), y = word, method = "cosine", norm = "l2")
worcs::git_update()
# load csv into object
dict_wordvecemb <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/asreview_simulation/dict_wordvec.csv", header = FALSE)
# adjust the first column name to word
colnames(dict_wordvecemb)[1] <- "word"
# check structure of dataframe
str(dict_wordvecemb)
# remove certain characters from the column V2 which now is column of the type character and contains a string as value.
library(tidyverse)
# create new data frame that can be used for applying adjustments
df_wordvecemb <- dict_wordvecemb
# remove the [ character from V2
df_wordvecemb$V2 <-gsub("\\[","",as.character(df_wordvecemb$V2))
# remove the ] character from V2
df_wordvecemb$V2 <-gsub("\\]","",as.character(df_wordvecemb$V2))
# remove the \n character from V2
df_wordvecemb$V2 <-gsub("\\\n","",as.character(df_wordvecemb$V2))
# check if removing the characters went correctly
df_wordvecemb[1,"V2"]
# split column V2 into multiple columns
library(splitstackshape)
# separate on the space
df_wordvecemb <- cSplit(df_wordvecemb, "V2", " ")
# retain dimensions of data frame
dim(df_wordvecemb)
# check how many unique words there are
length(unique(df_wordvecemb$word))
# rename the column names of the data frame.
# First column is named word and the other columns dim1-40
colnames(df_wordvecemb) <- c("word", paste0("dim", 1:40))
# check if there are any missings
summary(df_wordvecemb)
# WITHOUT TEXTRANK ALGORITHM APPLIED
# load the pre-processed file (without textrank algorithm dict filter having been applied)
df_include <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
View(df_include)
# check number of unique words and doc_id
length(unique(df_include$token))
length(unique(df_paper$doc_id))
length(unique(df_include$doc_id))
length(unique(df_include$lemma))
df_include[df_include$token != df_include$lemma, ]
length(df_include[df_include$token != df_include$lemma, ])
# check number of unique words and doc_id
length(unique(df_include$token))
# check difference between tokens and lemma's
diff <- length(df_include[df_include$token != df_include$lemma, ])
# check difference between tokens and lemma's
diff <- df_include[df_include$token != df_include$lemma, ]
View(diff)
# check number of unique words
length(unique(df_include$lemma))
# check number of unique words
length(unique(df_include$lemma))
# check if there are only nouns and adj in this dataframe
unique(df_include$upos)
# check data frame before exclusion filter is applied
df_bf_ex <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df.RData")
View(df_bf_ex)
# only select nouns and adjectives
df_ex <- df_bf_ex[upos %in% c("NOUN", "ADJ"), ]
# check number of unique words
length(unique(df_include$lemma))
# check number of unique words
length(unique(df_bf_ex$lemma))
# only select nouns and adjectives
df_ex <- df_bf_ex[upos %in% c("NOUN", "ADJ"), ]
View(df_ex)
# check number of unique words
length(unique(df_ex$lemma))
df_ex1 <- df_ex[grepl("^[a-zA-Z].", df_kw$lemma), ]
df_ex1 <- df_ex[grepl("^[a-zA-Z].", df_ex$lemma), ]
df_ex[grepl("^[a-zA-Z].", df_ex$lemma), ]
df_ex[grepl("[^a-zA-Z].", df_ex$lemma), ]
# check number of unique words
length(unique(df_ex1$lemma))
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df_ex1$lemma)))
# check number of unique words
length(unique(df_bf_ex$lemma))
# check number of unique words
length(unique(df_ex$lemma))
25535 - 16311
# check number of unique words
length(unique(df_ex1$lemma))
16311 - 15515
df_ex2 <- df_ex1[-exclude_these, ]
# check number of unique words
length(unique(df_ex1$lemma))
# check number of unique words
length(unique(df_ex2$lemma))
15515 - 15274
# check number of unique words and number of unique documents and add a term frequency per doc column
nounbydoc4<- df_include[, list(freq = .N), by = list(doc_id = doc_id, term = lemma)]
View(nounbydoc4)
# check number of unique words and documents.
length(unique(nounbydoc4$doc_id))
length(unique(nounbydoc4$term))
# create a data frame with one column including the unique terms
include_these2 <- unique(nounbydoc4$term)
include_these2 <- as.data.table(include_these)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
# create a data frame with one column including the unique terms
include_these2 <- unique(nounbydoc4$term)
include_these2 <- as.data.table(include_these)
include_these2 <- as.data.table(include_these2)
View(include_these2)
saveRDS(include_these2, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_these2.RData")
View(include_these2)
# load libraries
library(dplyr)
library(cluster)
library(ggplot2)
library(devtools)
# load data frame with word vectors into object
df <- readRDS("featurematrix_asreview.RData")
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# load data frame with word vectors into object
df <- readRDS("featurematrix_asreview.RData")
# load data frame with column with selection of words to include in analysis
df_include <- readRDS("include_these2.RData")
# load data frame with word vectors into object
df <- readRDS("featurematrix_asreview.RData")
# load data frame with column with selection of words to include in analysis
df_include <- readRDS("include_these2.RData")
View(df_include)
# filter the df so that only the words that we want to be included are left
df_final <- df[df_include$include_these2, ]
View(df_include)
# drop all words that were in the include these object but not in the data frame
df_final <- na.omit(df_final)
View(df_final)
# load data frame with word vectors into object
df <- readRDS("featurematrix_asreview.RData")
# load data frame with column with selection of words to include in analysis
df_include <- readRDS("include_these2.RData")
# filter the df so that only the words that we want to be included are left
df_final <- df[df_include$include_these, ]
df_final[Na1, ]
View(df)
View(df_final)
df_final[NA, ]
View(df_include)
# words that should be included but are not in the data frame with the words embeddings.
df_lost <- df[!df_include$include_these, ]
View(df_final)
# drop all words that were in the include these object but not in the word embedding data frame
df_final <- na.omit(df_final)
# check words that should be included but are not in the data frame with the words embeddings and therefore lost.
df_lost <- df_include[df_include$include_these2 =! rownames(df_final), ]
# check words that should be included but are not in the data frame with the words embeddings and therefore lost.
df_lost <- df_include[df_include$include_these2 != rownames(df_final), ]
# check words that should be included but are not in the data frame with the words embeddings and therefore lost.
df_lost <- subset(df_include, !(include_these2 %in% rownames(df_final)))
View(df_lost)
12720+3268
length(unique(df_lost$include_these2))
View(df_final)
15273-3268
# load data frame with word vectors into object
df <- readRDS("featurematrix_asreview.RData")
# load data frame with column with selection of words to include in analysis
df_include <- readRDS("include_these2.RData")
df["bias", ]
df["unpredictability", ]
df["feature", ]
df["featur", ]
df["data", ]
df["artist", ]
df["score", ]
df["failure", ]
df["impairment", ]
df["autism", ]
# filter the df so that only the words that we want to be included are left
df_final <- df[df_include$include_these, ]
# drop all words that were in the include these object but not in the word embedding data frame
df_final <- na.omit(df_final)
# check how many NA's there are
length(df_final[NA, ])
# load data frame with word vectors into object
df <- readRDS("featurematrix_asreview.RData")
# load data frame with column with selection of words to include in analysis
df_include <- readRDS("include_these2.RData")
# filter the df so that only the words that we want to be included are left
df_final <- df[df_include$include_these, ]
# check how many NA's there are
length(df_final[NA, ])
df_final[NA, ])
df_final[NA, ]
# check how many NA's there are
length(is.na(df_final))
# check how many NA's there are
length(is.na(df_final$dim1))
# check how many NA's there are
sum(is.na(df_final$dim1))
# drop all words that were in the include these object but not in the word embedding data frame
df_final1 <- na.omit(df_final)
# check how many NA's there are
sum(is.na(df_final$dim2))
# check how many NA's there are
summary(df_final)
12720 + 2554
# drop all words that were in the include these object but not in the word embedding data frame
df_final1 <- na.omit(df_final)
# check words that should be included but are not in the data frame with the words embeddings and therefore lost.
df_lost <- subset(df_include, !(include_these2 %in% rownames(df_final)))
# check words that should be included but are not in the data frame with the words embeddings and therefore lost.
df_lost <- subset(df_include, !(include_these2 %in% rownames(df_final1)))
# number of observations makes sense 15274 (words to be include) - 2554 (number of missings) = 12720 observations
unique(rownames(df_final1))
# number of observations makes sense 15274 (words to be include) - 2554 (number of missings) = 12720 observations
length(unique(rownames(df_final1)))
df["trans", ]
df["tran", ]
df["us", ]
df["uk", ]
df["netherlands", ]
# other way to check words that are not included
df_check <- df
df_check$words <- row.names(df_check)
View(df_check)
df_add <- df_include
df_add <- df_include
names(df_add)[names(df_add) == 'include_these2'] <- 'words'
View(df_add)
df_check$mismatch <- ifelse(df_check$words != df_add$words, 1, 0)
df_check$mismatch <- ifelse(!(include_these2 %in% rownames(df_final1)), 1, 0)
df_check$mismatch <- ifelse(!(df_check$word %in% df_add$words), 1, 0)
summary(df_check$mismatch)
as.factor(summary(df_check$mismatch))
summary(as.factor(df_check$mismatch))
df_check$mismatch <- ifelse(df_check$word %in% df_add$words, 1, 0)
summary(as.factor(df_check$mismatch))
df_check$mismatch <- ifelse(df_check$words %in% df_add$words, 1, 0)
summary(as.factor(df_check$mismatch))
View(df_check)
merge(df_check, df_add)
df_checkfinal <- merge(df_check, df_add)
summary(df_checkfinal)
View(df_checkfinal)
df_checkfinal <- merge(df_add, df_check,)
summary(df_checkfinal)
df_check$mismatch <- ifelse(df_check$words[df_check$words %in% df_add$words], 1, 0)
df_check$mismatch <- ifelse(df_check$words %in% df_add$words, 1, 0)
summary(as.factor(df_check$mismatch))
12006+3268
df_check$mismatch2 <- ifelse(df_check$words %in% rownames(df_final1), 1, 0)
summary(as.factor(df_check$mismatch2))
12635+6841
12006 + 7470
df_check$mismatch2 <- ifelse(df_check$words %in% rownames(df_final), 1, 0)
summary(as.factor(df_check$mismatch2))
summary(as.factor(df_check$mismatch2))
# load data frame with word vectors into object
df <- readRDS("featurematrix_asreview.RData")
# load data frame with column with selection of words to include in analysis
df_include <- readRDS("include_these2.RData")
View(df_include)
df_check[df_check$mismatch == 0 & df_check$words %in% rownames(df_final1), "words"]
# filter the df so that only the words that we want to be included are left
df_final <- df[df_include$include_these, ]
View(df_final)
View(df)
View(df_include)
df_include[df_include$include_these2 == "emotional", ]
df_final["emotional", ]
# check how many NA's there are
summary(df_final)
sum(is.na(df_final$dim2))
# drop all words that were in the include these object but not in the word embedding data frame
df_final1 <- na.omit(df_final)
# number of observations makes sense 15274 (words to be include) - 2554 (number of missings) = 12720 observations
length(unique(rownames(df_final1)))
# check words that should be included but are not in the data frame with the words embeddings and therefore lost.
df_lost <- subset(df_include, !(include_these2 %in% rownames(df_final1)))
# check lost in other way
df_lost <- df[!df_include$include_these, ]
# check lost in other way
df_lost <- df[-df_include$include_these, ]
# check lost in other way
df_lost <- df[!(df_include$include_these), ]
# check lost in other way
df_lost <- df[rownames(df) != df_include$include_these, ]
# check words that should be included but are not in the data frame with the words embeddings and therefore lost.
df_lost <- subset(df_include, !(include_these2 %in% rownames(df_final1)))
View(df_lost)
View(df_final1)
worcs::git_update()
