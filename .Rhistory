kmeans_col
# create function to find similar words based on cosine distance
create_cluster_df <- function(df_with_cluster_assignment, cluster_to_subset) {
kmeans_col <- cat(colnames(words_with_cluster[2]), "\n")
subset(df_with_cluster_assignment, subset= kmeans_col == cluster_to_subset)
print(kmeans_col)
}
create_cluster_df(words_with_cluster,89)
words_with_cluster[2]
words_with_cluster[1,2]
words_with_cluster[0,2]
words_with_cluster[0,1]
colnames(words_with_cluster[2])
# probeer andere aanpak for subsetten.
cluster1 <- subset(words_with_cluster, subset=kmeans100 == 1)
cluster1_1 <- words_with_cluster[, words_with_cluster$kmeans100 == 1]
View(cluster1)
cluster1_1 <- words_with_cluster[, words_with_cluster$kmeans100 == 1]
cluster1_1 <- subset(words_with_cluster, kmeans100 == 1)
# create function to find similar words based on cosine distance
create_cluster_df <- function(df_with_cluster_assignment, cluster_to_subset) {
kmeans_col <- cat(colnames(words_with_cluster[2]), "\n")
subset(df_with_cluster_assignment, kmeans_col == cluster_to_subset)
}
noquote(colnames(words_with_cluster[2]))
cat(colnames(words_with_cluster[2]), "\n")
print(colnames(words_with_cluster[2]), quote = FALSE)
str(noquote(colnames(words_with_cluster[2])))
create_cluster_df(words_with_cluster,89)
create_cluster_df(words_with_cluster,89)
# create function to find similar words based on cosine distance
create_cluster_df <- function(df_with_cluster_assignment, cluster_to_subset) {
#kmeans_col <- cat(colnames(words_with_cluster[2]), "\n")
subset(df_with_cluster_assignment, cat(colnames(words_with_cluster[2]), "\n") == cluster_to_subset)
}
create_cluster_df(words_with_cluster,89)
cluster1_1 <- words_with_cluster[words_with_cluster$kmeans100 == 1, ]
create_cluster_df <- function(df_with_cluster_assignment, cluster_to_subset) {
kmeans_col <- cat(colnames(words_with_cluster[2]), "\n")
df_with_cluster_assignmen[df_with_cluster_assignmen$kmeans_col == 1, ]
}
create_cluster_df(words_with_cluster,89)
create_cluster_df <- function(df_with_cluster_assignment, cluster_to_subset) {
kmeans_col <- cat(colnames(words_with_cluster[2]), "\n")
df_with_cluster_assignmen[df_with_cluster_assignment$kmeans_col == 1, ]
}
create_cluster_df(words_with_cluster,89)
create_cluster_df <- function(df_with_cluster_assignment, cluster_to_subset) {
kmeans_col <- cat(colnames(words_with_cluster[2]), "\n")
df_with_cluster_assignment[df_with_cluster_assignment$kmeans_col == 1, ]
}
create_cluster_df(words_with_cluster,89)
create_cluster_df <- function(df_with_cluster_assignment, cluster_to_subset) {
kmeans_col <- noquote(colnames(words_with_cluster[2]))
df_with_cluster_assignment[df_with_cluster_assignment$kmeans_col == 1, ]
}
create_cluster_df(words_with_cluster,89)
create_cluster_df <- function(df_with_cluster_assignment, cluster_to_subset) {
df_with_cluster_assignment[df_with_cluster_assignment[,2] == 1, ]
}
create_cluster_df(words_with_cluster,89)
# load glove word embedding file
glove_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_lemma.RData")
str(glove_embedding)
# load libraries
library(dplyr)
library(cluster)
library(ggplot2)
library(devtools)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
set.seed(88)
library(text2vec)
# create function to find similar words based on cosine distance
find_similar_words <- function(word, embedding_matrix, n = 5) {
similarities <- embedding_matrix[word, , drop = FALSE] %>%
sim2(embedding_matrix, y = ., method = "cosine")
similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}
# Run function on the word health and see the 25 closest words based on cosine similarity.
find_similar_words("white",glove_embedding,50)
library(rword2vec)
install.packages("rword2vec")
library(devtools)
install_github("mukul13/rword2vec")
library(rword2vec)
# load lemma filter
filter_lemma <- readRDS("filter_lemma.RData")
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# load lemma filter
filter_lemma <- readRDS("filter_lemma.RData")
# save as .csv
write.csv(filter_lemma,"/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/words_abstracts.csv", row.names = FALSE)
View(filter_lemma)
worcs::git_update()
worcs::git_update()
# WITHOUT DICT FILTER APPLIED
# load the pre-processed file (without dict filter)
df_paper <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_analyze_without_dict_filter.RData")
View(df_paper)
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev")
# Keyword extraction ------------------------------------------------------
df <- readRDS("study2_df.RData")
# Exclude words
# make sure to only include nouns and adjectives.
df_kw <- df[upos %in% c("NOUN", "ADJ"), ]
# make sure to only include strings that start with a letter. (. -> and end with any character?)
df_kw <- df_kw[grepl("^[a-zA-Z].", df_kw$lemma), ]
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
View(df_kw)
## check aantal dingen
df_kw[df_kw$lemma == "student", "lemma"]
df_kw[df_kw$lemma == "students", "lemma"]
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df_kw$lemma)))
df_kw <- df_kw[-exclude_these, ]
## check if these terms ar indeed excluded after applying filter
df_kw[df_kw$lemma == "student", "lemma"]
df_kw[df_kw$lemma == "students", "lemma"]
# textrank is used for identifying more meaningful units of analysis
# DO NOT UNDERSTAND WHY upos %in% c("NOUN", "ADJ") line is applied again here because I thought this was already used to create df_kw in the first place.
kw_tr <- textrank_keywords(x = df_kw$lemma[df_kw$upos %in% c("NOUN", "ADJ")], ngram_max = 3, sep = " ")
library(textrank)
# textrank is used for identifying more meaningful units of analysis
# DO NOT UNDERSTAND WHY upos %in% c("NOUN", "ADJ") line is applied again here because I thought this was already used to create df_kw in the first place.
kw_tr <- textrank_keywords(x = df_kw$lemma[df_kw$upos %in% c("NOUN", "ADJ")], ngram_max = 3, sep = " ")
View(df_kw)
View(df_kw)
# Merge back with original data
df_kw$keyword <- txt_recode_ngram(df_kw$lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = " ")
library(udpipe)
# Merge back with original data
df_kw$keyword <- txt_recode_ngram(df_kw$lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = " ")
View(df_kw)
View(kw_tr)
kw_tr[["keywords"]][["ngram"]]
View(df_paper)
View(kw_tr)
kw_tr[["keywords"]][["keyword"]]
View(df_kw)
df_kw$keyword[!df_kw$keyword %in% kw_tr$keywords$keyword] <- NA
View(df_kw)
View(df_kw)
df_kw[df_kw$keyword == NA, "lemma"]
View(df_kw)
df_kw[is.na(df_kw$keyword), "lemma"]
df_kw[is.na(df_kw$keyword), c("lemma", "token")]
length(is.na(df_kw$keyword))
kyw_na <- df_kw[is.na(df_kw$keyword), c("lemma", "token")]
# NOTE this time we do not want to apply the dictionary filter.
df_analyze <- df_kw[!is.na(df_kw$keyword), ]
391930 - 373307
View(df_analyze)
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# add lemma_filter column to dataframe so we keep the original column and filter can be applied on the new column.
df_check$filter_lemma <- df_check$lemma
# SPLIT AT -
library(splitstackshape)
# Split words in lemma column on the -
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "-", direction = "long")
# create data frame on which preprocessing will be applied
lemma_clean <- df_check
# add lemma_filter column to dataframe so we keep the original column and filter can be applied on the new column.
lemma_clean$filter_lemma <- df_check$lemma
# Split words in lemma column on the -
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "-", direction = "long")
# check results by looking at a term that contains - in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "non-risk", c("lemma", "filter_lemma", "doc_id")]
# SPLIT AT /
# Split words in lemma column on the /
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "/", direction = "long")
# check results by looking at term that contains / in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "police/judicial", c("lemma", "filter_lemma", "doc_id")]
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# load libraries
library(stringr)
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# check results by looking at term that contains a non-ASCII character in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# remove all the digit characters from words in lemma column
lemma_clean$filter_lemma <- gsub("[0-9]+" ,"", lemma_clean$filter_lemma)
# check results by looking at term that contains a number in the original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# check number of unique words after applying more filters
length(unique(lemma_clean$filter_lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
lemma_clean <- lemma_clean %>%
anti_join(stop_words, by= c("filter_lemma" = "word"))
# check number of unique words after removing stop words
length(unique(lemma_clean$filter_lemma))
13932 - 13620
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
exclude_these2 <- unique(unlist(lapply(exclude_terms, grep, x = lemma_clean$filter_lemma)))
lemma_clean[271560, ]
lemma_clean <- lemma_clean[-exclude_these2, ]
# check number of unique words after excluding terms
length(unique(lemma_clean$filter_lemma))
13620 - 13524
# SAVE COMPLETE DATA FRAME INCLUDING FILTER_LEMMA COLUMN
saveRDS(lemma_clean, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/study2_df_lemma.RData")
# SAVE INCLUDE_LEMMA
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_lemma <- lemma_clean[, list(freq = .N), by = list(doc_id = doc_id, term = filter_lemma)]
# check number of unique words and documents.
length(unique(nounbydoc_lemma$doc_id))
length(unique(nounbydoc_lemma$term))
# check
# create a data frame with one column including the unique terms
filter_lemma <- unique(nounbydoc_lemma$term)
filter_lemma <- as.data.table(filter_lemma)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
filter_lemma <- as.data.table(filter_lemma)
saveRDS(filter_lemma, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/final_filter.RData")
View(lemma_clean)
bigrams_df <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")],
ngram_max = 2, sep = "-")
View(bigrams_df)
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "-")
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "-")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "-")
View(lemma_clean)
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "_")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "_")
View(lemma_clean)
View(lemma_clean)
bigrams_data <- melt(lemma_clean, id.vars=c("filter_lemma","keyword"),
value.name = "filter_lemma")
View(bigrams_data)
View(bigrams_data)
View(bigrams_data)
bigrams_data <- melt(lemma_clean, id.vars=c("filter_lemma","keyword"),
value.name = "filter_bigrams")
View(bigrams_data)
bigrams_data <- melt(lemma_clean,
id.vars=c("filter_lemma","keyword"),
var = "filter_bigrams")
str(lemma_clean)
View(bigrams_data)
View(lemma_clean)
bigrams_data <- cbind(lemma_clean[1:14], stack(lemma_clean[15:16]))
View(bigrams_data)
View(lemma_clean)
# some visualization of text rank
stats <- subset(kw_tr$keywords, ngram > 1 & freq >= 5)
library(wordcloud)
wordcloud(words = kw_tr$keyword, freq = kw_tr$freq)
View(kw_tr)
wordcloud(words = kw_tr$keyword, freq = kw_tr$keywords$freq)
head(stats, 5)
head(stats, 15)
head(stats, 30)
bigrams_data <- melt(lemma_clean, id.var = [1:14], variable.name = 'filter_final')
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'filter_final')
View(bigrams_data)
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
View(bigrams_data)
# SAVE filter with lemma
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id, term = value)]
# check number of unique words and documents.
length(unique(nounbydoc_lemma$bigrams))
# check number of unique words and documents.
length(unique(nounbydoc_bigrams$doc_id))
length(unique(nounbydoc_bigrams$term))
length(unique(nounbydoc_lemma$term))
# check
# create a data frame with one column including the unique terms
filter_bigrams <- unique(nounbydoc_bigrams$term)
filter_bigrams <- as.data.table(filter_bigrams)
saveRDS(filter_lemma, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/bigrams_filter.RData")
# load final filter
final_filter <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/final_filter.RData")
# save as .csv
write.csv(filter_lemma,"/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/final_filter.csv", row.names = FALSE)
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
# load data frame with column with selection of words to include in analysis.
df_filter <- readRDS("final_filter.RData")
View(df_filter)
# create df in which only the words that we want to be included are kept
filtered_embedding <- subset(vectors_glove, word %in% df_filter$filter_lemma)
View(vectors_glove)
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
# create df in which only the words that we want to be included are kept
filtered_embedding <- subset(vectors_glove, word %in% df_filter$filter_lemma)
# check number of unique words
length(unique((filtered_embedding$word)))
# non-GloVe: check which words are in the final filter but are not in the glove embedding data and are therefore lost (unwanted).
lost_words <- subset(df_filter, !(filter_lemma %in% filtered_embedding$word))
length(unique((filtered_embedding$word))) + length(unique((lost_words$filter_lemma)))
# -> 13711. Which is equal to the line of code below.
length(unique((df_filter$filter_lemma)))
glove_embedding <- filtered_embedding %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
glove_embedding <- as.matrix(glove_embedding)
str(glove_embedding)
# save the glove embedding to which the filter is applied so this can be easily loaded into other scripts
saveRDS(glove_embedding, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_final.RData")
glove_embedding <-  readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_final.RData")
# load final filter
final_filter <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/final_filter.RData")
# save as .csv
write.csv(filter_lemma,"/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/final_filter.csv", row.names = FALSE)
# save as .csv
write.csv(final_filter,"/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/final_filter.csv", row.names = FALSE)
# load bigrams filter
bigrams_filter <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/bigrams_filter.RData")
# save as .csv
write.csv(bigrams_filter,"/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/bigrams_filter.csv", row.names = FALSE)
worcs::git_update()
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# create data frame on which preprocessing will be applied
lemma_clean <- df_check
# add lemma_filter column to dataframe so we keep the original column and filter can be applied on the new column.
lemma_clean$filter_lemma <- df_check$lemma
# SPLIT AT -
library(splitstackshape)
# Split words in lemma column on the -
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "-", direction = "long")
# check results by looking at a term that contains - in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "non-risk", c("lemma", "filter_lemma", "doc_id")]
# SPLIT AT /
# Split words in lemma column on the /
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "/", direction = "long")
# check results by looking at term that contains / in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "police/judicial", c("lemma", "filter_lemma", "doc_id")]
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# load libraries
library(stringr)
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# check results by looking at term that contains a non-ASCII character in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# remove all the digit characters from words in lemma column
lemma_clean$filter_lemma <- gsub("[0-9]+" ,"", lemma_clean$filter_lemma)
# check results by looking at term that contains a number in the original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# check number of unique words after applying more filters
length(unique(lemma_clean$filter_lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
lemma_clean <- lemma_clean %>%
anti_join(stop_words, by= c("filter_lemma" = "word"))
# check number of unique words after removing stop words
length(unique(lemma_clean$filter_lemma))
exclude_these2 <- unique(unlist(lapply(exclude_terms, grep, x = lemma_clean$filter_lemma)))
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
exclude_these2 <- unique(unlist(lapply(exclude_terms, grep, x = lemma_clean$filter_lemma)))
lemma_clean <- lemma_clean[-exclude_these2, ]
# check number of unique words after excluding terms
length(unique(lemma_clean$filter_lemma))
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "_")
library(textrank)
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "_")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "_")
library(udpipe)
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "_")
# investigating text rank results
stats <- subset(kw_tr$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
library(reshape2)
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
# SAVE filter with lemma
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id, term = value)]
View(bigrams_data)
# SAVE filter with lemma
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id, term = value)]
View(bigrams_data)
# SAVE FILTER_LEMMA
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_lemma <- lemma_clean[, list(freq = .N), by = list(doc_id = doc_id, term = filter_lemma)]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id, term = value)]
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id, term = value)]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id,
term = value)]
# load libraries
library(stringr)
library(udpipe)
library(Matrix)
library(yaml)
library(Rmpfr)
library(topicmodels)
library(udpipe)
library(slam)
library(tidytext)
library(textrank)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# create data frame on which preprocessing will be applied
lemma_clean <- df_check
# add lemma_filter column to dataframe so we keep the original column and filter can be applied on the new column.
lemma_clean$filter_lemma <- df_check$lemma
# SPLIT AT -
library(splitstackshape)
# Split words in lemma column on the -
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "-", direction = "long")
# check results by looking at a term that contains - in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "non-risk", c("lemma", "filter_lemma", "doc_id")]
# SPLIT AT /
# Split words in lemma column on the /
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "/", direction = "long")
# check results by looking at term that contains / in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "police/judicial", c("lemma", "filter_lemma", "doc_id")]
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# check results by looking at term that contains a non-ASCII character in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# remove all the digit characters from words in lemma column
lemma_clean$filter_lemma <- gsub("[0-9]+" ,"", lemma_clean$filter_lemma)
# check results by looking at term that contains a number in the original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# check number of unique words after applying more filters
length(unique(lemma_clean$filter_lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
lemma_clean <- lemma_clean %>%
anti_join(stop_words, by= c("filter_lemma" = "word"))
# check number of unique words after removing stop words
length(unique(lemma_clean$filter_lemma))
exclude_these2 <- unique(unlist(lapply(exclude_terms, grep, x = lemma_clean$filter_lemma)))
lemma_clean <- lemma_clean[-exclude_these2, ]
# check number of unique words after excluding terms
length(unique(lemma_clean$filter_lemma))
# SAVE FILTER_LEMMA
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_lemma <- lemma_clean[, list(freq = .N), by = list(doc_id = doc_id, term = filter_lemma)]
# check number of unique words and documents.
length(unique(nounbydoc_lemma$doc_id))
length(unique(nounbydoc_lemma$term))
# check
# create a data frame with one column including the unique terms
filter_lemma <- unique(nounbydoc_lemma$term)
filter_lemma <- as.data.table(filter_lemma)
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "_")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "_")
# investigating text rank results
stats <- subset(kw_tr$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
library(reshape2)
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id,
term = value)]
# check number of unique words and documents.
length(unique(nounbydoc_bigrams$doc_id))
length(unique(nounbydoc_bigrams$term))
# check
# create a data frame with one column including the unique terms
filter_bigrams <- unique(nounbydoc_bigrams$term)
filter_bigrams <- as.data.table(filter_bigrams)
View(filter_bigrams)
saveRDS(filter_bigrams, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/bigrams_filter.RData")
worcs::git_update()
