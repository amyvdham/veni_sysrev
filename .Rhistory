df_new[df_new$doc == 3667, 'DEclean']
# check if this is a result of data cleaning by looking at the original data set
recs[recs$doc == 3667, 'DE']
# -> shows that this is not a result of data cleaning and it seems that this article does really have 524 author keywords.
# NOTE: document/case/row shown below contains < in author keywords. Does this need to be cleaned?
df_new[df_new$doc == 5234, 'DEclean']
View(df)
unique(df$word)
df[df$doc == 5234, 'word']
length(unique(df$word))
# We need to tokenize our already tokenized set as input for text2vec, re-use cleaned text in df_new
it <- itoken(df_new$DEclean,
tokenizer = word_tokenizer,
ids = df_new$doc,
progressbar = TRUE)
# create a vocabulary out of the tokenset (stopword removal and bi-grams are optional)
vocab <- create_vocabulary(it) # use uni-grams
# text2vec has the option to prune the vocabulary of low-frequent words
vocab <- prune_vocabulary(vocab, term_count_min = 5)
# What's in the vocabulary?
print(vocab)
# Vectorize word to integers
vectorizer <- vocab_vectorizer(vocab)
# Create a Term-Count-Matrix, by default it will use a skipgram window of 5 (symmetrical)
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
# maximum number of co-occurrences to use in the weighting function, we choose the entire token set divided by 100
x_max <- length(vocab$doc_count)/100
# set up the embedding matrix and fit model
glove_model <- GloVe$new(rank = 32, x_max = x_max)
glove_embedding = glove_model$fit_transform(tcm, n_iter = 20, convergence_tol = 0.01, n_threads = 4)
# combine main embedding and context embeddings (sum) into one matrix
glove_embedding = glove_embedding + t(glove_model$components) # the transpose of the context matrix
# wat ligt er dicht bij 'school'
word <- glove_embedding["school", , drop = FALSE]
cos_sim = sim2(x = glove_embedding, y = word, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# wat ligt er dicht bij 'personality'
word <- glove_embedding["personality", , drop = FALSE]
cos_sim = sim2(x = glove_embedding, y = word, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# NOTE: document/case/row shown below contains < in author keywords. Does this need to be cleaned? It also includes the word literature whereas this word was part of the terms that had to be excluded.
df_new[df_new$doc == 5234, 'DEclean']
worcs::git_update()
# Loading required libraries
library(stringr)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(udpipe)
library(igraph)
library(wordcloud)
library(Matrix)
library(ggplot2)
library(yaml)
# makes it possible to call functions that are saved in separate R script
source("word_functions.R")
## Look at POS tags?
# reads file which contains the records into an object called recs
# this is were line 891 in manuscript file starts
recs <- read.csv("recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# if recs is not an object of type data table
# then code execution will be paused?
if(!is.data.table(recs)){
browser()
}
# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]
# create object that is a list with the dimensions of the recs data frame
study1details <- list(dim_recs = dim(recs))
View(study1details)
## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column.
df <- merge_df(recs, df, "word")
# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
# create an object with the number of unique documents (articles) in the data frame (df) and the number of unique (author key-) words in the data frame.
number_docs_words <- c(docs = length(unique(df$doc)), words =
length(unique(df$word)))
# save this information(# of articles and # of unique author keywords) in a yaml file
yaml::write_yaml(number_docs_words, "study1_number_docs_words.txt")
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
View(df)
unique(df$doc)
max(unique(df$doc))
length(unique(df$doc))
# makes it possible to call functions that are saved in separate R script
source("word_functions.R")
## Look at POS tags?
# reads file which contains the records into an object called recs
# this is were line 891 in manuscript file starts
recs <- read.csv("recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# if recs is not an object of type data table
# then code execution will be paused?
if(!is.data.table(recs)){
browser()
}
# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]
# create object that is a list with the dimensions of the recs data frame
study1details <- list(dim_recs = dim(recs))
## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column.
df <- merge_df(recs, df, "word")
# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
# create an object with the number of unique documents (articles) in the data frame (df) and the number of unique (author key-) words in the data frame.
number_docs_words <- c(docs = length(unique(df$doc)), words =
length(unique(df$word)))
# save this information(# of articles and # of unique author keywords) in a yaml file
yaml::write_yaml(number_docs_words, "study1_number_docs_words.txt")
# check how many unique documents and words there are before excluding terms.
length(unique(df$doc))
length(unique(df$word))
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
# check how many unique documents and how many unique words there are after excluding non substantive words.
length(unique(df$doc))
length(unique(df$word))
worcs::git_update()
# Loading required libraries
library(stringr)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(Matrix)
library(ggplot2)
library(dplyr)
# Required library for GloVe
library(text2vec)
# Loading script containing functions
source("word_functions.R")
# prepare data
# this is were line 891 in manuscript file starts
recs <- read.csv("recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]
## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column.
df <- merge_df(recs, df, "word")
# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
# check the number of author keywords per doc
df_n_keywords <- df %>%
group_by(doc) %>%
summarise(n_keywords = n()) # snap nog niet helemaal hoe deze code weet dat die n() dan betrekking heeft op de column word maar gaat wel goed.
# Now that the dataset is cleaned created a df in which each document is a row and the column DEclean contains the author keywords of the specific document. Shape the tokens back to their original form
df_new <- df %>%
group_by(word) %>%
group_by(doc) %>%
summarise(DEclean = str_c(word, collapse = " "))
# second attempt word count
df_new$DEclean_length <- sapply(df_new$DEclean, function(x){str_count(x, " ")+1})
df_new <- data.table(df_new)
# Max length of author keywords
max(df_new$DEclean_length)
# investigate with the highest value of number of author keywords
df_new[df_new$doc == 3667, 'DEclean']
# check if this is a result of data cleaning by looking at the original data set
recs[recs$doc == 3667, 'DE']
# NOTE: document/case/row shown below contains < in author keywords. Does this need to be cleaned? It also includes the word literature whereas this word was part of the terms that had to be excluded.
df_new[df_new$doc == 5234, 'DEclean']
# check why those words with literature were not excluded
df[df$doc == 5234, 'word']
# check how many unique documents and how many unique words there are after excluding non substantive words.
length(unique(df_new$doc))
length(unique(df_new$word))
length(unique(df$word))
# check how many unique documents and how many unique words there are after excluding non substantive words.
length(unique(df$doc))
## count the number of words per review and plot results
df_info <- df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(10,300,10),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_reviews)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_reviews), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df_info <- df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(10,300,10),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_reviews), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df_info <- df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(10,70,10),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_reviews), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df_info <- df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(10,70,10),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_reviews), position=position_dodge(width=0.9), vjust=-0.25)
View(df_info)
View(df_info)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(10,70,10),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_reviews), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(10,70,10),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(5,10,20),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(2,5,20),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(2,5,2),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(2,15,2),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(1,15,1),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(1,11,1),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
# We need to tokenize our already tokenized set as input for text2vec, re-use cleaned text in df_new
it <- itoken(df_new$DEclean,
tokenizer = word_tokenizer,
ids = df_new$doc,
progressbar = TRUE)
# create a vocabulary out of the tokenset (stopword removal and bi-grams are optional)
vocab <- create_vocabulary(it) # use uni-grams
# text2vec has the option to prune the vocabulary of low-frequent words
vocab <- prune_vocabulary(vocab, term_count_min = 5)
# What's in the vocabulary?
print(vocab)
# check word frequency
df_info <- df %>%
group_by(word) %>% summarize(word_freq=n()) %>%
mutate(min_5_freq = case_when(word_freq<5~'token frequency: <5',TRUE~'token frequency: >=5')) %>%
group_by(min_5_freq) %>% summarise(n_tokens = n()) %>% mutate(pct_tokens = n_tokens / sum(n_tokens))
View(df_info)
View(df_info)
# check word frequency: data_info contains information on how many of the articles contain more and less than 5 author keywords.
df %>%
group_by(word) %>% summarize(word_freq=n()) %>%
mutate(min_5_freq = case_when(word_freq<5~'token frequency: <5',TRUE~'token frequency: >=5')) %>%
group_by(min_5_freq) %>% summarise(n_tokens = n()) %>% mutate(pct_tokens = n_tokens / sum(n_tokens))
worcs::git_update()
# Loading required libraries
library(stringr)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(Matrix)
library(ggplot2)
library(dplyr)
# Required library for GloVe
library(text2vec)
# Loading script containing functions
source("word_functions.R")
# prepare data
# this is were line 891 in manuscript file starts
recs <- read.csv("recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]
## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column.
df <- merge_df(recs, df, "word")
# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
# check the number of author keywords per doc
df_n_keywords <- df %>%
group_by(doc) %>%
summarise(n_keywords = n()) # snap nog niet helemaal hoe deze code weet dat die n() dan betrekking heeft op de column word maar gaat wel goed.
## NOTE: some keywords contain more than one word, e.g. risk factors or follow-up studies and youth smoking behavior. So to now the lenght (number of words) per doc this won't work
# Now that the dataset is cleaned created a df in which each document is a row and the column DEclean contains the author keywords of the specific document. Shape the tokens back to their original form
df_new <- df %>%
group_by(word) %>%
group_by(doc) %>%
summarise(DEclean = str_c(word, collapse = " "))
# second attempt word count
df_new$DEclean_length <- sapply(df_new$DEclean, function(x){str_count(x, " ")+1})
# this gives us the number of words in the column DEclean for each row. NOTE: words that are connected by a - are seen as one word, e.g. follow-up or affect-regulatory. Also, abbreviations are also counted as a word, e.g. INFORMATION AND COMMUNICATION TECHNOLOGY (ICT). (ICT) is counted in this example.
df_new <- data.table(df_new)
# Max length of author keywords
max(df_new$DEclean_length)
# investigate with the highest value of number of author keywords
df_new[df_new$doc == 3667, 'DEclean']
# check if this is a result of data cleaning by looking at the original data set
recs[recs$doc == 3667, 'DE']
# -> shows that this is not a result of data cleaning and it seems that this article does really have 524 author keywords.
# NOTE: document/case/row shown below contains < in author keywords. Does this need to be cleaned? It also includes the word literature whereas this word was part of the terms that had to be excluded.
df_new[df_new$doc == 5234, 'DEclean']
# check why those words with literature were not excluded
df[df$doc == 5234, 'word']
# ->  they were not excluded because the regular expression ^literature$ tells to only delete those values that where start of the string is followed by literature followed by the end of the string.
# check how many unique documents and how many unique words there are after excluding non substantive words.
length(unique(df$doc))
length(unique(df$word))
# investigate with the highest value of number of author keywords
df_new[df_new$doc == 3667, 'DEclean']
View(df_new)
View(recs)
# check the doi of this article so I can look it up
recs[recs$doc == 3667, 'DI']
# -> does not have a doi so look at title
recs[recs$doc == 3667, 'TI']
# check the doi of this article so I can look it up
recs[recs$doc == 3667, 'doi']
# -> does not have a doi so look at title
recs[recs$doc == 3667, 'title']
# check the doi of this article so I can look it up
recs[recs$doc == 3667, c('doi', 'DI')]
# check the doi of this article so I can look it up
recs[recs$doc == 3667, c('doi', 'DI', 'title')]
# check the doi of this article so I can look it up
recs[recs$doc == 3667, c('doi', 'DI', 'title', 'AU')]
# check the doi of this article so I can look it up
recs[recs$doc == 3667, c('doi', 'DI', 'title', 'AU', 'PY')]
View(df_new)
# check article with 77 author keywords
recs[recs$doc == 5234, c('doi', 'DI', 'title', 'AU', 'PY')]
# check article with 77 author keywords
recs[recs$doc == 5234, c('doi', 'DI', 'title', 'AU', 'PY', 'DE')]
# check article with 33 author keywords
recs[recs$doc == 2750, c('doi', 'DI', 'title', 'AU', 'PY', 'DE')]
# check article with 33 author keywords
recs[recs$doc == 1186, c('doi', 'DI', 'title', 'AU', 'PY', 'DE')]
worcs::git_update()
# load .csv file into object
asreview_results <- read.csv("asreview_result_sysrevemotprob.csv")
# convert object into data table
asreview_results <- as.data.table(asreview_results)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
# convert object into data table
asreview_results <- as.data.table(asreview_results)
View(asreview_results)
unique(asreview_results$included)
lenght(asreview_results$included == 1)
length(asreview_results$included == 1)
length(asreview_results$included == 0)
length(asreview_results[asreview_results$included == 1])
length(asreview_results[asreview_results$included == 0])
length(asreview_results[asreview_results$included == NA])
View(asreview_results)
length(asreview_results[asreview_results$included == 0])
summary(asreview_results$included)
summary(as.factor(asreview_results$included))
asreview_results$asreview_ranking
asreview_results[, c('included', 'asreview_ranking', 'record_id')]
summary(as.factor(asreview_results$included))
823+278
559+541
max(asreview_results$asreview_ranking)
# check to recs file that is used in analysis to compare
recs <- read.csv("recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
View(recs)
# try to merge the datasets
total <- merge(asreview_results, recs, by="doi")
View(total)
total$included
summary(as.factor(total$included))
worcs::git_update()
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
# load .csv file into object
asreview_results <- read.csv("asreview_result_sysrevemotprob.csv")
# convert object into data table
asreview_results <- as.data.table(asreview_results)
unique(asreview_results$included)
summary(as.factor(asreview_results$included))
asreview_results[, c('included', 'asreview_ranking', 'record_id')]
max(asreview_results$asreview_ranking)
# check to recs file that is used in analysis to compare
recs <- read.csv("recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# try to merge the datasets
total <- merge(asreview_results, recs, by="doi")
total$included
summary(as.factor(total$included))
View(asreview_results)
View(total)
View(asreview_results)
worcs::git_update()
