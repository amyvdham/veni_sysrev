column_to_rownames(var = 'word')
View(w2v_embedding)
# convert dataframe to a matrix
w2v_embedding <- as.matrix(w2v_embedding)
str(glove_embedding)
str(w2v_embedding)
# save the glove embedding to which the filter is applied so this can be easily loaded into other scripts
saveRDS(w2v_embedding, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/w2v_embedding.RData")
w2v_bigrams_embedding <- df_bigrams %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
View(w2v_bigrams_embedding)
# convert dataframe to a matrix
w2v_bigrams_embedding <- as.matrix(w2v_bigrams_embedding)
str(w2v_bigrams_embedding)
# save the glove embedding to which the filter is applied so this can be easily loaded into other scripts
saveRDS(w2v_bigrams_embedding, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/w2v_bigrams_embedding.RData")
# load glove word embedding file
glove_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_final.RData")
str(glove_embedding)
# load glove word embedding file
w2v_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/w2v_embedding.RData")
# load glove word embedding file
w2v_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/w2v_embedding.RData")
str(glove_embedding)
str(w2v_embedding)
# load glove word embedding file
w2v_bigrams_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/w2v_bigrams_embedding.RData")
# load glove word embedding file
w2v_bigrams_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/w2v_bigrams_embedding.RData")
str(w2v_bigrams_embedding)
library(text2vec)
# create function to find similar words based on cosine distance
find_similar_words <- function(word, embedding_matrix, n = 5) {
similarities <- embedding_matrix[word, , drop = FALSE] %>%
sim2(embedding_matrix, y = ., method = "cosine")
similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}
# Run function on the word health and see the 25 closest words based on cosine similarity.
find_similar_words("white",glove_embedding,50)
find_similar_words("white",w2v_embedding,50)
find_similar_words("white",w2v_bigrams_embedding,50)
View(glove_embedding)
glove_embedding["monocultural", ]
find_similar_words("mental_health",w2v_bigrams_embedding,50)
# fit the k-means clustering with 75 clusters, glove
kmeans_fit75 <- kmeans(glove_embedding, 75, iter.max = 30, nstart = 25)
# fit the k-means clustering with 75 clusters, w2v
kmeans_fit75_w2v <- kmeans(w2v_embedding, 75, iter.max = 30, nstart = 25)
# fit the k-means clustering with 75 clusters, w2v bigrams
kmeans_fit75_bigr <- kmeans(w2v_bigrams_embedding, 75, iter.max = 30, nstart = 25)
# results
kmeans_fit75
# 25.2%
kmeans_fit75_w2v
# 20.2%
k_means_fit75_bigr
# 20.2%
kmeans_fit75_bigr
# Create data frame in which the cluster assignment is merged back to rows/word.
words_with_cluster75 <- as.data.frame(cbind(row.names(glove_embedding), kmeans_fit75$cluster))
# add column names
names(words_with_cluster) <- c("word", "kmeans75")
# add column names
names(words_with_cluster75) <- c("word", "kmeans75")
# Create data frame in which the cluster assignment is merged back to rows/word.
words_with_cluster75_w2v <- as.data.frame(cbind(row.names(w2v_embedding), kmeans_fit75_w2v$cluster))
# add column names
names(words_with_cluster75_w2v) <- c("word", "kmeans75")
# Create data frame in which the cluster assignment is merged back to rows/word.
words_with_cluster75_w2v_bigr <- as.data.frame(cbind(row.names(w2v_bigrams_embedding), kmeans_fit75_bigr$cluster))
# add column names
names(words_with_cluster_w2v_bigr) <- c("word", "kmeans75")
# add column names
names(words_with_cluster75_w2v_bigr) <- c("word", "kmeans75")
# Mother
# find out in which cluster the word mother is assigned
words_with_cluster[words_with_cluster75$word == "mental", ]
# Mother
# find out in which cluster the word mother is assigned
words_with_cluster75[words_with_cluster75$word == "mental", ]
words_with_cluster75_w2v[words_with_cluster75_w2v$word == "mental", ]
words_with_cluster75_w2v_bigr[words_with_cluster75_w2v_bigr$word == "mental", ]
cluster_mental_glv <- subset(words_with_cluster75, subset=kmeans75 == 9)
cluster2_mental_w2v <- subset(words_with_cluster75_w2v, subset=kmeans75 == 8)
cluster3_mental_w2v_bigr <- subset(words_with_cluster75_w2v_bigr, subset=kmeans75 == 34)
View(cluster_mental_glv)
View(cluster2_mental_w2v)
View(cluster3_mental_w2v_bigr)
# find out in which cluster the word mother is assigned
words_with_cluster75[words_with_cluster75$word == "mother", ]
words_with_cluster75_w2v[words_with_cluster75_w2v$word == "mother", ]
words_with_cluster75_w2v_bigr[words_with_cluster75_w2v_bigr$word == "mother", ]
cluster_mental_glv <- subset(words_with_cluster75, subset=kmeans75 == 74)
cluster2_mental_w2v <- subset(words_with_cluster75_w2v, subset=kmeans75 == 19)
cluster3_mental_w2v_bigr <- subset(words_with_cluster75_w2v_bigr, subset=kmeans75 == 43)
cluster_mother_glv <- subset(words_with_cluster75, subset=kmeans75 == 74)
cluster_mother_w2v <- subset(words_with_cluster75_w2v, subset=kmeans75 == 19)
cluster_mother_w2v_bigr <- subset(words_with_cluster75_w2v_bigr, subset=kmeans75 == 43)
View(cluster_mental_glv)
View(cluster_mother_glv)
View(cluster_mother_w2v)
View(cluster_mother_w2v_bigr)
cluster_mental_w2v_bigr <- subset(words_with_cluster75_w2v_bigr, subset=kmeans75 == 34)
View(cluster_mental_w2v_bigr)
# load glove vectors into R
vectors <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
vectors[vectors$word == "mental-health", ]
View(vectors)
# rename the columns
colnames(vectors) <- c('word',paste('dim',1:300,sep = '_'))
vectors[vectors$word == "risk-factor", ]
vectors[vectors$word == "mental-health", ]
# mental-health
vectors[142239, ]
vectors[vectors$word == "emotional-dysregulation", ]
vectors[vectors$word == "emotional-dysregulation", ]
# load libraries
library(stringr)
library(udpipe)
library(Matrix)
library(yaml)
library(Rmpfr)
library(topicmodels)
library(udpipe)
library(slam)
library(tidytext)
library(textrank)
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# create data frame on which preprocessing will be applied
lemma_clean <- df_check
# add lemma_filter column to dataframe so we keep the original column and filter can be applied on the new column.
lemma_clean$filter_lemma <- df_check$lemma
# SPLIT AT -
library(splitstackshape)
# Split words in lemma column on the -
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "-", direction = "long")
# check results by looking at a term that contains - in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "non-risk", c("lemma", "filter_lemma", "doc_id")]
# SPLIT AT /
# Split words in lemma column on the /
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "/", direction = "long")
# check results by looking at term that contains / in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "police/judicial", c("lemma", "filter_lemma", "doc_id")]
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# check results by looking at term that contains a non-ASCII character in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# remove all the digit characters from words in lemma column
lemma_clean$filter_lemma <- gsub("[0-9]+" ,"", lemma_clean$filter_lemma)
# check results by looking at term that contains a number in the original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# check number of unique words after applying more filters
length(unique(lemma_clean$filter_lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
lemma_clean <- lemma_clean %>%
anti_join(stop_words, by= c("filter_lemma" = "word"))
# check number of unique words after removing stop words
length(unique(lemma_clean$filter_lemma))
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
exclude_these2 <- unique(unlist(lapply(exclude_terms, grep, x = lemma_clean$filter_lemma)))
lemma_clean <- lemma_clean[-exclude_these2, ]
# check number of unique words after excluding terms
length(unique(lemma_clean$filter_lemma))
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "_")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "_")
# investigating text rank results
stats <- subset(kw_tr$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
library(reshape2)
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
kw_tr_glove <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "-")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr_glove$keywords$keyword, ngram = kw_tr_glove$keywords$ngram, sep = "_")
# investigating text rank results
stats <- subset(kw_tr_glove$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
View(bigrams_data)
View(bigrams_data)
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr_glove$keywords$keyword, ngram = kw_tr_glove$keywords$ngram, sep = "-")
# investigating text rank results
stats <- subset(kw_tr_glove$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
View(bigrams_data)
# SAVE filter with bigrams seperated with - instead of _ to apply on glove embedding
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams_glove <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id,
term = value)]
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# create data frame on which preprocessing will be applied
lemma_clean <- df_check
# add lemma_filter column to dataframe so we keep the original column and filter can be applied on the new column.
lemma_clean$filter_lemma <- df_check$lemma
# Split words in lemma column on the -
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "-", direction = "long")
# check results by looking at a term that contains - in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "non-risk", c("lemma", "filter_lemma", "doc_id")]
# SPLIT AT /
# Split words in lemma column on the /
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "/", direction = "long")
# check results by looking at term that contains / in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "police/judicial", c("lemma", "filter_lemma", "doc_id")]
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# check results by looking at term that contains a non-ASCII character in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# remove all the digit characters from words in lemma column
lemma_clean$filter_lemma <- gsub("[0-9]+" ,"", lemma_clean$filter_lemma)
# check results by looking at term that contains a number in the original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# check number of unique words after applying more filters
length(unique(lemma_clean$filter_lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
lemma_clean <- lemma_clean %>%
anti_join(stop_words, by= c("filter_lemma" = "word"))
# check number of unique words after removing stop words
length(unique(lemma_clean$filter_lemma))
exclude_these2 <- unique(unlist(lapply(exclude_terms, grep, x = lemma_clean$filter_lemma)))
lemma_clean <- lemma_clean[-exclude_these2, ]
# check number of unique words after excluding terms
length(unique(lemma_clean$filter_lemma))
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "_")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "_")
# investigating text rank results
stats <- subset(kw_tr$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id,
term = value)]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id,
term = value)]
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id,
term = value)]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id,
term = value)]
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# create data frame on which preprocessing will be applied
lemma_clean <- df_check
# add lemma_filter column to dataframe so we keep the original column and filter can be applied on the new column.
lemma_clean$filter_lemma <- df_check$lemma
# SPLIT AT -
library(splitstackshape)
# Split words in lemma column on the -
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "-", direction = "long")
# check results by looking at a term that contains - in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "non-risk", c("lemma", "filter_lemma", "doc_id")]
# SPLIT AT /
# Split words in lemma column on the /
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "/", direction = "long")
# check results by looking at term that contains / in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "police/judicial", c("lemma", "filter_lemma", "doc_id")]
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# load libraries
library(stringr)
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# check results by looking at term that contains a non-ASCII character in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# remove all the digit characters from words in lemma column
lemma_clean$filter_lemma <- gsub("[0-9]+" ,"", lemma_clean$filter_lemma)
# check results by looking at term that contains a number in the original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# check number of unique words after applying more filters
length(unique(lemma_clean$filter_lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
lemma_clean <- lemma_clean %>%
anti_join(stop_words, by= c("filter_lemma" = "word"))
# check number of unique words after removing stop words
length(unique(lemma_clean$filter_lemma))
exclude_these2 <- unique(unlist(lapply(exclude_terms, grep, x = lemma_clean$filter_lemma)))
lemma_clean <- lemma_clean[-exclude_these2, ]
# check number of unique words after excluding terms
length(unique(lemma_clean$filter_lemma))
# SAVE FILTER_LEMMA
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_lemma <- lemma_clean[, list(freq = .N), by = list(doc_id = doc_id, term = filter_lemma)]
# check number of unique words and documents.
length(unique(nounbydoc_lemma$doc_id))
length(unique(nounbydoc_lemma$term))
# check
# create a data frame with one column including the unique terms
filter_lemma <- unique(nounbydoc_lemma$term)
filter_lemma <- as.data.table(filter_lemma)
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "_")
library(textrank)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "_")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "_")
library(udpipe)
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "_")
# investigating text rank results
stats <- subset(kw_tr$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
library(reshape2)
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id,
term = value)]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id,
term = value)]
source("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/word_functions.R")
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id,
term = value)]
View(bigrams_data)
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id, term = value)]
# load libraries
library(stringr)
library(udpipe)
library(Matrix)
library(yaml)
library(Rmpfr)
library(topicmodels)
library(udpipe)
library(slam)
library(tidytext)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[, list(freq = .N), by = list(doc_id = doc_id, term = value)]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id, term = value)]
# check number of unique words and documents.
length(unique(nounbydoc_bigrams$doc_id))
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
list(freq = .N),
list(doc_id = doc_id, term = value)]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id, term = value)]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
(list(freq = .N),
by = list(doc_id = doc_id, term = value))]
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id, term = value)]
is.data.table(bigrams_data)
bigrams_data <- as.data.table(bigrams_data)
is.data.table(bigrams_data)
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id, term = value)]
kw_tr_glove <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "-")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr_glove$keywords$keyword, ngram = kw_tr_glove$keywords$ngram, sep = "-")
# investigating text rank results
stats <- subset(kw_tr_glove$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
bigrams_data <- as.data.table(bigrams_data)
is.data.table(bigrams_data)
# SAVE filter with bigrams seperated with - instead of _ to apply on glove embedding
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams_glove <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id,
term = value)]
# check number of unique words and documents.
length(unique(nounbydoc_bigrams_glove$doc_id))
length(unique(nounbydoc_bigrams_glove$term))
# check
# create a data frame with one column including the unique terms
filter_bigrams_glove <- unique(nounbydoc_bigrams_glove$term)
filter_bigrams_glove <- as.data.table(filter_bigrams_glove)
saveRDS(filter_bigrams, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/bigrams_filter_glove.RData")
saveRDS(filter_bigrams_glove, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/bigrams_filter_glove.RData")
# CREATE GLOVE EMBEDDING WITH BIGRAMS
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# CREATE GLOVE EMBEDDING WITH BIGRAMS
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
# load data frame with column with selection of words to include in analysis.
df_bigrams_glove <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/bigrams_filter_glove.RData")
View(vectors_glove)
View(df_bigrams_glove)
# create df in which only the words that we want to be included are kept
glove_embedding_bigrams <- subset(vectors_glove, word %in% df_bigrams_glove$filter_bigrams_glove)
# check number of unique words
length(unique((glove_embedding_bigrams$word)))
# non-GloVe: check which words are in the filter but are not in the feature matrix and are therefore lost (unwanted).
lost_words <- subset(df_bigrams_glove, !(filter_bigrams_glove %in% glove_embedding_bigrams$word))
View(lost_words)
length(unique((glove_embedding_bigrams$word))) + length(unique((lost_words$filter_bigrams_glove)))
# -> 27097. Which is equal to the line of code below.
length(unique((glove_embedding_bigrams$filter_bigrams_glove)))
# -> 27097. Which is equal to the line of code below.
length(unique((glove_embedding_bigrams$filter_bigrams_glove)))
View(glove_embedding_bigrams)
# -> 27097. Which is equal to the line of code below.
length(unique((df_bigrams_glove$filter_bigrams_glove)))
glove_embedding <-  readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_final.RData")
glv_embedding <- as.data.frame(glove_embedding)
View(glove_embedding)
View(glv_embedding)
11619-11342
# COMPARE THE GLOBE EMBEDDINGS IN WHICH BIGRAMS ARE AND ARE NOT INCLUDED
words_diff_glove <- subset(glove_embedding_bigrams, !(word %in% rownames(glv_embedding)))
View(words_diff_glove)
w2v_bigrams <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/w2v_bigrams_embedding.RData")
View(w2v_bigrams)
words_glovebg_vs_w2vbg <- subset(glove_embedding_bigrams, !(word %in% rownames(w2v_bigrams)))
View(words_glovebg_vs_w2vbg)
View(w2v_bigrams)
# transform so that it can be saved
glove_embedding_bigrams <- glove_embedding_bigrams %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
glove_embedding_bigrams <- as.matrix(glove_embedding_bigrams)
str(w2v_bigrams_embedding)
str(glove_embedding_bigrams)
saveRDS(glove_embedding_bigrams, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_bigrams.RData")
worcs::git_update()
# load libraries
library(stringr)
library(Matrix)
library(yaml)
