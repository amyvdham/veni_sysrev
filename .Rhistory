# GloVe - unigrams and bigrams
# check number of unique words
length(unique((rownames(glove_bigrams_embedding))))
# 11,943 unique words
# non-GloVe: check which words are in the bigrams glove filter but are not in the glove embedding data and are therefore lost (unwanted).
lost_glove_bigrams <- subset(df_bigrams_glove, !(filter_bigrams_glove %in% rownames(glove_bigrams_embedding)))
# 17,175 words lost
length(unique(rownames(glove_bigrams_embedding))) + length(unique((lost_glove_bigrams$filter_bigrams_glove)))
# -> 29,118. Which is equal to the line of code below.
length(unique((bigrams_filter$filter_bigrams)))
# Word2Vec - unigrams
# check number of unique words
length(unique((rownames(w2v_embedding))))
# 10,077 unique words
# non-w2v: check which words are in the final filter but are not in the w2v embedding data and are therefore lost (unwanted).
lost_w2v <- subset(final_filter, !(filter_lemma %in% rownames(w2v_embedding)))
# 3,670 words lost
length(unique(rownames(w2v_embedding))) + length(unique((lost_w2v$filter_lemma)))
# -> 13,747. Which is equal to the line of code below.
length(unique((final_filter$filter_lemma)))
# # Word2Vec - unigrams and bigrams
# check number of unique words
length(unique((rownames(w2v_bigrams_embedding))))
# 10,297 unique words
# non-w2v: check which words are in the bigrams filter but are not in the w2v embedding data and are therefore lost (unwanted).
lost_w2v_bigrams <- subset(bigrams_filter, !(filter_bigrams %in% rownames(w2v_bigrams_embedding)))
# 18,821 words lost
length(unique(rownames(w2v_bigrams_embedding))) + length(unique((lost_w2v_bigrams$filter_bigrams)))
# -> 29,118. Which is equal to the line of code below.
length(unique((bigrams_filter$filter_bigrams)))
# Comparing the two matrices with extracted GloVe word vectors - unigrams vs. unigrams and bigrams
diff_glove <- subset(glove_bigrams_embedding, !(rownames(glove_bigrams_embedding) %in% rownames(glove_embedding)))
# -> if we look at the rownames this matrix we see that difference between the two matrices are indeed all bigrams. This is to be expected.
# check the length of difference
length(unique(rownames(diff_glove)))
length(unique(rownames(glove_bigrams_embedding))) - length(unique(rownames(glove_embedding)))
# Comparing the two matrices with extracted Word2Vec word vectors - unigrams vs. unigrams and bigrams
diff_w2v <- subset(w2v_bigrams_embedding, !(rownames(w2v_bigrams_embedding) %in% rownames(w2v_embedding)))
# -> if we look at the rownames this matrix we see that difference between the two matrices are indeed all bigrams. This is to be expected.
# check words that are in the glove embedding that are also in the w2v embedding
# check the length of difference
length(unique(rownames(diff_w2v)))
length(unique(rownames(w2v_bigrams_embedding))) - length(unique(rownames(w2v_embedding)))
# Comparing the matrices with word vectors extracted from Glove vs. extracted from Word2vec - unigrams
# check words that are in the glove embedding that are also in the w2v embedding
words_same <- subset(glove_embedding, rownames(glove_embedding) %in% rownames(w2v_embedding))
length(unique(rownames(words_same)))
# check words that are in the glove embedding but not in the w2v embedding
words_diff <- subset(glove_embedding, !(rownames(glove_embedding) %in% rownames(w2v_embedding)))
length(unique(rownames(words_diff)))
length(unique(rownames(words_same))) + length(unique(rownames(words_diff))) == length(unique(rownames(glove_embedding)))
length(unique(rownames(w2v_embedding))) + length(unique(rownames(words_diff))) == length(unique(rownames(glove_embedding)))
# check words that are in the w2v embedding but not in the glove embedding
words_diff_w2v <- subset(w2v_embedding, !(rownames(w2v_embedding) %in% rownames(glove_embedding)))
length(unique(rownames(w2v_embedding))) + length(unique(rownames(words_diff))) -  length(unique(rownames(words_diff_w2v))) == length(unique(rownames(glove_embedding)))
# Comparing the matrices with word vectors extracted from Glove vs. extracted from Word2vec - bigrams
# Note that diff_glove and diff_w2v are used here because the diff represent the bigrams that are included extracted from both different datasets, glove and w2v.
# because the bigrams are split differently in the two dataset we first have to apply some data manipulations
diff_glove_bigram_comp <- as.data.frame(diff_glove)
diff_glove_bigram_comp$word <- rownames(diff_glove_bigram_comp)
diff_glove_bigram_comp <- diff_glove_bigram_comp["word"]
# create column in which the bigrams part of the glove embedding are split with _ and not -.
diff_glove_bigram_comp$w2v_style <- gsub('-', '_', diff_glove_bigram_comp$word)
# now check which bigrams are in the w2v embedding but not in the glove embedding
bigrams_in_w2v <- subset(diff_w2v, !(rownames(diff_w2v) %in% diff_glove_bigram_comp$w2v_style))
# check which bigrams are in the glove embedding but not in the w2v embedding
bigrams_in_glove <- subset(diff_glove_bigram_comp, !(w2v_style %in% rownames(diff_w2v)))
# check which and how many bigrams are in both embeddings
bigrams_both <- subset(diff_w2v, rownames(diff_w2v) %in% diff_glove_bigram_comp$w2v_style)
length(unique(rownames(bigrams_both)))
# -> interestingly the are only 45 bigrams that are in both embeddings.
# check if things add up for w2v
length(unique(rownames(diff_w2v))) == length(unique(rownames(bigrams_both))) + length(unique(rownames(bigrams_in_w2v)))
# check if things add up for glove
length(unique(rownames(diff_glove))) == length(unique(rownames(bigrams_both))) + length(unique(rownames(bigrams_in_glove)))
# clean and save the file with the word vectors extracted through ASReview
# check if file already exists, if not, code will be run to create the file
filenam13 <- "final_report_AvdH/asreview_embedding_sim_final.RData"
if(!file.exists(filenam13)){
# load csv that has been created in the Python file: gensim_to_dict.py into object
df_wordvecemb  <- read.csv("final_report_AvdH//dict_wordvec_sim.csv", header = FALSE)
# adjust the first column name to word
colnames(df_wordvecemb )[1] <- "word"
# remove the [ character from V2
df_wordvecemb$V2 <-gsub("\\[","",as.character(df_wordvecemb$V2))
# remove the ] character from V2
df_wordvecemb$V2 <-gsub("\\]","",as.character(df_wordvecemb$V2))
# remove the \n character from V2
df_wordvecemb$V2 <-gsub("\\\n","",as.character(df_wordvecemb$V2))
# check if removing the characters went correctly
df_wordvecemb[1,"V2"]
# -> yes there are now only numbers (the vectors)
# split column V2 into multiple columns
# separate on the space
df_wordvecemb <- cSplit(df_wordvecemb, "V2", " ")
# retain dimensions of data frame
dim(df_wordvecemb)
# -> we have 19,476 words that are defined by 40 dimensions. These dimensions define the context of the words.
# rename the column names of the data frame.
# First column is named word and the other columns dim1-40
colnames(df_wordvecemb) <- c("word", paste0("dim", 1:40))
# the first column contains the words so we want to set the row names accordingly
df_wordvecemb <- df_wordvecemb %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# Transform data frame to matrix
asr_embedding <- as.matrix(df_wordvecemb)
# -> Note that on this embedding the final filter has not yet been applied.
# save matrix as .RData
saveRDS(asr_embedding, "final_report_AvdH/asreview_embedding_sim_final.RData")
} else {
asr_embedding <- readRDS(filenam13)
}
# apply filter on the matrix with word vectors extracted through ASReview so that only the words that we want to be included (i.e., are part of the filter data frame) are kept.
asr_embedding_filtered <- subset(asr_embedding,  rownames(asr_embedding)%in% final_filter$filter_lemma)
# Transform back to matrix
asr_embedding_filtered  <- as.matrix(asr_embedding_filtered )
# library need for creating the function find_similar_words
library(text2vec)
# create function to find words that are close to each other based on cosine similarity. This function has been retrieved from the following tutorial page: https://blogs.rstudio.com/ai/posts/2017-12-22-word-embeddings-with-keras/
find_similar_words <- function(word, embedding_matrix, n = 5) {
similarities <- embedding_matrix[word, , drop = FALSE] %>%
sim2(embedding_matrix, y = ., method = "cosine")
similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}
# Run function on the word white and see the 50 closest words based on cosine similarity. Compare the different embeddings.
find_similar_words("white",glove_embedding,50)
find_similar_words("white",w2v_embedding,50)
find_similar_words("white",glove_bigrams_embedding,50)
find_similar_words("white",w2v_bigrams_embedding,50)
find_similar_words("white",asr_embedding_filtered,50)
# check if socio-emotional and socioemotional are similar words in the glove bigrams embedding
find_similar_words("socio-emotional",glove_bigrams_embedding,25)
find_similar_words("socioemotional",glove_bigrams_embedding,25)
# check words that are closely related to boy based on cosine similarity
find_similar_words("boy", asr_embedding_filtered,10)
find_similar_words("emotion", asr_embedding_filtered,15)
find_similar_words("boy", glove_embedding,10)
find_similar_words("boy", w2v_embedding,10)
# Function for creating a plot of the total within-groups sums of squares against the number of clusters in a K-means solution. This function has been retrieved from: https://www.r-bloggers.com/2013/08/k-means-clustering-from-r-in-action/.
wssplot <- function(data, nc=15, seed=88){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i, iter.max = 30)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
# apply function on matrix GloVe vectors and the maximum number of clusters to consider set to 150
wssplot(glove_embedding, nc=150)
# -> this the plot that is used in the report
# ONLY RUN THE FOLLOWING TWO LINES WHEN INTERESTED IN REPRODUCING THE RESULTS IN THE PAPER
# Silhouette plot that has been presented in the final report
# Note that here the nstart parameter has been set to the default value since it is not specified.
set.seed(88)
fviz_nbclust(glove_embedding, kmeans, iter.max = 30,  method = "silhouette", k.max = 150) +
labs(subtitle = "Silhouette method")
# fit the k-means clustering with 75 clusters, glove
set.seed(88)
kmeans_fit75 <- kmeans(glove_embedding, 75, iter.max = 30, nstart = 25)
# fit the k-means clustering with 100 clusters
set.seed(88)
kmeans_fit100 <- kmeans(glove_embedding, 100, iter.max = 30, nstart = 25)
# fit the k-means clustering with 125 clusters
set.seed(88)
kmeans_fit125 <- kmeans(glove_embedding, 125, iter.max = 30, nstart = 25)
# fit the k-means clustering with 150 clusters
set.seed(88)
kmeans_fit150 <- kmeans(glove_embedding, 150, iter.max = 30, nstart = 25)
# Internal validation
# load library that contains function for internal validation measures
library(clusterCrit)
# check internal validation measures for k-means cluster result with a value of 75 for k
int_idx_75 <- intCriteria(glove_embedding, kmeans_fit75$cluster, c("Calinski_Harabasz", "Davies_Bouldin", "S_Dbw"))
# check internal validation measures for k-means cluster result with a value of 100 for k
int_idx_100 <- intCriteria(glove_embedding, kmeans_fit100$cluster, c("Calinski_Harabasz", "Davies_Bouldin", "S_Dbw"))
# check internal validation measures for k-means cluster result with a value of 125 for k
int_idx_125 <- intCriteria(glove_embedding, kmeans_fit125$cluster, c("Calinski_Harabasz", "Davies_Bouldin", "S_Dbw"))
# check internal validation measures for k-means cluster result with a value of 150 for k
int_idx_150 <- intCriteria(glove_embedding, kmeans_fit150$cluster, c("Calinski_Harabasz", "Davies_Bouldin", "S_Dbw"))
# results
kmeans_fit125
# obtain the centroids
kmeans_fit125$centers
# look at the size of the clusters
kmeans_fit125$size
min(kmeans_fit125$size)
max(kmeans_fit125$size)
# -> do not see a cluster with extremely low number of observations. Minimum is 31 and max is 210
# The cost function in k means is the total sum of the squares
kmeans_fit125$totss
# silhouette width
sil <- silhouette(kmeans_fit125$cluster, dist(glove_embedding))
fviz_silhouette(sil)
# -> the average silhouette width is low and there are quite some negative values. When an observation has a low silhouette width value it means that it is poorly clustered and an assignment to some other cluster would probably improve the overall results. So when a cluster has a low average silhouette average it means that it contains observations that do belong well to that cluster.
# low silhouette average: cluster 7: -.05, cluster 87: -0.07, cluster 124: -04
# high silhouette average: cluster 47: 0.13, cluster 123, 0.11,
## Check which words are clustered together
# Create data frame in which the cluster assignment is merged back to rows/word.
words_with_cluster125 <- as.data.frame(cbind(row.names(glove_embedding), kmeans_fit125$cluster))
# add column names
names(words_with_cluster125) <- c("word", "kmeans125")
# Check cluster with highest silhouette average
cluster125_hghsil <- subset(words_with_cluster125, subset=kmeans125 == 47)
# Check cluster with the smallest size
cluster125_97 <- subset(words_with_cluster125, subset=kmeans125 == 97)
# find out to which cluster the word mother is assigned
words_with_cluster125[words_with_cluster125$word == "mother", ]
# create separate data frame containing all terms that belong to one specific cluster
cluster125_80 <- subset(words_with_cluster125, subset=kmeans125 == 80)
# find out in which cluster the word ethnic is assigned
words_with_cluster125[words_with_cluster125$word == "ethnic", ]
cluster125_75 <- subset(words_with_cluster125, subset=kmeans125 == 75)
# find out to which cluster the word academic is assigned
words_with_cluster125[words_with_cluster125$word == "academic", ]
cluster125_17 <- subset(words_with_cluster125, subset=kmeans125 == 17)
# find out to which cluster the word yoga is assigned
words_with_cluster125[words_with_cluster125$word == "yoga", ]
cluster125_28 <- subset(words_with_cluster125, subset=kmeans125 == 28)
# check a cluster with a low average silhouette width
cluster125_lowsil <- subset(words_with_cluster125, subset=kmeans125 == 27)
# check a cluster with a low average silhouette width
cluster125_lowsil2 <- subset(words_with_cluster125, subset=kmeans125 == 124)
# contains un and non words.
# cluster that will be used for clustering within cluster
cluster125_21 <- subset(words_with_cluster125, subset=kmeans125 == 21)
## Cluster analysis within cluster
# create a matrix that only contains the terms and their associated word vectors that belong to one cluster (cluster 21)
embedding_cluster21 <- subset(glove_embedding, rownames(glove_embedding) %in% cluster125_21$word)
# apply k-means on this subset with k = 3
set.seed(88)
kmeans_fit3_cluster21 <- kmeans(embedding_cluster21, 3, iter.max = 30, nstart = 25)
# -> Note that 3 as the value for k was decided on rather randomly. Ideally you would want to apply the same steps that were done before (elbow method, silhouette method and gap statistic etc.) on this subset to determine the optimal value of k.
# look at the resulting cluster assignments
# create data frame in which the merge cluster assignment back to rows/word.
words_with_cluster21_3 <- as.data.frame(cbind(row.names(embedding_cluster21), kmeans_fit3_cluster21$cluster))
# add column names
names(words_with_cluster21_3) <- c("word", "kmeans3")
# make data frames of the 3 resulting clusters to check which terms are assigned to which cluster
cluster21_1 <- subset(words_with_cluster21_3, subset=kmeans3 == 1)
cluster21_2 <- subset(words_with_cluster21_3, subset=kmeans3 == 2)
cluster21_3 <- subset(words_with_cluster21_3, subset=kmeans3 == 3)
# visualize the k-means (with k = 3) clusters
set.seed(88)
fviz_cluster(kmeans_fit3_cluster21, data = embedding_cluster21,
palette = c("#E7B800", "#2E9FDF", "#00AFBB"),
ellipse.type = "euclid", # Concentration ellipse
star.plot = TRUE, # Add segments from centroids to items
repel = TRUE, # Avoid label overplotting (slow)
ggtheme = theme_minimal()
)
# set max overlap to Inf to show all the labels in the plot.
#options(ggrepel.max.overlaps = Inf)
# -> makes the plot unreadable
# For some reason if the kmeans function is run straight after creating this plot I receive some remaining ggrepel warning when calling the kmeans function and this influences the results of the cluster analysis and causes them to not be reproducible. So even after having these ggrepel warnings displayed after creating the plot, when continuing with other commands the warning are printed again randomly and influence the set.seed and results. Online I found some others that have this same issue and they solved this by running the next command below. See the following page: https://github.com/slowkow/ggrepel/issues/187 for more information.
baseenv()$last.warning
assign("last.warning", NULL, envir = baseenv())
baseenv()$last.warning
# -> In my case this does however not seem to solve the issue, therefore the k-means clustering with 125 clusters on the data frame with word vectors extracted from word2vec and including bigram terms is fitted two times in a row. The second time you should get no warnings and be able to reproduce the results in the paper. I am aware that this is not the best way to solve this but due to time limits this was my best option for now.
# K-means clustering - Word2Vec including bigrams (k = 125)
# fit the k-means clustering with 125 clusters on the data frame with word vectors extracted from word2vec and including bigram terms.
set.seed(88)
kmeans_fit125_w2v <- kmeans(w2v_bigrams_embedding, 125, iter.max = 30, nstart = 25)
# NOTE: you will receive ggrepel warnings after running this line of code. The results in this cluster object should not be interpret instead the function should be called again with the lines of code below
# fit the k-means clustering with 125 clusters on the data frame with word vectors extracted from word2vec and including bigram terms.
set.seed(88)
kmeans_fit125_w2v <- kmeans(w2v_bigrams_embedding, 125, iter.max = 30, nstart = 25)
# NOTE: If for some reason you continue to receive the ggrepel warning when running the kmeans command, make sure you rerun this command until you don’t. Only then you will be able to reproduce the results from the final report.
# create data frame in which the cluster assignment is merged back to rows/word.
words_with_cluster125_w2v <- as.data.frame(cbind(row.names(w2v_bigrams_embedding), kmeans_fit125_w2v$cluster))
# add column names
names(words_with_cluster125_w2v) <- c("word", "kmeans125")
# find out to which cluster the word emotional_dysregulation is assigned
words_with_cluster125_w2v[words_with_cluster125_w2v$word == "emotional_dysregulation", ]
cluster125_w2v_59 <- subset(words_with_cluster125_w2v, subset=kmeans125 == 59)
# find out to which cluster the word emotion is assigned
words_with_cluster125_w2v[words_with_cluster125_w2v$word == "emotion", ]
cluster125_w2v_25 <- subset(words_with_cluster125_w2v, subset=kmeans125 == 25)
# find out to which cluster the word family is assigned
words_with_cluster125_w2v[words_with_cluster125_w2v$word == "family", ]
cluster125_w2v_113 <- subset(words_with_cluster125_w2v, subset=kmeans125 == 113)
# load the library that contains the Rtsne function
library(Rtsne)
# t-SNE visualization of cluster 97 (smallest cluster ) - GloVe (k = 125)
# create subset of terms and their associated word vectors that belong to cluster 97
embedding_cluster97 <- subset(glove_embedding, rownames(glove_embedding) %in% cluster125_97$word)
# apply t-sne
set.seed(88)
tsne_clst97 <- Rtsne(embedding_cluster97, dims = 2, perplexity = 10, verbose=TRUE, max_iter = 500, pca = TRUE)
# Function for creating plots of separate clusters
# name function and arguments
plot_cluster_tsne <-function(tsne_clst, embedding_cluster, clst_number){
# create data frame of matrix that can be used for making the plot
cluster_embedding_plot <- as.data.frame(embedding_cluster)
# create a column with the terms
cluster_embedding_plot$word <- rownames(cluster_embedding_plot)
# create a data frame that will be used for the plot
plot_df <- data.frame(tsne_clst$Y) %>%
mutate(
word = cluster_embedding_plot$word,
)
# create the plot
p <- ggplot(plot_df, aes(X1, X2)) +
geom_text(aes(X1, X2, label = word), size = 3) +
xlab("") + ylab("") +
ggtitle(paste0("2-dimensional t-SNE Mapping of Word Vectors Belonging to Cluster ", clst_number)) +
theme(legend.position = "none") +
theme_minimal()
return(p)
}
# call function to create plot
plot_cluster_tsne(tsne_clst97, embedding_cluster97, clst_number = 97)
# t-SNE visualization of cluster 80 (family related) - GloVe (k = 125)
# create subset of terms and their associated word vectors that belong to cluster 80
embedding_cluster80 <- subset(glove_embedding, rownames(glove_embedding) %in% cluster125_80$word)
# apply t-sne
set.seed(88)
tsne_clst80 <- Rtsne(embedding_cluster80, dims = 2, perplexity = 25, verbose=TRUE, max_iter = 500, pca = TRUE)
# call function to create plot
plot_cluster_tsne(tsne_clst80, embedding_cluster80, 80)
# t-SNE visualization of cluster 75 (ethnicity related) - GloVe (k = 125)
# create subset of terms and their associated word vectors that belong to cluster 75
embedding_cluster75 <- subset(glove_embedding, rownames(glove_embedding) %in% cluster125_75$word)
# apply t-sne
set.seed(88)
tsne_clst75 <- Rtsne(embedding_cluster75, dims = 2, perplexity = 20, verbose=TRUE, max_iter = 500, pca = TRUE)
# call function to create plot
plot_cluster_tsne(tsne_clst75, embedding_cluster75, 75)
# t-SNE visualization of cluster 17 (academic related) - GloVe (k = 125)
# create subset of terms and their associated word vectors that belong to cluster 17
embedding_cluster17 <- subset(glove_embedding, rownames(glove_embedding) %in% cluster125_17$word)
# apply t-sne
set.seed(88)
tsne_clst17 <- Rtsne(embedding_cluster17, dims = 2, perplexity = 25, verbose=TRUE, max_iter = 500, pca = TRUE)
# call function to create plot
plot_cluster_tsne(tsne_clst17, embedding_cluster17, 17)
# t-SNE visualization of cluster 124 (low silhouette average and terms with non, un, post, pre) - GloVe (k = 125)
# create subset of terms and their associated word vectors that belong to cluster 124
embedding_cluster124 <- subset(glove_embedding, rownames(glove_embedding) %in% cluster125_lowsil2$word)
# apply t-sne
set.seed(88)
tsne_clst124 <- Rtsne(embedding_cluster124, dims = 2, perplexity = 20, verbose=TRUE, max_iter = 500, pca = TRUE)
# call function to create plot
plot_cluster_tsne(tsne_clst124, embedding_cluster124, 124)
# t-SNE visualization of cluster 59 (emotional_dysregulation) - Word2Vec including bigrams (k = 125)
# create subset of terms and their associated word vectors that belong to cluster 59
embedding_cluster59_w2v <- subset(w2v_bigrams_embedding, rownames(w2v_bigrams_embedding) %in% cluster125_w2v_59$word)
# apply t-sne
set.seed(88)
tsne_clst_59_w2v <- Rtsne(embedding_cluster59_w2v, dims = 2, perplexity = 50, verbose=TRUE, max_iter = 500, pca = TRUE)
# call function to create plot and save in object
plot_clst59_w2v <- plot_cluster_tsne(tsne_clst_59_w2v, embedding_cluster59_w2v, clst_number = "59 - w2v")
# highlight the word emotional_dysregulation and adjust limits of x-axis
plot_clst59_w2v +
geom_text(aes(label = word, colour = word == "emotional_dysregulation"), size = 3, show.legend = FALSE) +
scale_colour_manual(values=c("#000000", "#ffcc00")) +
theme(legend.position = "none") +
xlim(-5, 5) +
theme_minimal()
# t-SNE visualization of cluster 113 (family related) - Word2Vec including bigrams (k = 125)
# create subset of terms and their associated word vectors that belong to cluster 113
embedding_cluster113_w2v <- subset(w2v_bigrams_embedding, rownames(w2v_bigrams_embedding) %in% cluster125_w2v_113$word)
# apply t-sne
set.seed(88)
tsne_clst_113_w2v <- Rtsne(embedding_cluster113_w2v, dims = 2, perplexity = 18, verbose=TRUE, max_iter = 500, pca = TRUE)
# call function to create plot
plot_cluster_tsne(tsne_clst_113_w2v, embedding_cluster113_w2v, clst_number = "113 - w2v")
# call function to create plot
plot_cluster_tsne(tsne_clst_113_w2v, embedding_cluster113_w2v, clst_number = "113 - w2v")
# load libraries that are need to perform dbscan clustering
library(fpc)
library(dbscan)
# Compute DBSCAN
set.seed(88)
db <- dbscan::dbscan(glove_embedding, 8, 5)
# Plot DBSCAN results
plot(db, glove_embedding, main = "DBSCAN", frame = FALSE)
# same plot using different function
library("factoextra")
fviz_cluster(db, glove_embedding, stand = FALSE, frame = FALSE, geom = "point")
# Print DBSCAN results
print(db)
# -> gives one big cluster and 411 noise points
# determine minPts
library(SciViews)
# I have found online (https://stackoverflow.com/questions/12893492/choosing-eps-and-minpts-for-dbscan-r) that you should set minPts to ln(n) so that is why this is calculated below.
ln(11562)
# determining optimal eps value
dbscan::kNNdistplot(glove_embedding, k =  9)
abline(h = 4.8, lty = 2)
abline(h = 8.3, lty = 2)
# -> based on this plot I would set eps to either 4.8 or 8.3 but not sure if I interpreted it correctly
# based on the running the codelines above try dbscan with minPts = 9 and eps = 8.3 and 4.8
db_8.3_9 <- dbscan::dbscan(glove_embedding, 8.3, 9)
print(db_8.3_9)
db_4.8_9 <- dbscan::dbscan(glove_embedding, 4.8, 9)
print(db_4.8_9)
# Create data frame in which the cluster assignment is merged back to rows/word.
kw_with_cluster9 <- as.data.frame(cbind(row.names(glove_embedding), db_4.8_9$cluster))
names(kw_with_cluster9) <- c("word", "dbscan9")
dbscan_cluster1 <- subset(kw_with_cluster9, subset=dbscan9 == 1)
dbscan_cluster2 <- subset(kw_with_cluster9, subset=dbscan9 == 2)
dbscan_cluster3 <- subset(kw_with_cluster9, subset=dbscan9 == 3)
dbscan_cluster4 <- subset(kw_with_cluster9, subset=dbscan9 == 4)
dbscan_cluster5 <- subset(kw_with_cluster9, subset=dbscan9 == 5)
dbscan_cluster6 <- subset(kw_with_cluster9, subset=dbscan9 == 6)
dbscan_cluster7 <- subset(kw_with_cluster9, subset=dbscan9 == 7)
dbscan_cluster8 <- subset(kw_with_cluster9, subset=dbscan9 == 8)
dbscan_cluster9 <- subset(kw_with_cluster9, subset=dbscan9 == 9)
# I have also read somewhere that you should set minPts to 2*dim (https://medium.com/@tarammullin/dbscan-parameter-estimation-ff8330e3a3bd). Which in our case means 2*300 = 600. Let try what happens if we do that
# determining optimal eps value
dbscan::kNNdistplot(glove_embedding, k =  600)
abline(h = 6.5, lty = 2)
abline(h = 9.2, lty = 2)
# based on the running the code lines above try dbscan with minPts = 600 and eps = 6.5
db_6.5_600 <- dbscan::dbscan(glove_embedding, 6.5, 600)
print(db_6.5_600)
# -> gives one cluster and other observations as noise points.
# TRYOUTS
# try dbscan met eps = 4 and minPts = 5
db4 <- dbscan::dbscan(glove_embedding, 4, 5)
# Print DBSCAN results
print(db4)
# gives 13 clusters but many noise points (11408)
# Create data frame in which the cluster assignment is merged back to rows/word.
kw_with_cluster13 <- as.data.frame(cbind(row.names(glove_embedding), db4$cluster))
names(kw_with_cluster13) <- c("word", "dbscan4")
dbscan_cluster13_1 <- subset(kw_with_cluster13, subset=dbscan4 == 1)
dbscan_cluster13_2 <- subset(kw_with_cluster13, subset=dbscan4 == 2)
dbscan_cluster13_3 <- subset(kw_with_cluster13, subset=dbscan4 == 3)
dbscan_cluster13_4 <- subset(kw_with_cluster13, subset=dbscan4 == 4)
dbscan_cluster13_5 <- subset(kw_with_cluster13, subset=dbscan4 == 5)
dbscan_cluster13_6 <- subset(kw_with_cluster13, subset=dbscan4 == 6)
dbscan_cluster13_7 <- subset(kw_with_cluster13, subset=dbscan4 == 7)
dbscan_cluster13_8 <- subset(kw_with_cluster13, subset=dbscan4 == 8)
dbscan_cluster13_9 <- subset(kw_with_cluster13, subset=dbscan4 == 9)
dbscan_cluster13_10 <- subset(kw_with_cluster13, subset=dbscan4 == 10)
dbscan_cluster13_11 <- subset(kw_with_cluster13, subset=dbscan4 == 11)
dbscan_cluster13_12 <- subset(kw_with_cluster13, subset=dbscan4 == 12)
dbscan_cluster13_13 <- subset(kw_with_cluster13, subset=dbscan4 == 13)
# try dbscan with different minPts value
db4_10 <- dbscan::dbscan(glove_embedding, 4, 10)
print(db4_10)
# random attempt
db_ra <- dbscan::dbscan(glove_embedding, .25, 10)
print(db_ra)
# -> results in all observations being classified as noise.
# try dbscan with smaller eps, eps = 0.10
db10_4 <- dbscan::dbscan(glove_embedding, 0.10, 4)
print(db10_4)
# -> this small eps value will lead to all the point being noise
# try dbscan with lager eps. : eps = 0.35
db1.5_4 <- dbscan::dbscan(glove_embedding, 1.5, 10000)
print(db1.5_4)
# -> results in all observations being classified as noise.
View(int_idx_75)
View(int_idx_100)
View(int_idx_100)
View(int_idx_150)
View(int_idx_125)
# check if socio-emotional and socioemotional are similar words in the glove bigrams embedding
find_similar_words("socio-emotional",glove_bigrams_embedding,25)
find_similar_words("socioemotional",glove_bigrams_embedding,25)
# check words that are closely related to boy based on cosine similarity
find_similar_words("boy", asr_embedding_filtered,10)
find_similar_words("emotion", asr_embedding_filtered,15)
find_similar_words("boy", glove_embedding,10)
find_similar_words("boy", w2v_embedding,10)
# Silhouette method
set.seed(88)
fviz_nbclust(glove_embedding, kmeans, iter.max = 50, nstart = 25, method = "silhouette", k.max = 150) +
labs(subtitle = "Silhouette method")
# Silhouette method
set.seed(88)
fviz_nbclust(glove_embedding, kmeans, iter.max = 30, nstart = 25, method = "silhouette", k.max = 150) +
labs(subtitle = "Silhouette method")
View(int_idx_150)
words_with_cluster125[words_with_cluster125$word == "august", ]
cluster125_110 <- subset(words_with_cluster125, subset=kmeans125 == 110)
View(cluster125_110)
embedding_cluster110 <- subset(glove_embedding, rownames(glove_embedding) %in% cluster125_110$word)
set.seed(88)
kmeans_fit3_cluster110 <- kmeans(embedding_cluster110, 3, iter.max = 30, nstart = 25)
fviz_cluster(kmeans_fit3_cluster110, data = embedding_cluster110,
palette = c("#E7B800", "#2E9FDF", "#00AFBB"),
ellipse.type = "euclid", # Concentration ellipse
star.plot = TRUE, # Add segments from centroids to items
repel = TRUE, # Avoid label overplotting (slow)
ggtheme = theme_minimal()
)
cluster110_1 <- subset(words_with_cluster110_3, subset=kmeans3 == 1)
words_with_cluster110_3 <- as.data.frame(cbind(row.names(embedding_cluster110), kmeans_fit3_cluster110$cluster))
cluster110_1 <- subset(words_with_cluster110_3, subset=kmeans3 == 1)
names(words_with_cluster110_3) <- c("word", "kmeans3")
cluster110_1 <- subset(words_with_cluster110_3, subset=kmeans3 == 1)
View(cluster110_1)
cluster110_2 <- subset(words_with_cluster110_3, subset=kmeans3 == 2)
View(cluster110_2)
cluster110_3 <- subset(words_with_cluster110_3, subset=kmeans3 == 3)
View(cluster110_3)
View(cluster110_1)
words_with_cluster125[words_with_cluster125$word == "family",]
warnings()
View(cluster125_80)
View(cluster125_w2v_25)
View(cluster125_w2v_59)
View(cluster125_w2v_113)
# find out to which cluster the word mother is assigned
## Cluster analysis within cluster
# create a matrix that only contains the terms and their associated word vectors that belong to one cluster (cluster 21)# apply k-means on this subset with k = 3
# look at the resulting cluster assignments
# create data frame in which the merge cluster assignment back to rows/word. # add column names
# make data frames of the 3 resulting clusters to check which terms are assigned to which cluster
