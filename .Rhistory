0.5+2+4+7+5
?worcs::git_update()
worcs::git_update(message = "organizing and cleaning files")
# load file with all the records that are screened in asreview into object
asreview_results <- read.csv("asreview_result_sysrevemotprob.csv")
View(asreview_results)
worcs::git_update(message = "organizing and cleaning files 2")
# load libraries
library(dplyr)
library(cluster)
library(ggplot2)
library(devtools)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
set.seed(88)
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
# load data frame with column with selection of words to include in analysis.
df_incltoken <- readRDS("filter_token.RData")
# create df in which only the words that we want to be included are kept
token_embedding <- subset(vectors_glove, word %in% df_incltoken$filter_token)
# check number of unique words
length(unique((token_embedding$word)))
# non-GloVe: check which words are in the token filter but are not in the feature matrix and are therefore lost (unwanted).
lost_tokens <- subset(df_incltoken, !(filter_token %in% token_embedding $word))
length(unique((token_embedding$word))) + length(unique((lost_tokens$filter_token)))
# -> 14860. Which is equal to the line of code below.
length(unique((df_incltoken$filter_token)))
# convert first column, word, to row index
library(tidyverse)
glove_embedding <- token_embedding %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
glove_embedding <- as.matrix(glove_embedding)
str(glove_embedding)
# save the glove embedding to which the filter is applied so this can be easily loaded into other scripts
saveRDS(glove_embedding, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/glove_embedding.RData")
library(text2vec)
library(text2vec)
# create function to find similar words based on cosine distance
find_similar_words <- function(word, embedding_matrix, n = 5) {
similarities <- embedding_matrix[word, , drop = FALSE] %>%
sim2(embedding_matrix, y = ., method = "cosine")
similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}
find_similar_words("health",glove_embedding,25)
# fit the k-means clustering with 100 clusters
k_means_fit <- kmeans(glove_embedding, 100, iter.max = 30, nstart = 25)
# obtain the centroids
k_means_fit$centers
# look at the size of the clusters
k_means_fit$size
min(k_means_fit$size)
max(k_means_fit$size)
# find the cluster to which each word belongs
k_means_fit$cluster
# The cost function in kmeans is the total sum of the squares
k_means_fit$totss
# results
k_means_fit
# Create data frame in which the cluster assignment is merged back to rows/word.
kw_with_cluster <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit$cluster))
# add column names
names(kw_with_cluster) <- c("word", "kmeans100")
# make a df for the first 5 cluster results, quickly "eyeball" results
cluster1 <- subset(kw_with_cluster, subset=kmeans100 == 1)
cluster2 <- subset(kw_with_cluster, subset=kmeans100 == 2)
cluster3 <- subset(kw_with_cluster, subset=kmeans100 == 3)
cluster4 <- subset(kw_with_cluster, subset=kmeans100 == 4)
cluster5 <- subset(kw_with_cluster, subset=kmeans100 == 5)
View(cluster1)
cluster2 <- subset(kw_with_cluster, subset=kmeans100 == 2)
View(cluster2)
View(cluster3)
View(glove_embedding)
View(df_incltoken)
View(cluster4)
View(cluster5)
# Mother
# find out in which cluster the word mother is assigned
kw_with_cluster[kw_with_cluster$word == "mother", ]
# make a df of cluster 82
cluster82 <- subset(kw_with_cluster, subset = kmeans100 == 82)
View(cluster82)
# Health
# find out in which cluster the word health is assigned
kw_with_cluster[kw_with_cluster$word == "health", ]
# make a df of cluster 26
cluster26 <- subset(kw_with_cluster, subset=kmeans100 == 26)
View(cluster26)
# Environment
# find out in which cluster the word environment is assigned
kw_with_cluster[kw_with_cluster$word == "environment", ]
# make a df of cluster 28
cluster28 <- subset(kw_with_cluster, subset=kmeans100 == 28)
View(cluster28)
# Depression
# find out to which cluster the word depression is assigned so that I can check if all the forms of depression are in there.
kw_with_cluster[kw_with_cluster$word == "depression", ]
# make a df of cluster 31
cluster31 <- subset(kw_with_cluster, subset=kmeans100 == 31)
View(cluster31)
# ->  If I look at the token_embedding dataframe and type depre in the filter within the word column I get the following words: nondepressed, antidepressant depressed, depression, depressive. It is surprising that some of these words fall in a different cluster. We wold expect at least depressed, depression and depressive to fall into the same cluster.
kw_with_cluster[kw_with_cluster$word == "depressed", ]
kw_with_cluster[kw_with_cluster$word == "depressive", ]
kw_with_cluster[kw_with_cluster$word == "nondepressed", ]
kw_with_cluster[kw_with_cluster$word == "antidepressant", ]
# make a df of cluster 43
cluster43 <- subset(kw_with_cluster, subset=kmeans100 == 43)
# make a df of cluster 36
cluster36 <- subset(kw_with_cluster, subset=kmeans100 == 36)
View(cluster43)
View(cluster36)
# cortisol
# find out in which cluster the word cortisol is assigned
kw_with_cluster[kw_with_cluster$word == "cortisol", ]
# make a df of cluster 100
cluster100 <- subset(kw_with_cluster, subset=kmeans100 == 100)
# empathy
# find out in which cluster the word empathy is assigned
kw_with_cluster[kw_with_cluster$word == "empathy", ]
# make a df of cluster 30
cluster30 <- subset(kw_with_cluster, subset = kmeans100 == 30)
View(cluster30)
# look at the size of the clusters
k_means_fit$size
# check the smallest cluster
cluster20 <- subset(kw_with_cluster, subset = kmeans100 == 20)
View(cluster20)
max(k_means_fit$size)
# check the largest cluster
cluster98 <- subset(kw_with_cluster, subset = kmeans100 == 98)
View(cluster98)
# check another large cluster
cluster17 <- subset(kw_with_cluster, subset = kmeans100 == 17)
View(cluster17)
View(cluster98)
# check another large cluster
cluster37 <- subset(kw_with_cluster, subset = kmeans100 == 37)
View(cluster37)
View(vectors_glove)
# check cluster number 28
cluster28 <- subset(kw_with_cluster, subset = kmeans100 == 28)
View(cluster28)
View(cluster28)
# check cluster number 69
cluster69 <- subset(kw_with_cluster, subset = kmeans100 == 69)
View(cluster69)
View(cluster43)
View(vectors_glove)
View(cluster98)
View(cluster37)
# create new subset data frame
embedding_cluster43 <- subset(glove_embedding, rownames(glove_embedding) %in% cluster43$word)
# fit k-means
k_means_fit_cluster43 <- kmeans(embedding_cluster43, 3, iter.max = 30)
# look at the results
# obtain the centroids
k_means_fit_cluster43$centers
# look at the size of the clusters
k_means_fit_cluster43$size
# find the cluster to which each word belongs
k_means_fit_cluster43$cluster
# Create data frame in which the merge cluster assignment back to rows/word.
token_within_clusters <- as.data.frame(cbind(row.names(embedding_cluster43), k_means_fit_cluster43$cluster))
# add column names
names(token_within_clusters) <- c("word", "kmeans3")
# make a df for the 3 cluster results
cluster43_1 <- subset(token_within_clusters, subset=kmeans3 == 1)
cluster43_2 <- subset(token_within_clusters, subset=kmeans3 == 2)
cluster43_3 <- subset(token_within_clusters, subset=kmeans3 == 3)
View(cluster43_1)
View(cluster43_2)
View(cluster43_3)
##  Determine the value of K
library(factoextra)
# Elbow method
fviz_nbclust(embedding_cluster43, kmeans, method = "wss") + labs(subtitle = "Elbow method") # add subtitle
# Elbow method
fviz_nbclust(embedding_cluster43, kmeans, method = "wss") + geom_vline(xintercept = 7, linetype = 2) + # add line for better visualisation.
labs(subtitle = "Elbow method") # add subtitle
# Silhouette method
fviz_nbclust(embedding_cluster43, kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
# Gap statistic
fviz_nbclust(embedding_cluster43, kmeans,
nstart = 25,
method = "gap_stat",
nboot = 500 # reduce it for lower computation time (but less precise results)
) +
labs(subtitle = "Gap statistic method")
# try to determine number of cluster with NbClust package
library(NbClust)
nbclust_out <- NbClust(
data = embedding_cluster43,
distance = "euclidean",
min.nc = 2, # minimum number of clusters
max.nc = 15, # maximum number of clusters
method = "kmeans", # one of: "ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid", "kmeans"
index = "silhouette"
)
nbclust_out
nbclust_out <- NbClust(
data = embedding_cluster43,
distance = "euclidean",
min.nc = 2, # minimum number of clusters
max.nc = 15, # maximum number of clusters
method = "kmeans", # one of: "ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid", "kmeans"
index = "all"
)
# Silhouette method
fviz_nbclust(embedding_cluster43, kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
nbclust_out <- NbClust(
data = embedding_cluster43,
distance = "euclidean",
min.nc = 2, # minimum number of clusters
max.nc = 15, # maximum number of clusters
method = "kmeans", # one of: "ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid", "kmeans"
index = "silhouette"
)
nbclust_out
# create plot of results
factoextra::fviz_nbclust(nbclust_out) + theme_minimal() + ggtitle("NbClust's optimal number of clusters")
nbclust_out
# create plot of results
factoextra::fviz_nbclust(nbclust_out) +
theme_minimal() +
ggtitle("NbClust's optimal number of clusters")
# create plot of results
factoextra::fviz_nbclust(nbclust_out$All.index) +
theme_minimal() +
ggtitle("NbClust's optimal number of clusters")
# Another package for determine number of clusters
library(ClusterR)
Optimal_Clusters_KMeans(embedding_cluster43,
max_clusters = 15,
criterion = "distortion_fK",
fK_threshold = 0.85,
num_init = 3,
max_iters = 50,
initializer = "kmeans++",
tol = 1e-04,
plot_clusters = TRUE,
verbose = T,
tol_optimal_init = 0.3,
seed = 1)
Optimal_Clusters_KMeans(embedding_cluster43,
max_clusters = 15,
criterion = "distortion_fK",
fK_threshold = 0.85,
num_init = 3,
max_iters = 50,
initializer = "kmeans++",
tol = 1e-04,
plot_clusters = TRUE,
verbose = T,
tol_optimal_init = 0.3,
seed = 1,
criterion = "shiloutte")
Optimal_Clusters_KMeans(embedding_cluster43, max_clusters = 15, plot_clusters = T, criterion = 'silhouette')
# RUN ANALYSIS WITH 9 CLUSTERS
# fit kmeans with 9 clusters
km_9 <- kmeans(embedding_cluster43, 9, iter.max = 30, nstart = 25)
sil <- silhouette(km_9$cluster, dist(embedding_cluster43))
fviz_silhouette(sil)
# check cluster with highest silhouette index
resulting_clusters <- as.data.frame(cbind(row.names(embedding_cluster43), km_res$cluster))
# check cluster with highest silhouette index
resulting_clusters <- as.data.frame(cbind(row.names(embedding_cluster43), km_9$cluster))
# add column names
names(resulting_clusters) <- c("word", "kmeans10")
# cluster with highest silhouette value
cluster4_highestsil <- subset(resulting_clusters, subset = kmeans10 == 5)
View(cluster4_highestsil)
# cluster with highest silhouette value
cluster4_highestsil <- subset(resulting_clusters, subset = kmeans10 == 4)
View(cluster4_highestsil)
# visualize the k-means (with k = 3) clusters
fviz_cluster(k_means_fit_cluster43, data = embedding_cluster43,
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
ellipse.type = "euclid", # Concentration ellipse
star.plot = TRUE, # Add segments from centroids to items
repel = TRUE, # Avoid label overplotting (slow)
ggtheme = theme_minimal()
)
# fit k-means
k_means_fit_cluster43 <- kmeans(embedding_cluster43, 3, iter.max = 30, nstart = 25)
# look at the results
# obtain the centroids
k_means_fit_cluster43$centers
# look at the size of the clusters
k_means_fit_cluster43$size
# find the cluster to which each word belongs
k_means_fit_cluster43$cluster
# Create data frame in which the merge cluster assignment back to rows/word.
token_within_clusters <- as.data.frame(cbind(row.names(embedding_cluster43), k_means_fit_cluster43$cluster))
# add column names
names(token_within_clusters) <- c("word", "kmeans3")
# make a df for the 3 cluster results
cluster43_1 <- subset(token_within_clusters, subset=kmeans3 == 1)
cluster43_2 <- subset(token_within_clusters, subset=kmeans3 == 2)
cluster43_3 <- subset(token_within_clusters, subset=kmeans3 == 3)
View(cluster43_1)
View(cluster43_2)
# visualize the k-means (with k = 3) clusters
fviz_cluster(k_means_fit_cluster43, data = embedding_cluster43,
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
ellipse.type = "euclid", # Concentration ellipse
star.plot = TRUE, # Add segments from centroids to items
repel = TRUE, # Avoid label overplotting (slow)
ggtheme = theme_minimal()
)
# visualize the k-means (with k = 9) clusters
fviz_cluster(km_g, data = embedding_cluster43,
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
ellipse.type = "euclid", # Concentration ellipse
star.plot = TRUE, # Add segments from centroids to items
repel = TRUE, # Avoid label overplotting (slow)
ggtheme = theme_minimal()
)
# visualize the k-means (with k = 9) clusters
fviz_cluster(km_9, data = embedding_cluster43,
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
ellipse.type = "euclid", # Concentration ellipse
star.plot = TRUE, # Add segments from centroids to items
repel = TRUE, # Avoid label overplotting (slow)
ggtheme = theme_minimal()
)
last_error()
rlang::last_error()
# visualize the k-means (with k = 9) clusters
fviz_cluster(km_9, data = embedding_cluster43,
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#3CB043", "#B2D3C2", "#354A21", "#EA3C53", "#8D021F", "#8F00FF"),
ellipse.type = "euclid", # Concentration ellipse
star.plot = TRUE, # Add segments from centroids to items
repel = TRUE, # Avoid label overplotting (slow)
ggtheme = theme_minimal()
)
# fit the k-means clustering with 133 clusters
k_means_fit119 <- kmeans(glove_embedding, 119, iter.max = 30, nstart = 25)
warning()
warningws()
warnings()
# obtain the centroids
k_means_fit119$centers
# look at the size of the clusters
k_means_fit119$size
min(k_means_fit119$size)
max(k_means_fit119$size)
# The cost function in kmeans is the total sum of the squares
k_means_fit119$totss
# check results
k_means_fit119
# Create data frame in which the merge cluster assignment back to rows/word.
words_with_cluster119 <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit119$cluster))
# add column names
names(words_with_cluster119) <- c("word", "kmeans119")
# check the largest cluster with 274 observations
cluster119_42 <- subset(words_with_cluster119,
subset=kmeans119 == 42)
# look at the size of the clusters
k_means_fit119$size
# check the largest cluster with 270 observations
cluster119_102 <- subset(words_with_cluster119,
subset=kmeans119 == 102)
View(cluster119_102)
# check the largest cluster with 270 observations
cluster119_92 <- subset(words_with_cluster119,
subset=kmeans119 == 92)
View(cluster119_92)
# check the smallest cluster with 35 observations
cluster119_78 <- subset(words_with_cluster119,
subset=kmeans119 == 78)
View(cluster119_78)
# Health
# find out in which cluster the word health is assigned
words_with_cluster119[words_with_cluster119$word == "health", ]
# make a df of cluster 117
cluster119_117 <- subset(words_with_cluster119,
subset=kmeans119 == 117)
check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/clustering119.RData")
View(check)
# save this data frame which contains a column with words and a column with to which cluster they belong
saveRDS(kw_with_cluster, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/clustering119.RData")
View(check)
# Depression
# find out in which cluster the word depression is assigned
words_with_cluster119[words_with_cluster119$word == "depression", ]
# make a df of cluster 83
cluster119_83 <- subset(words_with_cluster119,
subset=kmeans119 == 83)
View(cluster119_83)
words_with_cluster119[words_with_cluster119$word == "depressed", ]
words_with_cluster119[words_with_cluster119$word == "depressive", ]
words_with_cluster119[words_with_cluster119$word == "nondepressed", ]
words_with_cluster119[words_with_cluster119$word == "antidepressant", ]
# create new subset data frame
embedding_cluster37 <- subset(glove_embedding, rownames(glove_embedding) %in% cluster37$word)
# Silhouette method for determining k
fviz_nbclust(embedding_cluster37, kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
# fit k-means
k_means_fit5_cluster37 <- kmeans(embedding_cluster43, 5, iter.max = 30, nstart = 25)
# fit k-means
k_means_fit5_cluster37 <- kmeans(embedding_cluster37, 5, iter.max = 30, nstart = 25)
# look at the size of the clusters
k_means_fit5_cluster37$size
# Create data frame in which the merge cluster assignment back to rows/word.
token_within_clusters37 <- as.data.frame(cbind(row.names(embedding_cluster37), k_means_fit5_cluster37$cluster))
# add column names
names(token_within_clusters) <- c("word", "kmeans5")
# make a df for the 3 cluster results
cluster37_1 <- subset(token_within_clusters37, subset=kmeans5 == 1)
cluster37_2 <- subset(token_within_clusters37, subset=kmeans5 == 2)
# add column names
names(token_within_clusters37) <- c("word", "kmeans5")
# make a df for the 3 cluster results
cluster37_1 <- subset(token_within_clusters37, subset=kmeans5 == 1)
cluster37_2 <- subset(token_within_clusters37, subset=kmeans5 == 2)
cluster37_3 <- subset(token_within_clusters37, subset=kmeans5 == 3)
View(cluster37_1)
View(cluster37_2)
View(cluster37_3)
# visualize the k-means (with k = 3) clusters
fviz_cluster(k_means_fit5_cluster37, data = embedding_cluster37,
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07", "#8F00FF"),
ellipse.type = "euclid", # Concentration ellipse
star.plot = TRUE, # Add segments from centroids to items
repel = TRUE, # Avoid label overplotting (slow)
ggtheme = theme_minimal()
)
library(clValid)
intern <- clValid(glove_embedding, nClust = 2:125,
clMethods = c("hierarchical","kmeans"), validation = "internal")
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs")
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_before_excl <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df.RData")
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_before_excl <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df.RData")
# Exclude words
# make sure to only include nouns and adjectives.
df_kw <- df_before_excl[upos %in% c("NOUN", "ADJ"), ]
# make sure to only include strings that start with a letter. (. -> and end with any character?)
df_kw <- df_kw[grepl("^[a-zA-Z].", df_kw$lemma), ]
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df_kw$lemma)))
df_kw[21, "lemma"]
df_kw <- df_kw[-exclude_these, ]
df_kw[df_kw$lemma == "child", ]
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_before_excl <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df.RData")
# Exclude words
# make sure to only include nouns and adjectives.
df_kw <- df_before_excl[upos %in% c("NOUN", "ADJ"), ]
# make sure to only include strings that start with a letter. (. -> and end with any character?)
df_kw <- df_kw[grepl("^[a-zA-Z].", df_kw$lemma), ]
# exclude methodological terms and similar non-substantive words
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df_kw$lemma)))
# exclude_these contains the integers of the words (fit the regular expressions) that are in the exclude_terms file. One of the words that is in there is row 21.
df_kw[21, "lemma"]
df_kw[390, "lemma"]
df_kw[724, "lemma"]
# remove all those rows that include one of the terms that should be excluded
df_after_exclusion <- df_kw[-exclude_these, ]
df_kw[df_kw$lemma == "child", ]
df_after_exclusion[df_after_exclusion$lemma == "child", ]
df_after_exclusion[df_after_exclusion$token == "child", ]
# In the cluster analysis I found that the word correlation was still in the embedding so will check on this word
df_kw[df_kw$lemma == "correlation", ]
df_after_exclusion[df_after_exclusion$lemma == "correlation", ]
# In the cluster analysis I found that the word analysis was still in the embedding so will check on this word
df_kw[df_kw$lemma == "analysis", ]
df_after_exclusion[df_after_exclusion$lemma == "analysis", ]
# -> It seems that this have correctly been excluded. However I might that because of the splitting at - and / these terms have found there way back into the data frame.
# load the data frame that includes the token filter column.
check_filter_token <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/study2_df_token.RData")
View(check_filter_token)
# check on the word analysis
check_filter_token[check_filter_token$filter_token == "analysis", c("lemma", "filter_token")]
# check on the word correlation
check_filter_token[check_filter_token$filter_token == "correlation", c("lemma", "filter_token")]
# In the cluster analysis I found that the word confounder was still in the embedding so will check on this word
df_kw[df_kw$lemma == "confounder", ]
df_after_exclusion[df_after_exclusion$lemma == "confounder", ]
# In the cluster analysis I found that the word confounder was still in the embedding so will check on this word
df_kw[df_kw$lemma == "confounder", ]
df_after_exclusion[df_after_exclusion$lemma == "confounder", ]
# In the cluster analysis I found that the word control was still in the embedding so will check on this word
df_kw[df_kw$lemma == "control", ]
df_after_exclusion[df_after_exclusion$lemma == "control", ]
# check on the word correlation
check_filter_token[check_filter_token$filter_token == "finding", c("lemma", "filter_token")]
# check on the word finding
df_kw[df_kw$lemma == "finding", ]
df_after_exclusion[df_after_exclusion$lemma == "finding", ]
check_filter_token[check_filter_token$filter_token == "finding", c("lemma", "filter_token")]
check_filter_token[check_filter_token$filter_token == "finding", c("token", "lemma", "filter_token")]
# check on the word finding
df_kw[df_kw$lemma == "control", ]
df_after_exclusion[df_after_exclusion$lemma == "control", ]
check_filter_token[check_filter_token$filter_token == "control", c("token", "lemma", "filter_token")]
worcs::git_update()
