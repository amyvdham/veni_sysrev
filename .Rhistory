cluster133_6 <- subset(words_with_cluster133,
subset=kmeans133 == 6)
cluster133_71 <- subset(words_with_cluster133,
subset=kmeans133 == 71)
View(cluster133_6)
View(cluster133_71)
find_similar_words("health",glove_embedding,25)
library(text2vec)
find_similar_words <- function(word, embedding_matrix, n = 5) {
similarities <- embedding_matrix[word, , drop = FALSE] %>%
sim2(embedding_matrix, y = ., method = "cosine")
similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}
find_similar_words("health",glove_embedding,25)
View(cluster27)
# Health
# find out in which cluster the word health is assigned
words_with_cluster133[words_with_cluster133$word == "health", ]
# make a df of cluster 27
cluster133_124 <- subset(words_with_cluster133,
subset=kmeans133 == 124)
View(cluster133_124)
# Depression
# find out in which cluster the word depression is assigned
words_with_cluster133[words_with_cluster133$word == "depression", ]
# make a df of cluster 2
cluster133_2 <- subset(words_with_cluster133,
subset=kmeans133 == 2)
View(cluster133_2)
find_similar_words("health", depression,25)
find_similar_words("depression",glove_embedding,25)
# load libraries
library(dplyr)
library(cluster)
library(ggplot2)
library(devtools)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
set.seed(88)
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
# load data frame with column with selection of words to include in analysis. For now I will only
df_incllemma <- readRDS("include_lemma.RData")
# create df in which only the words that we want to be included are kept
lemma_embedding <- subset(vectors_glove, word %in% df_incllemma$include_lemma)
# check number of unique words
length(unique((lemma_embedding$word)))
# non-GloVe: check which words are in the included filter but are not in the feature matrix and are therefore lost (unwanted).
lost_lemma <- subset(df_incllemma, !(include_lemma %in% lemma_embedding $word))
length(unique((lemma_embedding$word))) + length(unique((lost_lemma$include_lemma)))
# ->  14195: dit is gelijk aan:
length(unique((df_incllemma$include_lemma)))
# convert first column, word, to row index
library(tidyverse)
glove_embedding <- lemma_embedding %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
glove_embedding <- as.matrix(glove_embedding)
str(glove_embedding)
library(text2vec)
find_similar_words <- function(word, embedding_matrix, n = 5) {
similarities <- embedding_matrix[word, , drop = FALSE] %>%
sim2(embedding_matrix, y = ., method = "cosine")
similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}
find_similar_words("health",glove_embedding,25)
# fit the k-means clustering with 100 clusters
k_means_fit <- kmeans(glove_embedding, 100, iter.max = 30, nstart = 25)
# obtain the centroids
k_means_fit$centers
# look at the size of the clusters
k_means_fit$size
min(k_means_fit$size)
max(k_means_fit$size)
# find the cluster to which each word belongs
k_means_fit$cluster
# The cost function in kmeans is the total sum of the squares
k_means_fit$totss
# results
k_means_fit
# Create data frame in which the merge cluster assignment back to rows/word.
kw_with_cluster <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit$cluster))
# add column names
names(kw_with_cluster) <- c("word", "kmeans100")
# make a df for the first 5 cluster results, quickly "eyeball" results
cluster1 <- subset(kw_with_cluster, subset=kmeans100 == 1)
cluster2 <- subset(kw_with_cluster, subset=kmeans100 == 2)
cluster3 <- subset(kw_with_cluster, subset=kmeans100 == 3)
cluster4 <- subset(kw_with_cluster, subset=kmeans100 == 4)
cluster5 <- subset(kw_with_cluster, subset=kmeans100 == 5)
# Mother
# find out in which cluster the word mother is assigned
kw_with_cluster[kw_with_cluster$word == "mother", ]
# make a df of cluster 25
cluster25 <- subset(kw_with_cluster, subset = kmeans100 == 25)
# I did not setseeded the cluster analysis yet so want to compare the difference between the two clusters that contain the word mother. -> Can not rerun this code now that I have set.seeded the analysis but it is important to be aware that every time you run the analysis and do not set.seed, the clusters will be different. The 3 times I have now run the analysis the cluster containing mother had either 71, 70 or 86 observations.
diff_setseed <- subset(cluster6, !(word %in% cluster26$word))
# Health
# find out in which cluster the word health is assigned
kw_with_cluster[kw_with_cluster$word == "health", ]
# make a df of cluster 74
cluster74 <- subset(kw_with_cluster, subset=kmeans100 == 74)
View(cluster74)
# Environment
# find out in which cluster the word environment is assigned
kw_with_cluster[kw_with_cluster$word == "environment", ]
# make a df of cluster 66
cluster66 <- subset(kw_with_cluster, subset=kmeans100 == 66)
# Depression
# find out in which cluster the word depression is assigned so that I can check if all the forms of depression are in there.
kw_with_cluster[kw_with_cluster$word == "depression", ]
# make a df of cluster 8
cluster8 <- subset(kw_with_cluster, subset=kmeans100 == 8)
View(cluster8)
# -> only the word depression is in this cluster. Might be that these are the only depression words in the embedding need to check this. -> If I look at the lemma_embedding dataframe and type depre in the filter within the word column I get the following words: nondepressed, antidepressant depressed, depression, depressive. You would expect some of these words to fall within the same cluster.
kw_with_cluster[kw_with_cluster$word == "depressed", ]
kw_with_cluster[kw_with_cluster$word == "depressive", ]
kw_with_cluster[kw_with_cluster$word == "nondepressed", ]
kw_with_cluster[kw_with_cluster$word == "antidepressant", ]
# Kelly
# find out in which cluster the word Kelly is assigned so that I can check if all names are put together for example.
kw_with_cluster[kw_with_cluster$word == "kelly", ]
# make a df of cluster 87
cluster87 <- subset(kw_with_cluster, subset=kmeans100 == 87)
View(cluster87)
# cortisol
# find out in which cluster the word cortisol is assigned so that I can check if all names are put together for example.
kw_with_cluster[kw_with_cluster$word == "cortisol", ]
# make a df of cluster 56
cluster56 <- subset(kw_with_cluster, subset=kmeans100 == 56)
# empathy
# find out in which cluster the word empathy is assigned so that I can check if all names are put together for example.
kw_with_cluster[kw_with_cluster$word == "empathy", ]
# make a df of cluster 71
cluster71 <- subset(kw_with_cluster, subset = kmeans100 == 71)
find_similar_words("empathy",glove_embedding,25)
View(cluster71)
# fit the k-means clustering with 133 clusters
k_means_fit133 <- kmeans(glove_embedding, 133, iter.max = 30, nstart = 25)
# obtain the centroids
k_means_fit133$centers
# look at the size of the clusters
k_means_fit133$size
min(k_means_fit133$size)
max(k_means_fit133$size)
# The cost function in kmeans is the total sum of the squares
k_means_fit133$totss
# check results
k_means_fit133
# Create data frame in which the merge cluster assignment back to rows/word.
words_with_cluster133 <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit133$cluster))
# add column names
names(words_with_cluster133) <- c("word", "kmeans133")
# check the cluster with only 3 and 6 observations
cluster133_32 <- subset(words_with_cluster133,
subset=kmeans133 == 32)
View(cluster133_32)
# Health
# find out in which cluster the word health is assigned
words_with_cluster133[words_with_cluster133$word == "health", ]
# make a df of cluster 50
cluster133_50 <- subset(words_with_cluster133,
subset=kmeans133 == 50)
View(cluster133_50)
# Depression
# find out in which cluster the word depression is assigned
words_with_cluster133[words_with_cluster133$word == "depression", ]
# make a df of cluster 2
cluster133_108 <- subset(words_with_cluster133,
subset=kmeans133 == 108)
View(cluster133_108)
max(k_means_fit133$size)
# look at the size of the clusters
k_means_fit133$size
# check the largest cluster with 216 observations
cluster133_76 <- subset(words_with_cluster133,
subset=kmeans133 == 76)
View(cluster133_76)
worcs::git_update()
View(kw_with_cluster)
View(lost_lemma)
View(lost_lemma)
View(df_incllemma)
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
clean_df <- df_check %>%
anti_join(stop_words, by= c("lemma" = "word"))
# check number of unique words after removing stop words
length(unique(clean_df$lemma))
# double check if the word "and" is removed from the file
clean_df[clean_df$lemma == "and", c("token", "lemma", "doc_id")]
# double check if the word "and" is removed from the file
df_check[df_check$lemma == "and", c("token", "lemma", "doc_id")]
# double check if the word "and" is removed from the file
df_check[df_check$token == "and", c("token", "lemma", "doc_id")]
# double check if the word "and" is removed from the file
df_check[df_check$token == "the", c("token", "lemma", "doc_id")]
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
View(df_check)
df_check <- df_check
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
clean_df <- df_check %>%
anti_join(stop_words, by= c("lemma" = "word"))
# 15274
df_check <- df_check
View(clean_df)
View(df_check)
worcs::git_update()
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# add lemma_filter column to dataframe so we keep the original column and filter can be applied on the new column.
df_check$filter_lemma <- df_check$lemma
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
lemma_clean <- df_check %>%
anti_join(stop_words, by= c("filter_lemma" = "word"))
# check number of unique words after removing stop words
length(unique(lemma_clean$filter_lemma))
# SPLIT AT -
library(splitstackshape)
# Split words in lemma column on the -
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "-", direction = "long")
# check results by looking at a term that contains - in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "non-risk", c("lemma", "filter_lemma", "doc_id")]
# SPLIT AT /
# Split words in lemma column on the /
lemma_clean <- cSplit(lemma_clean, "filter_lemma", "/", direction = "long")
# check results by looking at term that contains / in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "police/judicial", c("lemma", "filter_lemma", "doc_id")]
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# check results by looking at term that contains a non-ASCII character in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# remove all the digit characters from words in lemma column
lemma_clean$filter_lemma <- gsub("[0-9]+" ,"", lemma_clean$filter_lemma)
# check results by looking at term that contains a number in the original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# check number of unique words after applying more filters
length(unique(lemma_clean$filter_lemma))
# SAVE COMPLETE DATA FRAME INCLUDING FILTER_LEMMA COLUMN
saveRDS(lemma_clean, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/study2_df_lemma.RData")
# check number of unique words before applying more filters
length(unique(df_check$token))
# add token_filter column to dataframe so we keep the original column and filter can be applied on the new column.
df_check <- select(df_check, -filter_lemma)
View(df_check)
df_check$filter_token <- df_check$token
# REMOVING STOPWORDS.
token_clean <- df_check %>%
anti_join(stop_words, by= c("filter_token" = "word"))
# check number of unique words after removing stop words
length(unique(token_clean$filter_token))
# SPLIT AT -
# Split words in token column on the -
token_clean <- cSplit(token_clean, "filter_token", "-", direction = "long")
# check results by looking at term that contains - in the original token column on which the filter has not been applied.
token_clean[token_clean$token == "non-risk", c("token", "filter_token", "doc_id")]
# SPLIT AT /
# Split words in token column on the -
token_clean <- cSplit(token_clean, "filter_token", "/", direction = "long")
# check results by looking at term that contains - in the original token column on which the filter has not been applied.
token_clean[token_clean$token == "police/judicial", c("token", "filter_token", "doc_id")]
# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in token column
token_clean$filter_token <- str_replace_all(token_clean$filter_token, "[^[:alnum:]]", "")
# check results by looking at term that contains a non-ASCII character in the original token column on which the filter has not been applied.
token_clean[token_clean$token == "p=.47", c("token", "filter_token", "doc_id")]
# remove all the digit characters from words in token column
token_clean$filter_token <- gsub("[0-9]+" ,"", token_clean$filter_token)
# check results by looking at term that contains a number in the original token column on which the filter has not been applied here.
token_clean[token_clean$token == "p=.47", c("token", "filter_token", "doc_id")]
# check number of unique words after applying more filters
length(unique(token_clean$filter_token))
# SAVE COMPLETE DATA FRAME INCLUDING FILTER_TOKEN COLUMN
saveRDS(token_clean, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/study2_df_token.RData")
# SAVE INCLUDE_LEMMA
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_lemma <- lemma_clean[, list(freq = .N), by = list(doc_id = doc_id, term = filter_lemma)]
# check number of unique words and documents.
length(unique(nounbydoc_lemma$doc_id))
length(unique(nounbydoc_lemma$term))
# create a data frame with one column including the unique terms
filter_lemma <- unique(nounbydoc_lemma$term)
filter_lemma <- as.data.table(filter_lemma)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
filter_lemma <- as.data.table(filter_lemma)
saveRDS(filter_lemma, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/filter_lemma.RData")
# SAVE INCLUDE_TOKEN
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_token <- token_clean[, list(freq = .N), by = list(doc_id = doc_id, term = filter_token)]
# check number of unique words and documents.
length(unique(nounbydoc_token$doc_id))
length(unique(nounbydoc_token$term))
# create a data frame with one column including the unique terms
filter_token <- unique(nounbydoc_token$term)
filter_token <- as.data.table(filter_token)
saveRDS(filter_token, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/filter_token.RData")
# load libraries
library(dplyr)
library(cluster)
library(ggplot2)
library(devtools)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
set.seed(88)
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
# load data frame with column with selection of words to include in analysis.
df_incltoken <- readRDS("filter_token.RData")
# create df in which only the words that we want to be included are kept
token_embedding <- subset(vectors_glove, word %in% df_incltoken$filter_token)
# check number of unique words
length(unique((token_embedding$word)))
# 12743
# non-GloVe: check which words are in the token filter but are not in the feature matrix and are therefore lost (unwanted).
lost_tokens <- subset(df_incltoken, !(filter_token %in% token_embedding $word))
# 2117 words lost
length(unique((token_embedding$word))) + length(unique((lost_tokens$filter_token)))
# -> 14860. Which is equal to the line of code below.
length(unique((df_incltoken$filter_token)))
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
# load data frame with column with selection of words to include in analysis.
df_incltoken <- readRDS("filter_token.RData")
# create df in which only the words that we want to be included are kept
token_embedding <- subset(vectors_glove, word %in% df_incltoken$filter_token)
# check number of unique words
length(unique((token_embedding$word)))
# non-GloVe: check which words are in the token filter but are not in the feature matrix and are therefore lost (unwanted).
lost_tokens <- subset(df_incltoken, !(filter_token %in% token_embedding $word))
length(unique((token_embedding$word))) + length(unique((lost_tokens$filter_token)))
# -> 14860. Which is equal to the line of code below.
length(unique((df_incltoken$filter_token)))
# convert first column, word, to row index
library(tidyverse)
glove_embedding <- token_embedding %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
glove_embedding <- as.matrix(glove_embedding)
library(text2vec)
# create function to find similar words based on cosine distance
find_similar_words <- function(word, embedding_matrix, n = 5) {
similarities <- embedding_matrix[word, , drop = FALSE] %>%
sim2(embedding_matrix, y = ., method = "cosine")
similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}
find_similar_words("health",glove_embedding,25)
# fit the k-means clustering with 100 clusters
k_means_fit <- kmeans(glove_embedding, 100, iter.max = 30, nstart = 25)
# obtain the centroids
k_means_fit$centers
# look at the size of the clusters
k_means_fit$size
min(k_means_fit$size)
max(k_means_fit$size)
# find the cluster to which each word belongs
k_means_fit$cluster
# The cost function in kmeans is the total sum of the squares
k_means_fit$totss
# results
k_means_fit
# Create data frame in which the merge cluster assignment back to rows/word.
kw_with_cluster <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit$cluster))
# add column names
names(kw_with_cluster) <- c("word", "kmeans100")
View(kw_with_cluster)
# save this data frame which contains a column with words and a column with to which cluster they belong
saveRDS(kw_with_cluster, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/clustering100.RData")
# make a df for the first 5 cluster results, quickly "eyeball" results
cluster1 <- subset(kw_with_cluster, subset=kmeans100 == 1)
cluster2 <- subset(kw_with_cluster, subset=kmeans100 == 2)
cluster3 <- subset(kw_with_cluster, subset=kmeans100 == 3)
cluster4 <- subset(kw_with_cluster, subset=kmeans100 == 4)
cluster5 <- subset(kw_with_cluster, subset=kmeans100 == 5)
# Mother
# find out in which cluster the word mother is assigned
kw_with_cluster[kw_with_cluster$word == "mother", ]
# make a df of cluster 82
cluster82 <- subset(kw_with_cluster, subset = kmeans100 == 82)
View(cluster82)
# Health
# find out in which cluster the word health is assigned
kw_with_cluster[kw_with_cluster$word == "health", ]
# make a df of cluster 26
cluster26 <- subset(kw_with_cluster, subset=kmeans100 == 26)
# Environment
# find out in which cluster the word environment is assigned
kw_with_cluster[kw_with_cluster$word == "environment", ]
# make a df of cluster 28
cluster28 <- subset(kw_with_cluster, subset=kmeans100 == 28)
# Depression
# find out in which cluster the word depression is assigned so that I can check if all the forms of depression are in there.
kw_with_cluster[kw_with_cluster$word == "depression", ]
# make a df of cluster 31
cluster31 <- subset(kw_with_cluster, subset=kmeans100 == 31)
# ->  depressed, depression and depressive are in this cluster. If I look at the lemma_embedding dataframe and type depre in the filter within the word column I get the following words: nondepressed, antidepressant depressed, depression, depressive. It is not completely surprising that these other two words fall in a different cluster
kw_with_cluster[kw_with_cluster$word == "depressed", ]
kw_with_cluster[kw_with_cluster$word == "depressive", ]
kw_with_cluster[kw_with_cluster$word == "nondepressed", ]
kw_with_cluster[kw_with_cluster$word == "antidepressant", ]
View(cluster31)
# Kelly
# find out in which cluster the word Kelly is assigned so that I can check if all names are put together for example.
kw_with_cluster[kw_with_cluster$word == "kelly", ]
# make a df of cluster 5
cluster5 <- subset(kw_with_cluster, subset=kmeans100 == 5)
View(cluster5)
# cortisol
# find out in which cluster the word cortisol is assigned so that I can check if all names are put together for example.
kw_with_cluster[kw_with_cluster$word == "cortisol", ]
# make a df of cluster 100
cluster100 <- subset(kw_with_cluster, subset=kmeans100 == 100)
# empathy
# find out in which cluster the word empathy is assigned so that I can check if all names are put together for example.
kw_with_cluster[kw_with_cluster$word == "empathy", ]
# make a df of cluster 30
cluster30 <- subset(kw_with_cluster, subset = kmeans100 == 30)
View(cluster30)
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i, iter.max = 30)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
wssplot(glove_embedding, nc=150)
5*2,5
5*2.5
8*2,5
8*2.5
# fit the k-means clustering with 133 clusters
k_means_fit119 <- kmeans(glove_embedding, 119, iter.max = 30, nstart = 25)
# obtain the centroids
k_means_fit119$centers
# look at the size of the clusters
k_means_fit119$size
min(k_means_fit119$size)
max(k_means_fit119$size)
# find the cluster to which each word belongs
k_means_fit119$cluster
# The cost function in kmeans is the total sum of the squares
k_means_fit119$totss
# check results
k_means_fit119
# Create data frame in which the merge cluster assignment back to rows/word.
words_with_cluster119 <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit133$cluster))
# Create data frame in which the merge cluster assignment back to rows/word.
words_with_cluster119 <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit119$cluster))
# add column names
names(words_with_cluster119) <- c("word", "kmeans119")
# save this data frame which contains a column with words and a column with to which cluster they belong
saveRDS(kw_with_cluster, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/clustering119.RData")
max(k_means_fit119$size)
# look at the size of the clusters
k_means_fit119$size
# check the largest cluster with 274 observations
cluster119_42 <- subset(words_with_cluster119,
subset=kmeans119 == 42)
View(cluster119_42)
# Health
# find out in which cluster the word health is assigned
words_with_cluster119[words_with_cluster119$word == "health", ]
# make a df of cluster 107
cluster119_107 <- subset(words_with_cluster119,
subset=kmeans119 == 107)
View(cluster119_107)
# Depression
# find out in which cluster the word depression is assigned
words_with_cluster133[words_with_cluster133$word == "depression", ]
# Depression
# find out in which cluster the word depression is assigned
words_with_cluster133[words_with_cluster119$word == "depression", ]
# Depression
# find out in which cluster the word depression is assigned
words_with_cluster119[words_with_cluster119$word == "depression", ]
# make a df of cluster 86
cluster119_86 <- subset(words_with_cluster119,
subset=kmeans119 == 86)
View(cluster119_86)
words_with_cluster119[words_with_cluster119$word == "depressed", ]
worcs::git_update()
worcs::git_update()
