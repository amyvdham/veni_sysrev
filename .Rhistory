# REMOVE NON-ASCII CHARACTERS
# remove all the non-alphanumeric characters from words in lemma column
lemma_clean$filter_lemma <- str_replace_all(lemma_clean$filter_lemma, "[^[:alnum:]]", "")
# check results by looking at term that contains a non-ASCII character in original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# remove all the digit characters from words in lemma column
lemma_clean$filter_lemma <- gsub("[0-9]+" ,"", lemma_clean$filter_lemma)
# check results by looking at term that contains a number in the original lemma column on which the filter has not been applied.
lemma_clean[lemma_clean$lemma == "p=.47", c("lemma", "filter_lemma", "doc_id")]
# check number of unique words after applying more filters
length(unique(lemma_clean$filter_lemma))
# check number of unique words after applying more filters
length(unique(lemma_clean$filter_lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
lemma_clean <- lemma_clean %>%
anti_join(stop_words, by= c("filter_lemma" = "word"))
# check number of unique words after removing stop words
length(unique(lemma_clean$filter_lemma))
14065 - 13747
# SAVE COMPLETE DATA FRAME INCLUDING FILTER_LEMMA COLUMN
saveRDS(lemma_clean, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/study2_df_lemma.RData")
# SAVE FILTER_LEMMA
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_lemma <- lemma_clean[, list(freq = .N), by = list(doc_id = doc_id, term = filter_lemma)]
# check number of unique words and documents.
length(unique(nounbydoc_lemma$doc_id))
length(unique(nounbydoc_lemma$term))
unique(df$doc_id)
length(unique(df$doc_id))
length(unique(df_kw$doc_id))
df_res_kw <- df_res[upos %in% c("NOUN", "ADJ"), ]
length(unique(df_res_kw$doc_id))
6305-6098
length(unique(nounbydoc_lemma$term))
# check
# create a data frame with one column including the unique terms
filter_lemma <- unique(nounbydoc_lemma$term)
filter_lemma <- as.data.table(filter_lemma)
saveRDS(filter_lemma, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/final_filter.RData")
kw_tr <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "_")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr$keywords$keyword, ngram = kw_tr$keywords$ngram, sep = "_")
# investigating text rank results
stats <- subset(kw_tr$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
library(reshape2)
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
bigrams_data <- as.data.table(bigrams_data)
is.data.table(bigrams_data)
# SAVE filter with bigrams
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id, term = value)]
# check number of unique words and documents.
length(unique(nounbydoc_bigrams$doc_id))
length(unique(nounbydoc_bigrams$term))
# check
# create a data frame with one column including the unique terms
filter_bigrams <- unique(nounbydoc_bigrams$term)
filter_bigrams <- as.data.table(filter_bigrams)
saveRDS(filter_bigrams, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/bigrams_filter.RData")
kw_tr_glove <- textrank_keywords(x =  lemma_clean$filter_lemma[lemma_clean$upos %in% c("NOUN", "ADJ")], ngram_max = 2, sep = "-")
# merge bi grams to dataset
lemma_clean$keyword <- txt_recode_ngram(lemma_clean$filter_lemma, compound = kw_tr_glove$keywords$keyword, ngram = kw_tr_glove$keywords$ngram, sep = "-")
# investigating text rank results
stats <- subset(kw_tr_glove$keywords, ngram > 1 & freq >= 5)
head(stats, 30)
# stack the columns lemma_filter and keyword on top of each other
bigrams_data <- melt(lemma_clean, id.var = 1:14, variable.name = 'lemma_or_keyword')
# remove NA's
bigrams_data <- bigrams_data[!is.na(bigrams_data$value), ]
bigrams_data <- as.data.table(bigrams_data)
is.data.table(bigrams_data)
# SAVE filter with bigrams seperated with - instead of _ to apply on glove embedding
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc_bigrams_glove <- bigrams_data[,
list(freq = .N),
by = list(doc_id = doc_id,
term = value)]
# check number of unique words and documents.
length(unique(nounbydoc_bigrams_glove$doc_id))
length(unique(nounbydoc_bigrams_glove$term))
# check
# create a data frame with one column including the unique terms
filter_bigrams_glove <- unique(nounbydoc_bigrams_glove$term)
filter_bigrams_glove <- as.data.table(filter_bigrams_glove)
saveRDS(filter_bigrams_glove, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/bigrams_filter_glove.RData")
worcs::git_update()
# load final filter
final_filter <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/final_filter.RData")
# save as .csv
write.csv(final_filter,"/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/final_filter.csv", row.names = FALSE)
# load bigrams filter
bigrams_filter <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/bigrams_filter.RData")
# save as .csv
write.csv(bigrams_filter,"/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/bigrams_filter.csv", row.names = FALSE)
# load pretrained word2vec
wrd2vec_embedding <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/pretrained_w2v_filtered.csv", header = FALSE)
# adjust the first column name to word
colnames(wrd2vec_embedding)[1] <- "word"
# check structure of dataframe
str(wrd2vec_embedding)
# remove certain characters from the column V2 which now is column of the type character and contains a string as value.
library(tidyverse)
# create new data frame that can be used for applying adjustments
df_w2vemb <- wrd2vec_embedding
# remove the [ character from V2
df_w2vemb$V2 <-gsub("\\[","",as.character(df_w2vemb$V2))
# remove the ] character from V2
df_w2vemb$V2 <-gsub("\\]","",as.character(df_w2vemb$V2))
# remove the \n character from V2
df_w2vemb$V2 <-gsub("\\\n","",as.character(df_w2vemb$V2))
# check if removing the characters went correctly
df_w2vemb[1,c("word", "V2")]
# split column V2 into multiple columns
library(splitstackshape)
# separate on the space
df_w2vemb <- cSplit(df_w2vemb, "V2", " ")
# retain dimensions of data frame
dim(df_w2vemb)
# check values after splitting
df_w2vemb[1,]
# check how many unique words there are
length(unique(df_w2vemb$word))
# rename the column names of the data frame.
# First column is named word and the other columns dim1-40
colnames(df_w2vemb) <- c("word", paste0("dim", 1:300))
# check if there are any missings
summary(df_w2vemb)
# save the embedding
# convert the first column, word, to row index.
w2v_embedding <- df_w2vemb %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
w2v_embedding <- as.matrix(w2v_embedding)
str(w2v_embedding)
# load pretrained word2vec with bigrams included
w2v_embedding_bigram <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/pretrained_w2v_filtered_bigrams.csv", header = FALSE)
# adjust the first column name to word
colnames(w2v_embedding_bigram)[1] <- "word"
# check structure of dataframe
str(w2v_embedding_bigram)
# remove certain characters from the column V2 which now is column of the type character and contains a string as value.
# create new data frame that can be used for applying adjustments
df_bigrams <- w2v_embedding_bigram
# remove the [ character from V2
df_bigrams$V2 <-gsub("\\[","",as.character(df_bigrams$V2))
# remove the ] character from V2
df_bigrams$V2 <-gsub("\\]","",as.character(df_bigrams$V2))
# remove the \n character from V2
df_bigrams$V2 <-gsub("\\\n","",as.character(df_bigrams$V2))
# check if removing the characters went correctly
df_bigrams[1,c("word", "V2")]
# split column V2 into multiple columns
# separate on the space
df_bigrams <- cSplit(df_bigrams, "V2", " ")
# retain dimensions of data frame
dim(df_bigrams)
# check values after splitting
df_bigrams[1,]
# rename the column names of the data frame.
# First column is named word and the other columns dim1-40
colnames(df_bigrams) <- c("word", paste0("dim", 1:300))
# check if there are any missings
summary(df_bigrams)
# rename the column names of the data frame.
# First column is named word and the other columns dim1-40
colnames(df_bigrams) <- c("word", paste0("dim", 1:300))
# save the embedding
# convert the first column, word, to row index.
w2v_bigrams_embedding <- df_bigrams %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
w2v_bigrams_embedding <- as.matrix(w2v_bigrams_embedding)
str(w2v_bigrams_embedding)
# save the w2v embedding that includes bigrams
saveRDS(w2v_bigrams_embedding, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/w2v_bigrams_embedding.RData")
View(w2v_bigrams_embedding)
View(df_bigrams)
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
# create df in which only the words that we want to be included are kept
filtered_embedding <- subset(vectors_glove, word %in% final_filter$filter_lemma)
#### misschien ergens anders doen?? ###
# check number of unique words
length(unique((filtered_embedding$word)))
View(filtered_embedding)
# Make final GloVE embedding ready for analysis
# convert the first column, word, to row index.
glove_embedding <- filtered_embedding %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
glove_embedding <- as.matrix(glove_embedding)
str(glove_embedding)
# save the glove embedding to which the filter is applied so this can be easily loaded into other scripts
saveRDS(glove_embedding, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_final.RData")
# CREATE GLOVE EMBEDDING WITH BIGRAMS
# load data frame with column with selection of words to include in analysis.
df_bigrams_glove <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/bigrams_filter_glove.RData")
# create df in which only the words that we want to be included are kept
glove_embedding_bigrams <- subset(vectors_glove, word %in% df_bigrams_glove$filter_bigrams_glove)
#### misschien ergens anders doen?? ###
# check number of unique words
length(unique((glove_embedding_bigrams$word)))
View(glove_embedding_bigrams)
# transform so that it can be saved
glove_emb_bigrams <- glove_embedding_bigrams %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
glove_emb_bigrams <- as.matrix(glove_embedding_bigrams)
# convert dataframe to a matrix
glove_emb_bigrams <- as.matrix(glove_emb_bigrams)
# transform so that it can be saved
glove_emb_bigrams <- glove_embedding_bigrams %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
glove_emb_bigrams <- as.matrix(glove_emb_bigrams)
str(glove_emb_bigrams)
saveRDS(glove_emb_bigrams, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_bigrams.RData")
# check number of unique words
length(unique((rownames(glove_embedding))))
# non-GloVe: check which words are in the final filter but are not in the glove embedding data and are therefore lost (unwanted).
lost_glove <- subset(final_filter, !(filter_lemma %in% rownames(glove_embedding)))
View(lost_glove)
11562 + 2185
length(unique(rownames(glove_embedding))) + length(unique((lost_glove$filter_lemma)))
# -> 13,747. Which is equal to the line of code below.
length(unique((final_filter$filter_lemma)))
# glove embedding including bigrams
# check number of unique words
length(unique((rownames(glove_embedding_bigrams))))
View(bigrams_filter)
# glove embedding including bigrams
# check number of unique words
length(unique((rownames(glove_embedding_bigrams))))
# non-GloVe: check which words are in the bigrms filter but are not in the glove embedding data and are therefore lost (unwanted).
lost_glove_bigrams <- subset(bigrams_filter, !(filter_bigrams %in% rownames(glove_embedding_bigrams)))
length(unique(rownames(glove_embedding_bigrams))) + length(unique((lost_glove_bigrams$filter_bigrams)))
# -> 13,747. Which is equal to the line of code below.
length(unique((bigrams_filter$filter_bigrams)))
# non-GloVe: check which words are in the bigrms filter but are not in the glove embedding data and are therefore lost (unwanted).
lost_glove_bigrams <- subset(bigrams_filter, !(filter_bigrams %in% rownames(glove_embedding_bigrams)))
View(lost_glove_bigrams)
View(bigrams_filter)
View(df_bigrams_glove)
# non-GloVe: check which words are in the bigrms filter but are not in the glove embedding data and are therefore lost (unwanted).
lost_glove_bigrams <- subset(df_bigrams_glove, !(filter_bigrams %in% rownames(glove_embedding_bigrams)))
View(df_bigrams_glove)
# non-GloVe: check which words are in the bigrms filter but are not in the glove embedding data and are therefore lost (unwanted).
lost_glove_bigrams <- subset(df_bigrams_glove, !(filter_bigrams_glove %in% rownames(glove_embedding_bigrams)))
# non-GloVe: check which words are in the bigrms filter but are not in the glove embedding data and are therefore lost (unwanted).
lost_glove_bigrams <- subset(df_bigrams_glove, !(filter_bigrams_glove %in% rownames(glove_embedding_bigrams)))
View(lost_glove_bigrams)
View(glove_embedding_bigrams)
# non-GloVe: check which words are in the bigrms filter but are not in the glove embedding data and are therefore lost (unwanted).
lost_glove_bigram <- subset(df_bigrams_glove, !(filter_bigrams_glove %in% rownames(glove_embedding_bigrams)))
View(glove_embedding_bigrams)
# non-GloVe: check which words are in the bigrms filter but are not in the glove embedding data and are therefore lost (unwanted).
lost_glove_bigrams <- subset(df_bigrams_glove, !(filter_bigrams_glove %in% rownames(glove_bigrams_embedding)))
# glove embedding including bigrams
glove_bigrams_embedding <-  readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_bigrams.RData")
# non-GloVe: check which words are in the bigrms filter but are not in the glove embedding data and are therefore lost (unwanted).
lost_glove_bigrams <- subset(df_bigrams_glove, !(filter_bigrams_glove %in% rownames(glove_bigrams_embedding)))
length(unique(rownames(glove_bigrams_embedding))) + length(unique((lost_glove_bigrams$filter_bigrams_glove)))
# glove embedding including bigrams
# check number of unique words
length(unique((rownames(glove_bigrams_embedding))))
View(lost_glove_bigrams)
View(lost_glove)
# WORD2VEC EMBEDDING
# check number of unique words
length(unique((rownames(w2v_embedding))))
# non-w2v: check which words are in the final filter but are not in the w2v embedding data and are therefore lost (unwanted).
lost_w2v <- subset(final_filter, !(filter_lemma %in% rownames(w2v_embedding)))
length(unique(rownames(w2v_embedding))) + length(unique((lost_w2v$filter_lemma)))
View(lost_w2v)
# WORD2VEC EMBEDDING INCLUDING BIGRAMS
# check number of unique words
length(unique((rownames(w2v_bigrams_embedding))))
View(bigrams_filter)
# non-w2v: check which words are in the bigrams filter but are not in the w2v embedding data and are therefore lost (unwanted).
lost_w2v_bigrams <- subset(bigrams_filter, !(filter_bigrams %in% rownames(w2v_bigrams_embedding)))
length(unique(rownames(w2v_bigrams_embedding))) + length(unique((lost_w2v_bigrams$filter_bigrams)))
# -> 13,747. Which is equal to the line of code below.
length(unique((bigrams_filter$filter_bigrams)))
# COMPARE GLOVE AND GLOVE BIGRAMS
words_diff_glove <- subset(glove_bigrams_embedding, !(rownames(glove_bigrams_embedding) %in% rownames(glove_embedding)))
View(words_diff_glove)
length(words_diff_glove)
length(unique(rownames(words_diff_glove))
length(unique(rownames(words_diff_glove)))
length(unique(rownames(words_diff_glove)))
length(unique(rownames(glove_bigrams_embedding))) - length(unique(rownames(glove_embedding)))
# COMPARE GLOVE AND GLOVE BIGRAMS
diff_glove <- subset(glove_bigrams_embedding, !(rownames(glove_bigrams_embedding) %in% rownames(glove_embedding)))
# check the length of difference
length(unique(rownames(words_diff_glove)))
View(diff_glove)
# COMPARE W2V AND W2V BIGRAMS
diff_w2v <- subset(w2v_bigrams_embedding, !(rownames(w2v_bigrams_embedding) %in% rownames(w2v_embedding)))
View(diff_w2v)
diff_w2v["health_care", ]
diff_glove["health-care", ]
diff_glove["climate-change", ]
diff_glove["mental-health", ]
diff_glove["biopolar-disorder", ]
diff_w2v["biopolar_disorder", ]
diff_w2v["bipolar_disorder", ]
diff_glove["bipolar-disorder", ]
# check the length of difference
length(unique(rownames(diff_w2v)))
length(unique(rownames(w2v_bigrams_embedding))) - length(unique(rownames(w2v_embedding)))
# check the length of difference
length(unique(rownames(words_diff_glove)))
diff_w2v["emotional_dysregulation", ]
diff_w2v["emotional_regulation", ]
diff_w2v["emotion_regulation", ]
# COMPARE GLOVE AND W2V
# check words that are in the glove embedding that are also in the w2v embedding
words_same <- subset(glove_embedding, rownames(glove_embedding) %in% rownames(w2v_embedding))
length(unique(rownames(words_same)))
# check words that are in the glove embedding but not in the w2vec embedding
words_diff <- subset(glove_embedding, !(rownames(glove_embedding) %in% rownames(w2v_embedding)))
length(unique(rownames(words_diff)))
length(unique(rownames(words_same))) + length(unique(rownames(words_diff)))
length(unique(rownames(words_same))) + length(unique(rownames(words_diff))) == length(unique(rownames(glove_embedding)))
length(unique(rownames(words_same))) - length(unique(rownames(w2v_embedding)))
# check words that are in the w2v embedding but not in the glove embedding
words_diff_w2v <- subset(w2v_embedding, !(rownames(w2v_embedding) %in% rownames(glove_embedding)))
View(words_diff_w2v)
View(words_diff)
length(unique(rownames(words_diff)))
10,077 + 1499
10077 + 1499
11576-14
length(unique(rownames(w2v_embedding))) + length(unique(rownames(words_diff))) -  length(unique(rownames(words_diff_w2v))) == length(unique(rownames(glove_embedding)))
length(unique(rownames(words_same)))
length(unique(rownames(w2v_embedding))) + length(unique(rownames(words_diff))) == length(unique(rownames(glove_embedding)))
# COMPARE GLOVE BIGRAMS AND W2V BIGRAMS
# because the bigrams are split differently in the two dataset we first have to apply some data manipulations
diff_glove$word <- rownames(diff_glove)
View(diff_glove)
# COMPARE GLOVE AND GLOVE BIGRAMS
diff_glove <- subset(glove_bigrams_embedding, !(rownames(glove_bigrams_embedding) %in% rownames(glove_embedding)))
# COMPARE GLOVE BIGRAMS AND W2V BIGRAMS
# because the bigrams are split differently in the two dataset we first have to apply some data manipulations
diff_glove_bigram_comp <- as.data.frame(diff_glove)
diff_glove_bigram_comp$word <- rownames(diff_glove_bigram_comp)
View(diff_glove_bigram_comp)
diff_glove_bigram_comp$word <- rownames(diff_glove_bigram_comp)
View(diff_glove_bigram_comp)
diff_glove_bigram_comp <- diff_glove_bigram_comp["word"]
View(diff_glove_bigram_comp)
diff_glove_bigram_comp$w2v_style <- gsub('-', '_', diff_glove_bigram_comp$word)
View(diff_glove_bigram_comp)
# Now check which bigrams are in the w2v embedding but not in the glove embedding
bigrams_in_w2v <- subset(diff_w2v, !(rownames(diff_w2v) %in% diff_glove_bigram_comp$w2v_style))
View(bigrams_in_w2v)
# check which, and how many, bigrams are in both embeddings
bigrams_both <- subset(diff_w2v, rownames(diff_w2v) %in% diff_glove_bigram_comp$w2v_style)
lenght(unique(rownnames(bigrams_both)))
length(unique(rownnames(bigrams_both)))
length(unique(rownames(bigrams_both)))
View(bigrams_both)
# Check which bigrams are in the glove embedding but not in the w2v embedding
bigrams_in_glove <- subset(diff_glove_bigram_comp, !(w2v_style %in% rownames(diff_w2v)))
View(bigrams_in_glove)
length(unique(rownames(diff_w2v))) == length(unique(rownames(bigrams_both))) + length(unique(rownames(bigrams_in_w2v)))
# check if things add up for glove
length(unique(rownames(diff_glove))) == length(unique(rownames(bigrams_both))) + length(unique(rownames(bigrams_in_glove)))
1,494 + 10,077
1494 + 10077
1485 + 10077
View(bigrams_in_glove)
View(bigrams_in_w2v)
# load glove word embedding file
glove_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_final.RData")
str(glove_embedding)
# load glove bigrams word embedding file
glove_bigrams_embedding <-  readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_bigrams.RData")
str(glove_bigrams_embedding)
11934 - 11562
19/2
10077-10297
# COMPARE GLOVE AND GLOVE BIGRAMS
diff_glove <- subset(glove_bigrams_embedding, !(rownames(glove_bigrams_embedding) %in% rownames(glove_embedding)))
# check the length of difference
length(unique(rownames(diff_glove)))
length(unique(rownames(glove_bigrams_embedding))) - length(unique(rownames(glove_embedding)))
# COMPARE W2V AND W2V BIGRAMS
diff_w2v <- subset(w2v_bigrams_embedding, !(rownames(w2v_bigrams_embedding) %in% rownames(w2v_embedding)))
length(unique(rownames(glove_bigrams_embedding)))
length(unique(rownames(glove_embedding)))
11943 - 11562
# load glove word embedding file
glove_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_final.RData")
# load glove bigrams word embedding file
glove_bigrams_embedding <-  readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/try_outs/glove_embedding_bigrams.RData")
# load w2v word embedding file
w2v_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/w2v_embedding.RData")
# load w2v bigrams word embedding file
w2v_bigrams_embedding <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/w2v_bigrams_embedding.RData")
library(text2vec)
# create function to find similar words based on cosine distance
find_similar_words <- function(word, embedding_matrix, n = 5) {
similarities <- embedding_matrix[word, , drop = FALSE] %>%
sim2(embedding_matrix, y = ., method = "cosine")
similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}
# Run function on the word health and see the 25 closest words based on cosine similarity.
find_similar_words("white",glove_embedding,50)
# check if socio-emotional and socioemotional are similar words in the glove bigrams embedding
find_similar_words("socio-emotional",glove_bigrams_embedding,50)
find_similar_words("socio-emotional",glove_bigrams_embedding,25)
find_similar_words("socioemotional",glove_bigrams_embedding,25)
# load data frame with word vectors into object
df <- readRDS("asreview_embedding_sim.Rdata")
# final filter
final_filter <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/final_filter.RData")
View(df)
## CREATE EMBEDDING WITH FINAL FILTER
# add word column
df$word <- rownames(df)
View(df)
# final filter
final_filter <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/final_filter.RData")
asr_final_embedding <- subset(df, rownames(df) %in% final_filter$final_lemma)
View(final_filter)
asr_final_embedding <- subset(df, rownames(df) %in% final_filter$filter_lemma)
View(asr_final_embedding)
find_similar_words("mother",asr_final_embedding,25)
# load data frame with word vectors into object
df <- readRDS("asreview_embedding_sim.Rdata")
# load data frame with word vectors into object
df <- readRDS("asreview_embedding_sim.Rdata")
## CREATE EMBEDDING WITH FINAL FILTER
# add word column
df$word <- rownames(df)
# final filter
final_filter <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/final_filter.RData")
asr_final_embedding <- subset(df, rownames(df) %in% final_filter$filter_lemma)
asr_final_embedding <- as.matrix(asr_final_embedding)
# find similar words
word <- asr_final_embedding["mother", , drop = FALSE]
cos_sim = sim2(x = asr_final_embedding, y = word, method = "cosine", norm = "l2")
str(asr_embedding)
str(asr_final_embedding)
# transform dataframe to matrix
asr_final_embedding <- t(asr_final_embedding)
str(asr_final_embedding)
# load data frame with word vectors into object
df <- readRDS("asreview_embedding_sim.Rdata")
# final filter
final_filter <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/final_filter.RData")
# extract word vectors of terms in final filter
asr_final_embedding <- subset(df, rownames(df) %in% final_filter$filter_lemma)
# transform dataframe to matrix
asr_final_embedding <- as.matrix(asr_final_embedding)
str(asr_final_embedding)
# load data frame with word vectors into object
df <- readRDS("asreview_embedding_sim.Rdata")
## CREATE EMBEDDING WITH FINAL FILTER
# add word column
df_checkbigrams <- df
df_checkbigrams$word <- rownames(df)
# final filter
final_filter <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/final_filter.RData")
# extract word vectors of terms in final filter
asr_final_embedding <- subset(df, rownames(df) %in% final_filter$filter_lemma)
# transform dataframe to matrix
asr_final_embedding <- t(asr_final_embedding)
str(asr_final_embedding)
# transform dataframe to matrix
asr_final_embedding <- t(asr_final_embedding)
str(asr_final_embedding)
# find similar words
word <- asr_final_embedding["mother", , drop = FALSE]
cos_sim = sim2(x = asr_final_embedding, y = word, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 25)
head(sort(cos_sim[,1], decreasing = TRUE), 50)
# find similar words
word <- asr_final_embedding["depression", , drop = FALSE]
cos_sim = sim2(x = asr_final_embedding, y = word, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 50)
# find similar words
word <- asr_final_embedding["escalator", , drop = FALSE]
cos_sim = sim2(x = asr_final_embedding, y = word, method = "cosine", norm = "l2")
# find similar words
word <- asr_final_embedding[" escalators", , drop = FALSE]
# find similar words
word <- asr_final_embedding["escalators", , drop = FALSE]
cos_sim = sim2(x = asr_final_embedding, y = word, method = "cosine", norm = "l2")
# find similar words
word <- asr_final_embedding["mother", , drop = FALSE]
cos_sim = sim2(x = asr_final_embedding, y = word, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 50)
