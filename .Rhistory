ncol = ncol(X),
dimnames = list(colnames(X), colnames(X)))
cos <- as.data.frame(m)
# calculate the pairwise cosine similarity
for(i in 1:ncol(X)) {
for(j in i:ncol(X)) {
co_rate_1 <- X[which(X[, i] & X[, j]), i]
co_rate_2 <- X[which(X[, i] & X[, j]), j]
cos[i, j] <- CosineFun(co_rate_1, co_rate_2)
# fill in the opposite diagonal entry
cos[j, i] <- cos[i, j]
}
}
return(cos)
}
# Note that t() is used to transform the data frame to a matrix
cosine_similarity <- CosineSim(t(df_final))
View(cosine_similarity)
# calculate the cosine silhouette width, which in cosine land is
# (1) the lowest average dissimilarity of the data point to any other cluster,
#  minus
# (2) the average dissimilarity of the data point to all other data points in
#     the same cluster
cosineSilhouette <- function(cosine.matrix, membership) {
# Args:
#   cosine.matrix: the cosine similarity matrix for the words
#   membership: the named membership vector for the rows and columns.
#               The entries should be cluster centers and the vector
#               names should be the words.
if (!is.factor(membership)) {
stop("membership must be a factor")
}
# note that there are some floating point issues:
# (some "1" entires are actually sliiightly larger than 1)
cosine.dissim <- acos(round(cosine.matrix, 10)) / pi
widths.list <- lapply(levels(membership), function(clust) {
# filter rows of the similarity matrix to words in the current cluster
# filter cols of the similarity matrix to words in the current cluster
cosine.matrix.inside <- cosine.dissim[membership == clust,
membership == clust]
# a: average dissimilarity of i with all other data in the same cluster
a <- apply(cosine.matrix.inside, 1, mean)
# filter rows of the similarity matrix to words in the current cluster
# filter cols of the similarity matrix to words NOT in the current cluster
other.clusters <- levels(membership)[levels(membership) != clust]
cosine.matrix.outside <- sapply(other.clusters, function(other.clust) {
cosine.dissim[membership == clust, membership == other.clust] %>%
apply(1, mean) # average over clusters
})
# b is the lowest average dissimilarity of i to any other cluster of
# which i is not a member
b <- apply(cosine.matrix.outside, 1, min)
# silhouette width is b - a
cosine.sil.width <- b - a
data.frame(word = names(cosine.sil.width), width = cosine.sil.width)
})
widths.list <- do.call(rbind, widths.list)
# join membership onto data.frame
membership.df <- data.frame(word = names(membership),
membership = membership)
widths.list <- left_join(widths.list, membership.df, by = "word")
return(widths.list)
}
set.seed(238942)
# calculate the average silhouette width for k=5, ..., 20
sil.width <- sapply(5:20, function(k) {
# generate k clusters
membership <- pam(cosine_similarity, k = k)
# calcualte the silhouette width for each observation
width <- cosineSilhouette(cosine_similarity,
membership = factor(membership$clustering))$width
return(mean(width))
})
set.seed(238942)
# calculate the average silhouette width for k=5, ..., 20
sil.width <- sapply(5:20, function(k) {
# generate k clusters
membership <- pam(cosine_similarity, k = k)
# calcualte the silhouette width for each observation
width <- cosineSilhouette(cosine_similarity,
membership = factor(membership$clustering))$width
return(mean(width))
})
# plot k verus silhouette width
data.frame(k = 5:20, width = sil.width) %>%
ggplot(aes(x = k, y = width)) +
geom_line() +
geom_point() +
scale_y_continuous(name = "Avergae silhouette width")
library(cluster)
generateClusters <- function(similarity.mat, k.range, N) {
random.subset.list <- lapply(1:100, function(i) {
sample(1:nrow(similarity.mat), 0.9 * nrow(similarity.mat))
})
lapply(k.range, function(k) {
print(paste("k =", k))
lapply(1:N, function(i) {
# randomly sample 90% of words
cosine.sample <- similarity.mat[random.subset.list[[i]], random.subset.list[[i]]]
# perform clustering
pam.clusters <- pam(1 - cosine.sample, k = k, diss = TRUE)
})
})
}
# generate clusters ranging from 5 to 20 cluster groups for each of 100 subsamples
# This will take a little while to run
cluster.iterations <- generateClusters(cosine_similarity,
k.range = 5:20,
N = 100)
join.cluster.iterations <- lapply(cluster.iterations, function(list) {
# for each list of iterations (for a specific k),
# full-join the membership vectors into a data frame
# (there will be missing values in each column)
Reduce(function(x, y) full_join(x, y, by = "words"),
lapply(list, function(cluster.obj) {
df <- data.frame(words = names(cluster.obj$clustering),
clusters = cluster.obj$clustering)
}))
})
# clean column names
join.cluster.iterations <- lapply(join.cluster.iterations, function(x) {
colnames(x) <- c("words", paste0("membership", 1:100))
return(x)
})
# view the first 8 columns of the first data frame (correpsonding to k=5)
kable(head(join.cluster.iterations[[1]][, 1:8]))
# calculate the pairwise jaccard similarity between each of the cluster
# memberships accross the common words
# to avoid correlation, we do this pairwise between simulations 1 and 2,
# and then between simulations 3 and 4, and so on
library(Rcpp)
library(reshape2)
# use Rcpp to speed up the computation
sourceCpp('Rcpp_similarity.cpp')
jaccard.similarity <- sapply(join.cluster.iterations,
function(cluster.iteration) {
sapply(seq(2, ncol(cluster.iteration) - 1, by = 2),
function(i) {
# calculate the Jaccard similarity between each pair of columns
cluster.iteration.pair <- cluster.iteration[ , c(i, i + 1)]
colnames(cluster.iteration.pair) <- c("cluster1", "cluster2")
# remove words that do not appear in both 90% sub-samples
cluster.iteration.pair <- cluster.iteration.pair %>%
filter(!is.na(cluster1), !is.na(cluster2))
# Calcualte the Jaccard similarity between the two cluster vectors
RcppSimilarity(cluster.iteration.pair[ , 1],
cluster.iteration.pair[ , 2])
})
})
# average similarity over simulations
jaccard.similarity.long <- melt(jaccard.similarity)
colnames(jaccard.similarity.long) <- c("iter", "k", "similarity")
# k is the number of clusters
jaccard.similarity.long$k <- jaccard.similarity.long$k + 4
jaccard.similarity.long <- jaccard.similarity.long %>%
filter(k <= 20)
# average over iterations
jaccard.similarity.avg <- jaccard.similarity.long %>%
group_by(k) %>%
summarise(similarity = mean(similarity))
# plot number of clusters versus Jaccard similarity
ggplot(jaccard.similarity.long) +
geom_boxplot(aes(x = k, y = similarity, group = k)) +
geom_line(aes(x = k, y = similarity),
linetype = "dashed",
data = jaccard.similarity.avg) +
ggtitle("Jaccard similarity versus k")
View(jaccard.similarity.long)
# plot number of clusters versus Jaccard similarity
ggplot(data = jaccard.similarity.long) +
geom_boxplot(mapping = aes(x = k, y = similarity, group = k)) +
geom_line(aes(x = k, y = similarity),
linetype = "dashed",
data = jaccard.similarity.avg) +
ggtitle("Jaccard similarity versus k")
# plot number of clusters versus Jaccard similarity
ggplot(data = jaccard.similarity.long) +
geom_boxplot(jaccard.similarity.long, aes(x = k, y = similarity, group = k)) +
geom_line(aes(x = k, y = similarity),
linetype = "dashed",
data = jaccard.similarity.avg) +
ggtitle("Jaccard similarity versus k")
# plot number of clusters versus Jaccard similarity
ggplot() +
geom_boxplot(jaccard.similarity.long, mapping = aes(x = k, y = similarity, group = k)) +
geom_line(aes(x = k, y = similarity),
linetype = "dashed",
data = jaccard.similarity.avg) +
ggtitle("Jaccard similarity versus k")
View(jaccard.similarity.avg)
# plot number of clusters versus Jaccard similarity
ggplot(jaccard.similarity.long) +
geom_boxplot(aes(x = k, y = similarity, group = k)) +
# geom_line(aes(x = k, y = similarity),
#linetype = "dashed",
#data = jaccard.similarity.avg) +
ggtitle("Jaccard similarity versus k")
# plot number of clusters versus Jaccard similarity
ggplot(jaccard.similarity.long) +
geom_boxplot(aes(x = k, y = similarity, group = k)) +
geom_line(aes(x = k, y = similarity),
linetype = "dashed",
data = jaccard.similarity.long) +
ggtitle("Jaccard similarity versus k")
# average over iterations
jaccard.similarity.avg <- jaccard.similarity.long %>%
group_by(k) %>%
summarise(similarity = mean(similarity))
?summarise()
# average over iterations
jaccard.similarity.avg <- jaccard.similarity.long %>%
group_by(k) %>%
summarise(similarity = mean(similarity), .groups = 'keep')
View(jaccard.similarity.avg)
# average over iterations
jaccard.similarity.avg <- jaccard.similarity.long %>%
group_by(k) %>%
similarity = mean(similarity)
# average over iterations
jaccard.similarity.avg <- jaccard.similarity.long %>%
group_by(k)
View(jaccard.similarity.avg)
# average over iterations
jaccard.similarity.avg <- jaccard.similarity.long %>%
group_by(k) %>%
summarize(similarity = mean(similarity))
View(jaccard.similarity.long)
# average over iterations
jaccard.similarity.avg <-
jaccard.similarity.long %>%
group_by(k) %>%
summarize(similarity_mean = mean(similarity))
# average over iterations
jaccard.similarity.avg <-
View(jaccard.similarity.avg)
# average over iterations
jaccard.similarity.avg <-
jaccard.similarity.long %>%
group_by(k) %>%
mutate(similarity = mean(similarity) )
View(jaccard.similarity.avg)
View(jaccard.similarity.long)
# average over iterations
jaccard.similarity.avg <-
jaccard.similarity.long %>%
group_by(k) %>%
mutate(similarity_mean = mean(similarity))
View(jaccard.similarity.avg)
d <- accard.similarity.long  %>%
group_by(k) %>%
summarise_at(vars(similarity), funs(mean(., na.rm=TRUE)))
d <- jaccard.similarity.long  %>%
group_by(k) %>%
summarise_at(vars(similarity), funs(mean(., na.rm=TRUE)))
d <- jaccard.similarity.long  %>%
group_by(k) %>%
summarise(across(similarity, mean, na.rm = TRUE))
View(d)
# plot number of clusters versus Jaccard similarity
ggplot(jaccard.similarity.long) +
geom_boxplot(aes(x = k, y = similarity, group = k)) +
# hier gaat iets mis kan k niet vinden omdat bij data jaccard.similarity.avg staat maar daar zit alleen maar 1 waarde in.
geom_line(aes(x = k, y = similarity),
linetype = "dashed",
data = d) +
ggtitle("Jaccard similarity versus k")
d <- jaccard.similarity.long  %>%
group_by(k) %>%
summarise_at(.vars = similarity,.funs = c(mean="mean"))
View(jaccard.similarity.avg)
View(jaccard.similarity.long)
View(jaccard.similarity.long)
d <- jaccard.similarity.long  %>%
group_by(k) %>%
summarise_at(.vars = names(.)[3],.funs = c(mean="mean"))
View(d)
# average over iterations
jaccard.similarity.avg <- jaccard.similarity.long  %>%
group_by(k) %>%
summarise_at(.vars = names(.)[3],.funs = c(similarity ="mean"))
View(jaccard.similarity.avg)
# plot number of clusters versus Jaccard similarity
ggplot(jaccard.similarity.long) +
geom_boxplot(aes(x = k, y = similarity, group = k)) +
geom_line(aes(x = k, y = similarity),
linetype = "dashed",
data = jaccard.similarity.avg) +
ggtitle("Jaccard similarity versus k")
# note that there are some floating point issues in the similarity matrix:
# some "1" entires are actually sliiightly larger than 1, so we round to
# the nearest 10 dp when calcualting the distance matrix
word.clusters <- pam(acos(round(cosine.similarity.full, 10)) / pi, k = 20, diss = TRUE)
# note that there are some floating point issues in the similarity matrix:
# some "1" entires are actually sliiightly larger than 1, so we round to
# the nearest 10 dp when calcualting the distance matrix
word.clusters <- pam(acos(round(cosine_similarity, 10)) / pi, k = 20, diss = TRUE)
word.clusters.13 <- pam(acos(round(cosine_similarity, 10)) / pi, k = 13, diss = TRUE)
# print the cluster medoids
word.clusters$medoids
# convert the membership vector to a factor
word.membership <- factor(word.clusters$clustering)
# print the cluster medoids
word.clusters.13$medoids
# convert the membership vector to a factor
word.membership.13 <- factor(word.clusters.13$clustering)
# replace integer membership by medoid membership
levels(word.membership) <- word.clusters$medoids
# replace integer membership by medoid membership
levels(word.membership.13) <- word.clusters.13$medoids
# compare the membership vectors with 20 and 13 clusters
word.membership.split <- split(word.membership, word.membership)
word.membership.split.13 <- split(word.membership.13, word.membership.13)
compare.20.13 <- sapply(word.membership.split, function(i) {
sapply(word.membership.split.13, function(j) {
sum(names(i) %in% names(j)) / length(i)
})
})
superheat(compare.20.13,
heat.pal = c("white", "grey", "black"),
heat.pal.values = c(0, 0.1, 1),
column.title = "11 clusters",
row.title = "12 clusters",
bottom.label.text.angle = 90,
bottom.label.size = 0.4)
superheat(compare.20.13,
heat.pal = c("white", "grey", "black"),
heat.pal.values = c(0, 0.1, 1),
column.title = "20 clusters",
row.title = "13 clusters",
bottom.label.text.angle = 90,
bottom.label.size = 0.4)
# calculate the cosine silhouette width
cosine.silhouette <-
cosineSilhouette(cosine.similarity.full, word.membership)
# calculate the cosine silhouette width
cosine.silhouette <-
cosineSilhouette(cosine_similarity, word.membership)
# arrange the words in the same order as the original matrix
rownames(cosine.silhouette) <- cosine.silhouette$word
cosine.silhouette <- cosine.silhouette[rownames(cosine_similarity), ]
# calculate the average width for each cluster
avg.sil.width <- cosine.silhouette %>%
group_by(membership) %>%
summarise(avg.width = mean(width)) %>%
arrange(avg.width)
# add a blank space after each word (for aesthetic purposes)
word.membership.padded <- paste0(word.membership, " ")
# reorder levels based on increasing separation
word.membership.padded <- factor(word.membership.padded,
levels = paste0(avg.sil.width$membership, " "))
superheat(cosine_similarity,
# row and column clustering
membership.rows = word.membership.padded,
membership.cols = word.membership.padded,
# top plot: silhouette
yt = cosine.silhouette$width,
yt.axis.name = "Cosine\nsilhouette\nwidth",
yt.plot.type = "bar",
yt.bar.col = "grey35",
# order of rows and columns within clusters
order.rows = order(cosine.silhouette$width),
order.cols = order(cosine.silhouette$width),
# bottom labels
bottom.label.col = c("grey95", "grey80"),
bottom.label.text.angle = 90,
bottom.label.text.alignment = "right",
bottom.label.size = 0.28,
# left labels
left.label.col = c("grey95", "grey80"),
left.label.text.alignment = "right",
left.label.size = 0.26,
# title
title = "(a)")
superheat(cosine_similarity,
# row and column clustering
membership.rows = word.membership.padded,
membership.cols = word.membership.padded,
# top plot: silhouette
yt = cosine.silhouette$width,
yt.axis.name = "Cosine\nsilhouette\nwidth",
yt.plot.type = "bar",
yt.bar.col = "grey35",
# order of rows and columns within clusters
order.rows = order(cosine.silhouette$width),
order.cols = order(cosine.silhouette$width),
# bottom labels
bottom.label.col = c("grey95", "grey80"),
bottom.label.text.angle = 90,
bottom.label.text.alignment = "right",
bottom.label.size = 0.28,
# left labels
left.label.col = c("grey95", "grey80"),
left.label.text.alignment = "right",
left.label.size = 0.26,
# smooth heatmap within clusters
smooth.heat = T,
# title
title = "(b)")
library(RColorBrewer)
library(wordcloud)
# define a function that takes the cluster name and the membership vector
# and returns a word cloud
makeWordCloud <- function(cluster, word.membership, words.freq) {
words <- names(word.membership[word.membership == cluster])
words.freq <- words.freq[words]
# make all words black except for the cluster center
words.col <- rep("black", length = length(words.freq))
words.col[words == cluster] <- "red"
# the size of the words will be the frequency from the NY Times headlines
wordcloud(words, words.freq, colors = words.col,
ordered.colors = TRUE, random.order = FALSE, max.words = 80)
}
# plot word clouds
set.seed(52545)
for (word in levels(word.membership)) {
makeWordCloud(word, word.membership, words.freq = freq)
}
# convert the membership vector to a factor
word.membership <- factor(word.clusters$clustering)
for (word in levels(word.membership)) {
makeWordCloud(word, word.membership, words.freq = freq())
}
for (word in levels(word.membership)) {
makeWordCloud(word, word.membership, words.freq = freq)
}
# convert the membership vector to a factor
word.membership$membership <- factor(word.clusters$clustering)
View(word.membership)
View(word.clusters)
word <- df_final["empathy", , drop = FALSE]
cos_sim = sim2(x = df_final, y = word, method = "cosine", norm = "l2")
# Loading required libraries
library(stringr)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(Matrix)
library(ggplot2)
library(dplyr)
# Required library for GloVe
library(text2vec)
# Loading script containing functions
source("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/word_functions.R")
# prepare data
# this is were line 891 in manuscript file starts
recs <- read.csv("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]
## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column.
df <- merge_df(recs, df, "word")
# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
# check the number of author keywords per doc
df_n_keywords <- df %>%
group_by(doc) %>%
summarise(n_keywords = n()) # snap nog niet helemaal hoe deze code weet dat die n() dan betrekking heeft op de column word maar gaat wel goed.
## NOTE: some keywords contain more than one word, e.g. risk factors or follow-up studies and youth smoking behavior. So to now the lenght (number of words) per doc this won't work
# Now that the dataset is cleaned created a df in which each document is a row and the column DEclean contains the author keywords of the specific document. Shape the tokens back to their original form
df_new <- df %>%
group_by(word) %>%
group_by(doc) %>%
summarise(DEclean = str_c(word, collapse = " "))
# second attempt word count
df_new$DEclean_length <- sapply(df_new$DEclean, function(x){str_count(x, " ")+1})
# this gives us the number of words in the column DEclean for each row. NOTE: words that are connected by a - are seen as one word, e.g. follow-up or affect-regulatory. Also, abbreviations are also counted as a word, e.g. INFORMATION AND COMMUNICATION TECHNOLOGY (ICT). (ICT) is counted in this example.
df_new <- data.table(df_new)
# Max length of author keywords
max(df_new$DEclean_length)
# investigate with the highest value of number of author keywords
df_new[df_new$doc == 3667, 'DEclean']
# check if this is a result of data cleaning by looking at the original data set
recs[recs$doc == 3667, 'DE']
# -> shows that this is not a result of data cleaning and it seems that this article does really have 524 author keywords.
# check the doi of this article so I can look it up
recs[recs$doc == 3667, c('doi', 'DI', 'title', 'AU', 'PY')]
# -> online this journal contains all the keywords of the articles belonging to this journal in the past 5 years.
# check article with 77 author keywords
recs[recs$doc == 5234, c('doi', 'DI', 'title', 'AU', 'PY', 'DE')]
# -> looked up the article online and does not seem to have any author keywords, do not not know where the words come from.
# check article with 33 author keywords
recs[recs$doc == 2750, c('doi', 'DI', 'title', 'AU', 'PY', 'DE')]
# -> this is correct looked up the article and does indeed contain 33 words in the author keywords. Same is the case for the article with 30 author keywords (doc = 1186)
# NOTE: document/case/row shown below contains < in author keywords. Does this need to be cleaned? It also includes the word literature whereas this word was part of the terms that had to be excluded.
df_new[df_new$doc == 5234, 'DEclean']
# check why those words with literature were not excluded
df[df$doc == 5234, 'word']
# ->  they were not excluded because the regular expression ^literature$ tells to only delete those values that where start of the string is followed by literature followed by the end of the string.
# check how many unique documents and how many unique words there are after excluding non substantive words.
length(unique(df$doc))
length(unique(df$word))
# wat ligt er dicht bij 'school'
word <- glove_embedding["school", , drop = FALSE]
word <- df_final["empathy", , drop = FALSE]
cos_sim = sim2(x = df_final, y = word, method = "cosine", norm = "l2")
cos_sim = sim2(x = t(df_final), y = word, method = "cosine", norm = "l2")
df_final <- t(df_final)
word <- df_final["empathy", , drop = FALSE]
cos_sim = sim2(x = t(df_final), y = word, method = "cosine", norm = "l2")
worcs::git_update()
