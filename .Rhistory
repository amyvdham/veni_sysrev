# check if filter went correctly
df2[df2$lemma == "data.", c("lemma", "doc_id")]
df2 <- clean_df %>%
mutate(lemma1 = str_remove_all(lemma, "[.]"))
# check if filter went correctly
df2[df2$lemma == "data.", c("lemma", "doc_id")]
df2 <- clean_df
df2$lemma <- gsub("\\.","",lemma)
View(df2)
df2$lemma <- gsub("\\.","",df2$lemma)
# check if filter went correctly
df2[df2$lemma == "data.", c("lemma", "doc_id")]
# check if filter went correctly
df2[df2$lemma == "data", c("lemma", "doc_id")]
# check if filter went correctly
clean_df[clean_df$lemma == "data.", c("lemma", "doc_id")]
# check if filter went correctly
clean_df[clean_df$lemma == "data", c("lemma", "doc_id")]
# remove dots from string values in lemma column
clean_df$lemma <- gsub("\\.","",clean_df$lemma)
# check if filter went correctly
clean_df[clean_df$lemma == "data.", c("lemma", "doc_id")]
# check if filter went correctly
clean_df[clean_df$lemma == "data", c("lemma", "doc_id")]
# check number of unique words after removing dots. Should be the same as before.
length(unique(clean_df$lemma))
# first I want to load in a file that still contains the upos column to see if the stopwords have been assignent NOUN or ADJ. On this file all filters have been applied until textrank algorith. So this is the file that has been used to create include_these2. On this file we want to apply more filters (mentioned above) before saving it as include_these3.
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check which value is assigned to the word "and" in the upos column.
df_check[df_check$lemma == "and", c("lemma", "upos")]
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
clean_df <- df_check %>%
anti_join(stop_words, by= c("lemma" = "word"))
# check number of unique words after removing stop words
length(unique(clean_df$lemma))
# double check if the word "and" is removed from the file
clean_df[clean_df$lemma == "and", c("lemma", "doc_id")]
# REMOVE TERMS THAT HAVE A NUMBER
# check if term with number is on the df
clean_df[clean_df$lemma == "rmsea=.046", c("lemma", "doc_id")]
# apply filter that removes terms that contain a digit.
clean_df <- clean_df %>%
filter(!str_detect(token, "[:digit:]"))
# check if filter went correctly
clean_df[clean_df$lemma == "rmsea=.046", c("lemma", "doc_id")]
# check number of unique words after removing words with digits
length(unique(clean_df$lemma))
# REMOVE DOTS FROM STRING VALUES IN LEMMA COLUMN
# check if the word "data." is in the file.
clean_df[clean_df$lemma == "data.", c("lemma", "doc_id")]
# Check number of observations before applying filter on terms. We want the number of observations to stay the same because we are not removing words, we are only removing part parts of the words.
nrow(clean_df)
# remove dots from string values in lemma column
clean_df$lemma <- gsub("\\.","",clean_df$lemma)
# check if filter went correctly
clean_df[clean_df$lemma == "data.", c("lemma", "doc_id")]
# check if filter went correctly
clean_df[clean_df$lemma == "data", c("lemma", "doc_id")]
# check number of unique words after removing dots. Should be the same as before.
length(unique(clean_df$lemma))
14691-14471
# check if number of observations is still the same
nrow(clean_df)
# also check unique number of tokens
length(unique(clean_df$token))
# SAVE INCLUDE_TOKEN
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc5<- df_include[, list(freq = .N), by = list(doc_id = doc_id, term = token)]
# SAVE INCLUDE_TOKEN
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc5<- clean_df[, list(freq = .N), by = list(doc_id = doc_id, term = token)]
# check number of unique words and documents.
length(unique(nounbydoc5$doc_id))
length(unique(nounbydoc5$term))
# create a data frame with one column including the unique terms
include_token <- unique(nounbydoc5$term)
include_token <- as.data.table(include_token)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
include_token <- as.data.table(include_token)
saveRDS(include_token, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_token.RData")
# SAVE INCLUDE_LEMMA
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc6<- clean_df[, list(freq = .N), by = list(doc_id = doc_id, term = lemma)]
# check number of unique words and documents.
length(unique(nounbydoc6$doc_id))
length(unique(nounbydoc6$term))
# create a data frame with one column including the unique terms
include_lemma <- unique(nounbydoc6$term)
include_lemma <- as.data.table(include_lemma)
saveRDS(include_lemma, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_lemma.RData")
worcs::git_update()
# load data frame with word vectors into object
df <- readRDS("featurematrix_asreview.RData")
# load data frame with column with selection of words to include in analysis
df_incltoken <- readRDS("include_token.RData")
df_incllemma <- readRDS("include_lemma.RData")
# create df in which only the words that we want to be included are kept
final_token <- subset(df, rownames(df) %in% df_incltoken$include_token)
final_lemma <- subset(df, rownames(df) %in% df_incllemma$include_lemma)
View(df_incltoken)
View(final_lemma)
View(final_token)
# check which words are in the included filter but are not in the feature matrix and are therefore excluded (unwanted).
excluded_token <- subset(final_token, !(rownames(final_token) %in% df_incltoken$include_token))
View(excluded_token)
# check which words are in the included filter but are not in the feature matrix and are therefore lost (unwanted).
lost_token <- subset(df_incltoken, !(include_token %in% rownames(final_token)))
15877-14010
lost_lemma <- subset(df_incllemma, !(include_lemma %in% rownames(final_lemma)))
View(lost_lemma)
View(lost_token)
14471-11814
# check which value is assigned to the word "and" in the upos column.
df_check[df_check$token == "and", c("token", "upos")]
# first I want to load in a file that still contains the upos column to see if the stopwords have been assignent NOUN or ADJ. On this file all filters have been applied until textrank algorith. So this is the file that has been used to create include_these2. On this file we want to apply more filters (mentioned above) before saving it as include_these3.
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check which value is assigned to the word "and" in the upos column.
df_check[df_check$token == "and", c("token", "upos")]
View(df_check)
# check which value is assigned to the word "and" in the upos column.
df_check[df_check$lemma == "and", c("token", "upos")]
# check which value is assigned to the word "and" in the upos column.
df_check[df_check$lemma == "the", c("token", "upos")]
# check which value is assigned to the word "and" in the upos column.
df_check[df_check$lemma == "in", c("token", "upos")]
# check which value is assigned to the word "and" in the upos column.
df_check[df_check$token == "in", c("token", "lemma", "upos")]
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# check number of unique words before applying more filters
length(unique(df_check$token))
clean_df <- df_check %>%
anti_join(stop_words, by= c("token" = "word"))
# check number of unique words after removing stop words
length(unique(clean_df$token))
16499 - 16198
# double check if the word "in" is removed from the file
clean_df[clean_df$token == "in", c("token", "lemma", "doc_id")]
# check which value is assigned to the word "in" in the upos column. For some reason the column token does not contain the word "and"
df_check[df_check$token == "in", c("token", "lemma", "upos")]
# REMOVE TERMS THAT HAVE A NUMBER
# check if term with number is on the df
clean_df[clean_df$token == "rmsea=.046", c("token", "lemma", "doc_id")]
# first I want to load in a file that still contains the upos column to see if the stopwords have been assignent NOUN or ADJ. On this file all filters have been applied until textrank algorith. So this is the file that has been used to create include_these2. On this file we want to apply more filters (mentioned above) before saving it as include_these3.
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check which value is assigned to the word "and" in the upos column.
df_check[df_check$lemma == "and", c("lemma", "upos")]
# check number of unique words before applying more filters
length(unique(df_check$lemma))
clean_df <- df_check %>%
anti_join(stop_words, by= c("lemma" = "word"))
# check number of unique words after removing stop words
length(unique(clean_df$lemma))
# double check if the word "and" is removed from the file
clean_df[clean_df$lemma == "and", c("lemma", "doc_id")]
# REMOVE TERMS THAT HAVE A NUMBER
# check if term with number is on the df
clean_df[clean_df$lemma == "rmsea=.046", c("lemma", "doc_id")]
# apply filter that removes terms that contain a digit.
clean_df <- clean_df %>%
filter(!str_detect(lemma, "[:digit:]"))
# check if filter went correctly
clean_df[clean_df$lemma == "rmsea=.046", c("lemma", "doc_id")]
# check number of unique words after removing words with digits
length(unique(clean_df$lemma))
# REMOVE DOTS FROM STRING VALUES IN LEMMA COLUMN
# check if the word "data." is in the file.
clean_df[clean_df$lemma == "data.", c("lemma", "doc_id")]
# Check number of observations before applying filter on terms. We want the number of observations to stay the same because we are not removing words, we are only removing part parts of the words.
nrow(clean_df)
# remove dots from string values in lemma column
clean_df$lemma <- gsub("\\.","",clean_df$lemma)
# check if filter went correctly
clean_df[clean_df$lemma == "data.", c("lemma", "doc_id")]
# check if filter went correctly
clean_df[clean_df$lemma == "data", c("lemma", "doc_id")]
# check number of unique words after removing dots. Should be the same as before.
length(unique(clean_df$lemma))
# also check unique number of tokens
length(unique(clean_df$token))
# check if number of observations is still the same
nrow(clean_df)
# check which value is assigned to the word "in" in the upos column. For some reason the column token does not contain the word "and"
df_check[df_check$token == "in", c("token", "lemma", "upos")]
# check number of unique words before applying more filters
length(unique(df_check$token))
clean_df <- clean_df %>%
anti_join(stop_words, by= c("token" = "word"))
# check number of unique words after removing stop words
length(unique(clean_df$token))
# check number of unique words before applying more filters
length(unique(df_clean$token))
# check number of unique words before applying more filters
length(unique(clean_df$token))
clean_df <- clean_df %>%
anti_join(stop_words, by= c("token" = "word"))
# check which value is assigned to the word "in" in the upos column. For some reason the column token does not contain the word "and"
df_clean[df_clean$token == "in", c("token", "lemma", "upos")]
# check which value is assigned to the word "in" in the upos column. For some reason the column token does not contain the word "and"
clean_df[clean_df$token == "in", c("token", "lemma", "upos")]
# REMOVE TERMS THAT HAVE A NUMBER
# check if term with number is in the df
clean_df[clean_df$token == "rmsea=.046", c("token", "lemma", "doc_id")]
# REMOVE DOTS FROM STRING VALUES IN LEMMA COLUMN
# check if the word "data." is in the file.
clean_df[clean_df$token == "data.", c("lemma", "doc_id")]
# REMOVE DOTS FROM STRING VALUES IN LEMMA COLUMN
# check if the word "data." is in the file.
clean_df[clean_df$token == "data.", c("token", "lemma", "doc_id")]
# Check number of observations before applying filter on terms. We want the number of observations to stay the same because we are not removing words, we are only removing part parts of the words.
nrow(clean_df)
# check if filter went correctly
clean_df[clean_df$token == "data.", c("toke", "lemma", "doc_id")]
# remove dots from string values in lemma column
clean_df$token <- gsub("\\.","",clean_df$token)
# check if filter went correctly
clean_df[clean_df$token == "data.", c("token", "lemma", "doc_id")]
# check if filter went correctly
clean_df[clean_df$token == "data", c("token" "lemma", "doc_id")]
# check if filter went correctly
clean_df[clean_df$token == "data", c("token", "lemma", "doc_id")]
# check number of unique words after removing dots. Should be the same as before.
length(unique(clean_df$token))
# also check unique number of tokens
length(unique(clean_df$token))
# check if number of observations is still the same
nrow(clean_df)
source('~/.active-rstudio-document', echo=TRUE)
# first I want to load in a file that still contains the upos column to see if the stopwords have been assignent NOUN or ADJ. On this file all filters have been applied until textrank algorith. So this is the file that has been used to create include_these2. On this file we want to apply more filters (mentioned above) before saving it as include_these3.
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check which value is assigned to the word "and" in the upos column.
df_check[df_check$lemma == "and", c("lemma", "upos")]
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
clean_df <- df_check %>%
anti_join(stop_words, by= c("lemma" = "word"))
# check number of unique words after removing stop words
length(unique(clean_df$lemma))
# double check if the word "and" is removed from the file
clean_df[clean_df$lemma == "and", c("token", "lemma", "doc_id")]
# REMOVE TERMS THAT HAVE A NUMBER
# check if term with number is on the df
clean_df[clean_df$lemma == "rmsea=.046", c("token", "lemma", "doc_id")]
# apply filter that removes terms that contain a digit.
clean_df <- clean_df %>%
filter(!str_detect(lemma, "[:digit:]"))
# check if filter went correctly
clean_df[clean_df$lemma == "rmsea=.046", c("token", "lemma", "doc_id")]
# check number of unique words after removing words with digits
length(unique(clean_df$lemma))
# REMOVE DOTS FROM STRING VALUES IN LEMMA COLUMN
# check if the word "data." is in the file.
clean_df[clean_df$lemma == "data.", c("token", "lemma", "doc_id")]
# Check number of observations before applying filter on terms. We want the number of observations to stay the same because we are not removing words, we are only removing part parts of the words.
nrow(clean_df)
# remove dots from string values in lemma column
clean_df$lemma <- gsub("\\.","",clean_df$lemma)
# remove dots from string values in token column
clean_df$token <- gsub("\\.","",clean_df$token)
# check if filter went correctly
clean_df[clean_df$lemma == "data.", c("token", "lemma", "doc_id")]
# check if filter went correctly
clean_df[clean_df$lemma == "data", c("token", "lemma", "doc_id")]
# check number of unique words after removing dots. Should be the same as before.
length(unique(clean_df$lemma))
# also check unique number of tokens
length(unique(clean_df$token))
# check if number of observations is still the same
nrow(clean_df)
# SAVE INCLUDE_TOKEN
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc5<- clean_df[, list(freq = .N), by = list(doc_id = doc_id, term = token)]
# check number of unique words and documents.
length(unique(nounbydoc5$doc_id))
length(unique(nounbydoc5$term))
# create a data frame with one column including the unique terms
include_token <- unique(nounbydoc5$term)
include_token <- as.data.table(include_token)
saveRDS(include_token, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_token.RData")
# SAVE INCLUDE_LEMMA
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc6<- clean_df[, list(freq = .N), by = list(doc_id = doc_id, term = lemma)]
# check number of unique words and documents.
length(unique(nounbydoc6$doc_id))
length(unique(nounbydoc6$term))
# create a data frame with one column including the unique terms
include_lemma <- unique(nounbydoc6$term)
include_lemma <- as.data.table(include_lemma)
saveRDS(include_lemma, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_lemma.RData")
# load data frame with word vectors into object
df <- readRDS("featurematrix_asreview.RData")
# load data frame with column with selection of words to include in analysis
df_incltoken <- readRDS("include_token.RData")
df_incllemma <- readRDS("include_lemma.RData")
# create df in which only the words that we want to be included are kept
final_token <- subset(df, rownames(df) %in% df_incltoken$include_token)
final_lemma <- subset(df, rownames(df) %in% df_incllemma$include_lemma)
# check which words are in the included filter but are not in the feature matrix and are therefore lost (unwanted).
lost_token <- subset(df_incltoken, !(include_token %in% rownames(final_token)))
lost_lemma <- subset(df_incllemma, !(include_lemma %in% rownames(final_lemma)))
15623-14071
View(lost_lemma)
View(lost_token)
# As expected least words are lost with the token_filter. Therefore for now I will keep working with final_token even though it might be that some words are more in there than need
final_lemma["smoke", ]
final_lemma["smoker", ]
final_lemma["smokes", ]
final_token["smoke", ]
final_token["smoker", ]
final_token["smokes", ]
final_token["smoking", ]
final_lemma["smoking", ]
df["non-anxious", ]
df["nonanxious", ]
final_lemma["absteiner", ]
final_token["absteiner", ]
final_lemma["aberration", ]
final_token["aberration", ]
final_lemma["aberration", ]
final_token["aberration", ]
row.names(final_token == "aberration")
# first I want to load in a file that still contains the upos column to see if the stopwords have been assignent NOUN or ADJ. On this file all filters have been applied until textrank algorith. So this is the file that has been used to create include_these2. On this file we want to apply more filters (mentioned above) before saving it as include_these3.
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check which value is assigned to the word "and" in the upos column.
df_check[df_check$lemma == "and", c("lemma", "upos")]
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
clean_df <- df_check %>%
anti_join(stop_words, by= c("lemma" = "word"))
# check number of unique words after removing stop words
length(unique(clean_df$lemma))
# double check if the word "and" is removed from the file
clean_df[clean_df$lemma == "and", c("token", "lemma", "doc_id")]
# REMOVE TERMS THAT HAVE A NUMBER
# check if term with number is on the df
clean_df[clean_df$lemma == "rmsea=.046", c("token", "lemma", "doc_id")]
# apply filter that removes terms that contain a digit.
clean_df <- clean_df %>%
filter(!str_detect(lemma, "[:digit:]"))
# check if filter went correctly
clean_df[clean_df$lemma == "rmsea=.046", c("token", "lemma", "doc_id")]
# check number of unique words after removing words with digits
length(unique(clean_df$lemma))
# REMOVE DOTS FROM STRING VALUES IN LEMMA AND TOKEN COLUMN
# check if the word "data." is in the file.
clean_df[clean_df$lemma == "data.", c("token", "lemma", "doc_id")]
# Check number of observations before applying filter on terms. We want the number of observations to stay the same because we are not removing words, we are only removing part parts of the words.
nrow(clean_df)
# remove dots from string values in lemma column
clean_df$lemma <- gsub("\\.","",clean_df$lemma)
# remove dots from string values in token column
clean_df$token <- gsub("\\.","",clean_df$token)
# check if filter went correctly
clean_df[clean_df$lemma == "data.", c("token", "lemma", "doc_id")]
# check if filter went correctly
clean_df[clean_df$lemma == "data", c("token", "lemma", "doc_id")]
# check number of unique words after removing dots. Should be the same as before.
length(unique(clean_df$lemma))
# also check unique number of tokens
length(unique(clean_df$token))
# check if number of observations is still the same
nrow(clean_df)
# REMOVE - FROM STRING VALUES IN LEMMA and TOKEN COLUMN
# check if the word "non-clinical" is in the file.
clean_df[clean_df$lemma == "non-clinical", c("token", "lemma", "doc_id")]
# Check number of observations before applying filter on terms. We want the number of observations to stay the same because we are not removing words, we are only removing part parts of the words.
nrow(clean_df)
# remove dots from string values in lemma column
clean_df$lemma <- gsub("\\-","",clean_df$lemma)
# remove dots from string values in token column
clean_df$token <- gsub("\\-","",clean_df$token)
# check if filter went correctly
clean_df[clean_df$lemma == "non-clinical", c("token", "lemma", "doc_id")]
# check if filter went correctly
clean_df[clean_df$lemma == "nonclinical", c("token", "lemma", "doc_id")]
# check number of unique words after removing -. Should be the same as before.
length(unique(clean_df$lemma))
15632-14195
# check number of unique words after removing dots. Should be the same as before.
length(unique(clean_df$lemma))
# also check unique number of tokens
length(unique(clean_df$token))
# check if number of observations is still the same
nrow(clean_df)
# first I want to load in a file that still contains the upos column to see if the stopwords have been assignent NOUN or ADJ. On this file all filters have been applied until textrank algorith. So this is the file that has been used to create include_these2. On this file we want to apply more filters (mentioned above) before saving it as include_these3.
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check which value is assigned to the word "and" in the upos column.
df_check[df_check$lemma == "and", c("lemma", "upos")]
# check number of unique words before applying more filters
length(unique(df_check$lemma))
clean_df <- df_check %>%
anti_join(stop_words, by= c("lemma" = "word"))
# check number of unique words after removing stop words
length(unique(clean_df$lemma))
# double check if the word "and" is removed from the file
clean_df[clean_df$lemma == "and", c("token", "lemma", "doc_id")]
# REMOVE TERMS THAT HAVE A NUMBER
# check if term with number is on the df
clean_df[clean_df$lemma == "rmsea=.046", c("token", "lemma", "doc_id")]
# apply filter that removes terms that contain a digit.
clean_df <- clean_df %>%
filter(!str_detect(lemma, "[:digit:]"))
# check if filter went correctly
clean_df[clean_df$lemma == "rmsea=.046", c("token", "lemma", "doc_id")]
# check number of unique words after removing words with digits
length(unique(clean_df$lemma))
# REMOVE DOTS FROM STRING VALUES IN LEMMA AND TOKEN COLUMN
# check if the word "data." is in the file.
clean_df[clean_df$lemma == "data.", c("token", "lemma", "doc_id")]
# Check number of observations before applying filter on terms. We want the number of observations to stay the same because we are not removing words, we are only removing part parts of the words.
nrow(clean_df)
# check number of unique tokens before applying the next filter
length(unique(clean_df$lemma))
# check number of unique tokens before applying the next filter
length(unique(clean_df$token))
# Check number of observations before applying filter on terms. We want the number of observations to stay the same because we are not removing words, we are only removing part parts of the words.
nrow(clean_df)
# remove dots from string values in lemma column
clean_df$lemma <- gsub("\\.","",clean_df$lemma)
# remove dots from string values in token column
clean_df$token <- gsub("\\.","",clean_df$token)
# check if filter went correctly
clean_df[clean_df$lemma == "data.", c("token", "lemma", "doc_id")]
# check if filter went correctly
clean_df[clean_df$lemma == "data", c("token", "lemma", "doc_id")]
# check number of unique words after removing dots. Should be the same as before.
length(unique(clean_df$lemma))
length(unique(clean_df$token))
15877 - 15623
# check if number of observations is still the same
nrow(clean_df)
# REMOVE - FROM STRING VALUES IN LEMMA and TOKEN COLUMN
# check if the word "non-clinical" is in the file.
clean_df[clean_df$lemma == "non-clinical", c("token", "lemma", "doc_id")]
# Check number of observations before applying filter on terms. We want the number of observations to stay the same because we are not removing words, we are only removing part parts of the words.
nrow(clean_df)
# remove - from string values in lemma column
clean_df$lemma <- gsub("\\-","",clean_df$lemma)
# remove - from string values in token column
clean_df$token <- gsub("\\-","",clean_df$token)
# check if filter went correctly
clean_df[clean_df$lemma == "non-clinical", c("token", "lemma", "doc_id")]
# check if filter went correctly
clean_df[clean_df$lemma == "nonclinical", c("token", "lemma", "doc_id")]
# check number of unique words after removing -. Should be the same as before.
length(unique(clean_df$lemma))
14195- 14471
# also check unique number of tokens
length(unique(clean_df$token))
15632 - 15343
# check if number of observations is still the same
nrow(clean_df)
# SAVE INCLUDE_TOKEN
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc5<- clean_df[, list(freq = .N), by = list(doc_id = doc_id, term = token)]
# check number of unique words and documents.
length(unique(nounbydoc5$doc_id))
length(unique(nounbydoc5$term))
# create a data frame with one column including the unique terms
include_token <- unique(nounbydoc5$term)
include_token <- as.data.table(include_token)
saveRDS(include_token, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_token.RData")
# SAVE INCLUDE_LEMMA
# create data frame with column doc_id term and add a term frequency per doc column column
nounbydoc6<- clean_df[, list(freq = .N), by = list(doc_id = doc_id, term = lemma)]
# check number of unique words and documents.
length(unique(nounbydoc6$doc_id))
length(unique(nounbydoc6$term))
# create a data frame with one column including the unique terms
include_lemma <- unique(nounbydoc6$term)
include_lemma <- as.data.table(include_lemma)
saveRDS(include_lemma, "/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/amy/include_lemma.RData")
# load data frame with word vectors into object
df <- readRDS("featurematrix_asreview.RData")
# load data frame with column with selection of words to include in analysis
df_incltoken <- readRDS("include_token.RData")
df_incllemma <- readRDS("include_lemma.RData")
# create df in which only the words that we want to be included are kept
final_token <- subset(df, rownames(df) %in% df_incltoken$include_token)
final_lemma <- subset(df, rownames(df) %in% df_incllemma$include_lemma)
# check which words are in the included filter but are not in the feature matrix and are therefore lost (unwanted).
lost_token <- subset(df_incltoken, !(include_token %in% rownames(final_token)))
15343-14088
lost_lemma <- subset(df_incllemma, !(include_lemma %in% rownames(final_lemma)))
14195-11834
View(final_token)
View(lost_lemma)
View(lost_token)
View(lost_token)
View(lost_lemma)
View(lost_token)
final_token["mindfulness", ]
final_token["background", ]
df["background", ]
df_incltoken[df_incltoken$include_token == "background", ]
df["healthy", ]
final_token["healthy", ]
final_token["conclusions", ]
worcs::git_update()
