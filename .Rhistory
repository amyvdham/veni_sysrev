yaml::write_yaml(number_docs_words, "study1_number_docs_words.txt")
# Exclude words
exclude_terms <- readLines("exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
df <- df[!exclude_these, ]
# Categorize words
res_cat <- cat_words(df$word, dict, handle_dups = "all")
# Check coding issues
#res_cat$dup
#head(res_cat$unmatched)
df <- merge_df(df, res_cat$words, "word_coded")
saveRDS(df, "study1_df.RData")
} else {
df <- readRDS("study1_df.RData")
number_docs_words <- yaml::read_yaml("study1_number_docs_words.txt")
}
# Frequency of word by doc
nounbydoc <- df[, list(freq = .N), by = list(doc_id = doc, term = word_coded)]
# Set frequency to 1; we're not interpreting word frequency, only occurrence
nounbydoc$freq <- 1
dtm <- udpipe::document_term_matrix(document_term_frequencies(nounbydoc))
term_freq <- table(colSums(dtm))
# Continue plotting word frequency ----------------------------------------
set.seed(5348)
dtm_top <- dtm[, select_words(dtm, .975)]
dtm_top <- dtm_top[rowSums(dtm_top) > 0, ]
if(run_everything){
term_freqtop <- table(colSums(dtm_top))
term_freq <- as.data.frame.table(term_freq)
term_freq$pruned <- term_freq$Var1 %in% names(term_freqtop)
write_yaml(term_freq, "study1_term_freq_dist.yml")
write_yaml(dim(dtm_top), "study1_dtm_top.yml")
# Wordcloud ---------------------------------------------------------------
## Word frequencies
topterms <- colSums(dtm_top)
baseline <- readRDS("baseline.RData")
word_freq <- data.frame(Word = names(topterms), Frequency = topterms, row.names = NULL)
write.csv(word_freq, "study1_word_freq.csv", row.names = FALSE)
df_plot <- word_freq
df_plot <- df_plot[order(df_plot$Frequency, decreasing = TRUE), ]
categ <- read.csv("study1_categorization.csv", stringsAsFactors = FALSE)
df_plot$cat <- categ$category[match(df_plot$Word, categ$name)]
df_plot$baseline <- as.character(df_plot$Word %in% baseline)
df_plot$faded <- df_plot$Word %in% baseline
# Tag words that are not in the cooccurrence graph
in_graph <- row.names(read.csv("s1_cooc.csv", row.names = 1))
notingraph <- !df_plot$Word %in% in_graph
df_plot$notingraph <- notingraph
italic_labels <- as.character(df_plot$Word)
italic_labels[notingraph] <- sapply(italic_labels[notingraph], function(x){
parse(text = paste0("italic('", x, "')"))
})
# Prettify words
df_plot$Word <- pretty_words(df_plot$Word)
df_plot$Word <- ordered(df_plot$Word, levels = df_plot$Word[order(df_plot$Frequency)])
cat_cols <- c(Outcome = "gray55", Indicator = "tomato", Cause = "darkgoldenrod3", Protective = "olivedrab4")
df_plot$cat <- ordered(df_plot$cat, levels = c("Outcome", "Indicator", "Cause", "Protective"))
write_yaml(df_plot$Word, "s1_words.yml")
p <- ggplot(df_plot, aes(y = Word, x = Frequency)) +
geom_segment(aes(x = 0, xend = Frequency,
y = Word, yend = Word, linetype = notingraph), colour = "grey50"
) +
geom_vline(xintercept = 0, colour = "grey50", linetype = 1) + xlab("Word frequency") +
geom_point(data = df_plot[df_plot$faded, ], aes(colour = cat, shape = cat), fill = "white", size = 1.5) +
geom_point(data = df_plot[!df_plot$faded, ], aes(colour = cat, fill = cat, shape = cat), size = 1.5) +
scale_colour_manual(values = c(Outcome = "gray55", Indicator = "tomato", Cause = "darkgoldenrod3", Protective = "olivedrab4"))+ #, guide = NULL
scale_shape_manual(values = scale_shapes)+#, guide = NULL
scale_fill_manual(values = c(Outcome = "gray55", Indicator = "tomato", Cause = "darkgoldenrod3", Protective = "olivedrab4")) +
scale_x_log10() +
#scale_y_discrete(labels = italic_labels) +
scale_linetype_manual(values = c("TRUE" = 3, "FALSE" = 1), guide = NULL) +
theme_bw() + theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(), axis.title.y = element_blank(),
legend.position = c(.70,.125),
legend.title = element_blank(),
axis.text.y = element_text(hjust=0, vjust = 0, size = 6))
saveRDS(p, "s1_varimp.RData")
svg("s1_varimp.svg", width = 7/2.54, height = 14/2.54)
eval(p)
dev.off()
ggsave("s1_varimp.png", p, device = "png", width = 7, height = 14, units = "cm")
p1 <- p + theme(legend.position = "none")
df_plot$Frequency <- sqrt(df_plot$Frequency)
## Visualise them with wordclouds
p <- quote({
set.seed(46)
wordcloud(words = df_plot$Word, freq = df_plot$Frequency, scale = c(2,.4), max.words = 150, rot.per = 0,  random.order = FALSE, colors = brewer.pal(8, "Dark2"))
})
svg("study1_wordcloud.svg")
eval(p)
dev.off()
png("study1_wordcloud.png")
eval(p)
dev.off()
}
View(word_freq)
View(term_freq)
View(nounbydoc)
worcs::git_update(message = "running manuscript word_freq up to date")
# Loading required libraries
library(stringr)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(Matrix)
library(ggplot2)
library(dplyr)
# Required library for GloVe
library(text2vec)
# Loading script containing functions
source("word_functions.R")
# prepare data
# this is were line 891 in manuscript file starts
recs <- read.csv("recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]
## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column.
df <- merge_df(recs, df, "word")
# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
# check the number of author keywords per doc
df_n_keywords <- df %>%
group_by(doc) %>%
summarise(n_keywords = n()) # snap nog niet helemaal hoe deze code weet dat die n() dan betrekking heeft op de column word maar gaat wel goed.
## NOTE: some keywords contain more than one word, e.g. risk factors or follow-up studies and youth smoking behavior. So to now the lenght (number of words) per doc this won't work
# Now that the dataset is cleaned created a df in which each document is a row and the column DEclean contains the author keywords of the specific document. Shape the tokens back to their original form
df_new <- df %>%
group_by(word) %>%
group_by(doc) %>%
summarise(DEclean = str_c(word, collapse = " "))
# second attempt word count
df_new$DEclean_length <- sapply(df_new$DEclean, function(x){str_count(x, " ")+1})
# this gives us the number of words in the column DEclean for each row. NOTE: words that are connected by a - are seen as one word, e.g. follow-up or affect-regulatory. Also, abbreviations are also counted as a word, e.g. INFORMATION AND COMMUNICATION TECHNOLOGY (ICT). (ICT) is counted in this example.
df_new <- data.table(df_new)
# Max length of author keywords
max(df_new$DEclean_length)
# investigate with the highest value of number of author keywords
df_new[df_new$doc == 3667, 'DEclean']
# check if this is a result of data cleaning by looking at the original data set
recs[recs$doc == 3667, 'DE']
# -> shows that this is not a result of data cleaning and it seems that this article does really have 524 author keywords.
# NOTE: document/case/row shown below contains < in author keywords. Does this need to be cleaned?
df_new[df_new$doc == 5234, 'DEclean']
View(df)
unique(df$word)
df[df$doc == 5234, 'word']
length(unique(df$word))
# We need to tokenize our already tokenized set as input for text2vec, re-use cleaned text in df_new
it <- itoken(df_new$DEclean,
tokenizer = word_tokenizer,
ids = df_new$doc,
progressbar = TRUE)
# create a vocabulary out of the tokenset (stopword removal and bi-grams are optional)
vocab <- create_vocabulary(it) # use uni-grams
# text2vec has the option to prune the vocabulary of low-frequent words
vocab <- prune_vocabulary(vocab, term_count_min = 5)
# What's in the vocabulary?
print(vocab)
# Vectorize word to integers
vectorizer <- vocab_vectorizer(vocab)
# Create a Term-Count-Matrix, by default it will use a skipgram window of 5 (symmetrical)
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
# maximum number of co-occurrences to use in the weighting function, we choose the entire token set divided by 100
x_max <- length(vocab$doc_count)/100
# set up the embedding matrix and fit model
glove_model <- GloVe$new(rank = 32, x_max = x_max)
glove_embedding = glove_model$fit_transform(tcm, n_iter = 20, convergence_tol = 0.01, n_threads = 4)
# combine main embedding and context embeddings (sum) into one matrix
glove_embedding = glove_embedding + t(glove_model$components) # the transpose of the context matrix
# wat ligt er dicht bij 'school'
word <- glove_embedding["school", , drop = FALSE]
cos_sim = sim2(x = glove_embedding, y = word, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# wat ligt er dicht bij 'personality'
word <- glove_embedding["personality", , drop = FALSE]
cos_sim = sim2(x = glove_embedding, y = word, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# NOTE: document/case/row shown below contains < in author keywords. Does this need to be cleaned? It also includes the word literature whereas this word was part of the terms that had to be excluded.
df_new[df_new$doc == 5234, 'DEclean']
worcs::git_update()
# Loading required libraries
library(stringr)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(udpipe)
library(igraph)
library(wordcloud)
library(Matrix)
library(ggplot2)
library(yaml)
# makes it possible to call functions that are saved in separate R script
source("word_functions.R")
## Look at POS tags?
# reads file which contains the records into an object called recs
# this is were line 891 in manuscript file starts
recs <- read.csv("recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# if recs is not an object of type data table
# then code execution will be paused?
if(!is.data.table(recs)){
browser()
}
# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]
# create object that is a list with the dimensions of the recs data frame
study1details <- list(dim_recs = dim(recs))
View(study1details)
## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column.
df <- merge_df(recs, df, "word")
# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
# create an object with the number of unique documents (articles) in the data frame (df) and the number of unique (author key-) words in the data frame.
number_docs_words <- c(docs = length(unique(df$doc)), words =
length(unique(df$word)))
# save this information(# of articles and # of unique author keywords) in a yaml file
yaml::write_yaml(number_docs_words, "study1_number_docs_words.txt")
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
View(df)
unique(df$doc)
max(unique(df$doc))
length(unique(df$doc))
# makes it possible to call functions that are saved in separate R script
source("word_functions.R")
## Look at POS tags?
# reads file which contains the records into an object called recs
# this is were line 891 in manuscript file starts
recs <- read.csv("recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# if recs is not an object of type data table
# then code execution will be paused?
if(!is.data.table(recs)){
browser()
}
# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]
# create object that is a list with the dimensions of the recs data frame
study1details <- list(dim_recs = dim(recs))
## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column.
df <- merge_df(recs, df, "word")
# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
# create an object with the number of unique documents (articles) in the data frame (df) and the number of unique (author key-) words in the data frame.
number_docs_words <- c(docs = length(unique(df$doc)), words =
length(unique(df$word)))
# save this information(# of articles and # of unique author keywords) in a yaml file
yaml::write_yaml(number_docs_words, "study1_number_docs_words.txt")
# check how many unique documents and words there are before excluding terms.
length(unique(df$doc))
length(unique(df$word))
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
# check how many unique documents and how many unique words there are after excluding non substantive words.
length(unique(df$doc))
length(unique(df$word))
worcs::git_update()
# Loading required libraries
library(stringr)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(Matrix)
library(ggplot2)
library(dplyr)
# Required library for GloVe
library(text2vec)
# Loading script containing functions
source("word_functions.R")
# prepare data
# this is were line 891 in manuscript file starts
recs <- read.csv("recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]
## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column.
df <- merge_df(recs, df, "word")
# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
# check the number of author keywords per doc
df_n_keywords <- df %>%
group_by(doc) %>%
summarise(n_keywords = n()) # snap nog niet helemaal hoe deze code weet dat die n() dan betrekking heeft op de column word maar gaat wel goed.
# Now that the dataset is cleaned created a df in which each document is a row and the column DEclean contains the author keywords of the specific document. Shape the tokens back to their original form
df_new <- df %>%
group_by(word) %>%
group_by(doc) %>%
summarise(DEclean = str_c(word, collapse = " "))
# second attempt word count
df_new$DEclean_length <- sapply(df_new$DEclean, function(x){str_count(x, " ")+1})
df_new <- data.table(df_new)
# Max length of author keywords
max(df_new$DEclean_length)
# investigate with the highest value of number of author keywords
df_new[df_new$doc == 3667, 'DEclean']
# check if this is a result of data cleaning by looking at the original data set
recs[recs$doc == 3667, 'DE']
# NOTE: document/case/row shown below contains < in author keywords. Does this need to be cleaned? It also includes the word literature whereas this word was part of the terms that had to be excluded.
df_new[df_new$doc == 5234, 'DEclean']
# check why those words with literature were not excluded
df[df$doc == 5234, 'word']
# check how many unique documents and how many unique words there are after excluding non substantive words.
length(unique(df_new$doc))
length(unique(df_new$word))
length(unique(df$word))
# check how many unique documents and how many unique words there are after excluding non substantive words.
length(unique(df$doc))
## count the number of words per review and plot results
df_info <- df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(10,300,10),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_reviews)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_reviews), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df_info <- df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(10,300,10),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_reviews), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df_info <- df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(10,70,10),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_reviews), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df_info <- df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(10,70,10),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_reviews), position=position_dodge(width=0.9), vjust=-0.25)
View(df_info)
View(df_info)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(10,70,10),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_reviews), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(10,70,10),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(5,10,20),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(2,5,20),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(2,5,2),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(2,15,2),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(1,15,1),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
## count the number of words per review and plot results
df %>%
group_by(doc) %>% summarise(n_tokens = n()) %>%
mutate(n_tokens_binned = cut(n_tokens, breaks = c(0,seq(1,11,1),Inf))) %>%
group_by(n_tokens_binned) %>% summarise(n_articles = n()) %>%
## pass result to ggplot
ggplot(aes(x=n_tokens_binned,y=n_articles)) +
geom_bar(stat='identity',fill='green') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
geom_text(size=2, aes(label=n_articles), position=position_dodge(width=0.9), vjust=-0.25)
# We need to tokenize our already tokenized set as input for text2vec, re-use cleaned text in df_new
it <- itoken(df_new$DEclean,
tokenizer = word_tokenizer,
ids = df_new$doc,
progressbar = TRUE)
# create a vocabulary out of the tokenset (stopword removal and bi-grams are optional)
vocab <- create_vocabulary(it) # use uni-grams
# text2vec has the option to prune the vocabulary of low-frequent words
vocab <- prune_vocabulary(vocab, term_count_min = 5)
# What's in the vocabulary?
print(vocab)
# check word frequency
df_info <- df %>%
group_by(word) %>% summarize(word_freq=n()) %>%
mutate(min_5_freq = case_when(word_freq<5~'token frequency: <5',TRUE~'token frequency: >=5')) %>%
group_by(min_5_freq) %>% summarise(n_tokens = n()) %>% mutate(pct_tokens = n_tokens / sum(n_tokens))
View(df_info)
View(df_info)
# check word frequency: data_info contains information on how many of the articles contain more and less than 5 author keywords.
df %>%
group_by(word) %>% summarize(word_freq=n()) %>%
mutate(min_5_freq = case_when(word_freq<5~'token frequency: <5',TRUE~'token frequency: >=5')) %>%
group_by(min_5_freq) %>% summarise(n_tokens = n()) %>% mutate(pct_tokens = n_tokens / sum(n_tokens))
worcs::git_update()
