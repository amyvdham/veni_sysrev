geom_vline(xintercept = 4, linetype = 2)
df <- as.data.frame(embedding_cluster33)
fviz_nbclust(x = df, kmeans, method = "wss") +
geom_vline(xintercept = 4, linetype = 2)
fviz_nbclust(x = df, kmeans(), method = "wss") +
geom_vline(xintercept = 4, linetype = 2)
nclust <- fviz_nbclust(x = df, kmeans(), method = "wss")
# fit the k-means clustering with 100 clusters
k_means_fit <- kmeans(glove_embedding, 100, iter.max = 30, nstart = 25)
# Create data frame in which the merge cluster assignment back to rows/word.
kw_with_cluster <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit$cluster))
# add column names
names(kw_with_cluster) <- c("word", "kmeans100")
# Mother
# find out in which cluster the word mother is assigned
kw_with_cluster[kw_with_cluster$word == "mother", ]
cluster29 <- subset(kw_with_cluster, subset=kmeans100 == 29)
View(cluster29)
# fit the k-means clustering with 100 clusters
k_means_fit <- kmeans(glove_embedding, 100, iter.max = 30)
# try to make a plot
clusplot(glove_embedding,
kc$cluster,
color = TRUE,
shade = TRUE,
labels = 2,
lines = 0)
# try to make a plot
clusplot(glove_embedding,
k_means_fit$cluster,
color = TRUE,
shade = TRUE,
labels = 2,
lines = 0)
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i, iter.max = 30)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i, iter.max = 30)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# load libraries
library(dplyr)
library(cluster)
library(ggplot2)
library(devtools)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
set.seed(88)
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
# load data frame with column with selection of words to include in analysis. For now I will only
df_incllemma <- readRDS("include_lemma.RData")
# create df in which only the words that we want to be included are kept
lemma_embedding <- subset(vectors_glove, word %in% df_incllemma$include_lemma)
# check number of unique words
length(unique((lemma_embedding$word)))
# non-GloVe: check which words are in the included filter but are not in the feature matrix and are therefore lost (unwanted).
lost_lemma <- subset(df_incllemma, !(include_lemma %in% lemma_embedding $word))
# convert first column, word, to row index
library(tidyverse)
glove_embedding <- lemma_embedding %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
glove_embedding <- as.matrix(glove_embedding)
str(glove_embedding)
##  Determine the value of K
library(factoextra)
# try to determine number of cluster with NbClust package
library(NbClust)
# Another pacakge for determine number of clusters
library(ClusterR)
# Silhouette method
fviz_nbclust(glove_embedding, kmeans, method = "silhouette", k.max = 150) +
labs(subtitle = "Silhouette method")
?xlim()
# Silhouette method
silplot <- fviz_nbclust(glove_embedding, kmeans, method = "silhouette", k.max = 150) +
labs(subtitle = "Silhouette method")
warnimng()
warning()
silplot +
xlim(0,200)
silplot
# Elbow method
fviz_nbclust(glove_embedding, kmeans, method = "wss") + labs(subtitle = "Elbow method") # add subtitle
# try to determine number of cluster with NbClust package
library(NbClust)
nbclust_out <- NbClust(
data = embedding_cluster33,
distance = "euclidean",
min.nc = 2, # minimum number of clusters
max.nc = 200, # maximum number of clusters
method = "kmeans", # one of: "ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid", "kmeans"
index = "silhouette"
)
nbclust_out <- NbClust(
data = glove_embedding,
distance = "euclidean",
min.nc = 2, # minimum number of clusters
max.nc = 200, # maximum number of clusters
method = "kmeans", # one of: "ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid", "kmeans"
index = "silhouette"
)
warnings()
?NbClust()
# Elbow method
fviz_nbclust(glove_embedding, kmeans, method = "wss", k.max = 150) + labs(subtitle = "Elbow method") # add subtitle
warnings()
?fviz_nbclust
worcs::git_update()
# load libraries
library(dplyr)
library(cluster)
library(ggplot2)
library(devtools)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
set.seed(88)
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
# load data frame with column with selection of words to include in analysis. For now I will only
df_incllemma <- readRDS("include_lemma.RData")
# create df in which only the words that we want to be included are kept
lemma_embedding <- subset(vectors_glove, word %in% df_incllemma$include_lemma)
# check number of unique words
length(unique((lemma_embedding$word)))
# non-GloVe: check which words are in the included filter but are not in the feature matrix and are therefore lost (unwanted).
lost_lemma <- subset(df_incllemma, !(include_lemma %in% lemma_embedding $word))
# 2768 words lost
length(unique((lemma_embedding$word))) + length(unique((lost_lemma$include_lemma)))
# ->  14195: dit is gelijk aan:
length(unique((df_incllemma$include_lemma)))
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
# load data frame with column with selection of words to include in analysis. For now I will only
df_incllemma <- readRDS("include_lemma.RData")
# create df in which only the words that we want to be included are kept
lemma_embedding <- subset(vectors_glove, word %in% df_incllemma$include_lemma)
# check number of unique words
length(unique((lemma_embedding$word)))
# non-GloVe: check which words are in the included filter but are not in the feature matrix and are therefore lost (unwanted).
lost_lemma <- subset(df_incllemma, !(include_lemma %in% lemma_embedding $word))
# 2768 words lost
length(unique((lemma_embedding$word))) + length(unique((lost_lemma$include_lemma)))
# ->  14195: dit is gelijk aan:
length(unique((df_incllemma$include_lemma)))
# convert first column, word, to row index
library(tidyverse)
glove_embedding <- lemma_embedding %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
glove_embedding <- as.matrix(glove_embedding)
str(glove_embedding)
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i, iter.max = 30)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
wssplot(glove_embedding, nc=150)
50/20
3*2,5
3*2.5
7.5+125
# fit the k-means clustering with 133 clusters
k_means_fit133 <- kmeans(glove_embedding, 133, iter.max = 30, nstart = 25)
# obtain the centroids
k_means_fit133$centers
# look at the size of the clusters
k_means_fit133$size
# find the cluster to which each word belongs
k_means_fit133$cluster
# The cost function in kmeans is the total sum of the squares
k_means_fit133$totss
# check results
k_means_fit133
# fit the k-means clustering with 100 clusters
k_means_fit <- kmeans(glove_embedding, 100, iter.max = 30, nstart = 25)
# obtain the centroids
k_means_fit$centers
# look at the size of the clusters
k_means_fit$size
# look at the size of the clusters
min(k_means_fit$size)
min(k_means_fit$size)
max(k_means_fit$size)
# find the cluster to which each word belongs
k_means_fit$cluster
# The cost function in kmeans is the total sum of the squares
k_means_fit$totss
# results
k_means_fit
# Create data frame in which the merge cluster assignment back to rows/word.
kw_with_cluster <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit$cluster))
# add column names
names(kw_with_cluster) <- c("word", "kmeans100")
# make a df for the first 5 cluster results, quickly "eyeball" results
cluster1 <- subset(kw_with_cluster, subset=kmeans100 == 1)
cluster2 <- subset(kw_with_cluster, subset=kmeans100 == 2)
cluster3 <- subset(kw_with_cluster, subset=kmeans100 == 3)
cluster4 <- subset(kw_with_cluster, subset=kmeans100 == 4)
cluster5 <- subset(kw_with_cluster, subset=kmeans100 == 5)
# Mother
# find out in which cluster the word mother is assigned
kw_with_cluster[kw_with_cluster$word == "mother", ]
# make a df of cluster 25
cluster25 <- subset(kw_with_cluster, subset = kmeans100 == 25)
# Health
# find out in which cluster the word health is assigned
kw_with_cluster[kw_with_cluster$word == "health", ]
# make a df of cluster 27
cluster27 <- subset(kw_with_cluster, subset=kmeans100 == 27)
# Environment
# find out in which cluster the word environment is assigned
kw_with_cluster[kw_with_cluster$word == "environment", ]
# make a df of cluster 75
cluster75 <- subset(kw_with_cluster, subset=kmeans100 == 75)
# Depression
# find out in which cluster the word depression is assigned so that I can check if all the forms of depression are in there.
kw_with_cluster[kw_with_cluster$word == "depression", ]
# make a df of cluster 22
cluster22 <- subset(kw_with_cluster, subset=kmeans100 == 22)
View(cluster22)
# -> only the word depression is in this cluster. Might be that these are the only depression words in the embedding need to check this. -> If I look at the lemma_embedding dataframe and type depre in the filter within the word column I get the following words: nondepressed, antidepressant depressed, depression, depressive. You would expect some of these words to fall within the same cluster.
kw_with_cluster[kw_with_cluster$word == "depressed", ]
kw_with_cluster[kw_with_cluster$word == "depressive", ]
kw_with_cluster[kw_with_cluster$word == "nondepressed", ]
kw_with_cluster[kw_with_cluster$word == "antidepressant", ]
# Kelly
# find out in which cluster the word Kelly is assigned so that I can check if all names are put together for example.
kw_with_cluster[kw_with_cluster$word == "kelly", ]
# make a df of cluster 47
cluster47 <- subset(kw_with_cluster, subset=kmeans100 == 47)
View(cluster47)
# cortisol
# find out in which cluster the word cortisol is assigned so that I can check if all names are put together for example.
kw_with_cluster[kw_with_cluster$word == "cortisol", ]
# make a df of cluster 87
cluster87 <- subset(kw_with_cluster, subset=kmeans100 == 87)
View(cluster87)
# empathy
# find out in which cluster the word empathy is assigned so that I can check if all names are put together for example.
kw_with_cluster[kw_with_cluster$word == "empathy", ]
# make a df of cluster 6
cluster6 <- subset(kw_with_cluster, subset = kmeans100 == 6)
View(cluster6)
# obtain the centroids
k_means_fit133$centers
# look at the size of the clusters
k_means_fit133$size
min(k_means_fit133$size)
max(k_means_fit133$size)
# find the cluster to which each word belongs
k_means_fit133$cluster
# The cost function in kmeans is the total sum of the squares
k_means_fit133$totss
# check results
k_means_fit133
k_means_fit
# Create data frame in which the merge cluster assignment back to rows/word.
words_with_cluster133 <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit133$cluster))
# add column names
names(words_with_cluster133) <- c("word", "kmeans133")
# check the cluster with only 3 and 6 observations
cluster6 <- subset(kw_with_cluster, subset=kmeans133 == 6)
# check the cluster with only 3 and 6 observations
cluster133_6 <- subset(words_with_cluster133,
subset=kmeans133 == 6)
cluster133_71 <- subset(words_with_cluster133,
subset=kmeans133 == 71)
View(cluster133_6)
View(cluster133_71)
find_similar_words("health",glove_embedding,25)
library(text2vec)
find_similar_words <- function(word, embedding_matrix, n = 5) {
similarities <- embedding_matrix[word, , drop = FALSE] %>%
sim2(embedding_matrix, y = ., method = "cosine")
similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}
find_similar_words("health",glove_embedding,25)
View(cluster27)
# Health
# find out in which cluster the word health is assigned
words_with_cluster133[words_with_cluster133$word == "health", ]
# make a df of cluster 27
cluster133_124 <- subset(words_with_cluster133,
subset=kmeans133 == 124)
View(cluster133_124)
# Depression
# find out in which cluster the word depression is assigned
words_with_cluster133[words_with_cluster133$word == "depression", ]
# make a df of cluster 2
cluster133_2 <- subset(words_with_cluster133,
subset=kmeans133 == 2)
View(cluster133_2)
find_similar_words("health", depression,25)
find_similar_words("depression",glove_embedding,25)
# load libraries
library(dplyr)
library(cluster)
library(ggplot2)
library(devtools)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
set.seed(88)
# load existing word embeddings
# load glove vectors into R
vectors_glove <- data.table::fread('glove.840B.300d.txt', data.table = F,  encoding = 'UTF-8', quote="")
# rename the columns
colnames(vectors_glove) <- c('word',paste('dim',1:300,sep = '_'))
# load data frame with column with selection of words to include in analysis. For now I will only
df_incllemma <- readRDS("include_lemma.RData")
# create df in which only the words that we want to be included are kept
lemma_embedding <- subset(vectors_glove, word %in% df_incllemma$include_lemma)
# check number of unique words
length(unique((lemma_embedding$word)))
# non-GloVe: check which words are in the included filter but are not in the feature matrix and are therefore lost (unwanted).
lost_lemma <- subset(df_incllemma, !(include_lemma %in% lemma_embedding $word))
length(unique((lemma_embedding$word))) + length(unique((lost_lemma$include_lemma)))
# ->  14195: dit is gelijk aan:
length(unique((df_incllemma$include_lemma)))
# convert first column, word, to row index
library(tidyverse)
glove_embedding <- lemma_embedding %>%
remove_rownames() %>%
column_to_rownames(var = 'word')
# convert dataframe to a matrix
glove_embedding <- as.matrix(glove_embedding)
str(glove_embedding)
library(text2vec)
find_similar_words <- function(word, embedding_matrix, n = 5) {
similarities <- embedding_matrix[word, , drop = FALSE] %>%
sim2(embedding_matrix, y = ., method = "cosine")
similarities[, 1] %>% sort(decreasing = TRUE) %>% head(n)
}
find_similar_words("health",glove_embedding,25)
# fit the k-means clustering with 100 clusters
k_means_fit <- kmeans(glove_embedding, 100, iter.max = 30, nstart = 25)
# obtain the centroids
k_means_fit$centers
# look at the size of the clusters
k_means_fit$size
min(k_means_fit$size)
max(k_means_fit$size)
# find the cluster to which each word belongs
k_means_fit$cluster
# The cost function in kmeans is the total sum of the squares
k_means_fit$totss
# results
k_means_fit
# Create data frame in which the merge cluster assignment back to rows/word.
kw_with_cluster <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit$cluster))
# add column names
names(kw_with_cluster) <- c("word", "kmeans100")
# make a df for the first 5 cluster results, quickly "eyeball" results
cluster1 <- subset(kw_with_cluster, subset=kmeans100 == 1)
cluster2 <- subset(kw_with_cluster, subset=kmeans100 == 2)
cluster3 <- subset(kw_with_cluster, subset=kmeans100 == 3)
cluster4 <- subset(kw_with_cluster, subset=kmeans100 == 4)
cluster5 <- subset(kw_with_cluster, subset=kmeans100 == 5)
# Mother
# find out in which cluster the word mother is assigned
kw_with_cluster[kw_with_cluster$word == "mother", ]
# make a df of cluster 25
cluster25 <- subset(kw_with_cluster, subset = kmeans100 == 25)
# I did not setseeded the cluster analysis yet so want to compare the difference between the two clusters that contain the word mother. -> Can not rerun this code now that I have set.seeded the analysis but it is important to be aware that every time you run the analysis and do not set.seed, the clusters will be different. The 3 times I have now run the analysis the cluster containing mother had either 71, 70 or 86 observations.
diff_setseed <- subset(cluster6, !(word %in% cluster26$word))
# Health
# find out in which cluster the word health is assigned
kw_with_cluster[kw_with_cluster$word == "health", ]
# make a df of cluster 74
cluster74 <- subset(kw_with_cluster, subset=kmeans100 == 74)
View(cluster74)
# Environment
# find out in which cluster the word environment is assigned
kw_with_cluster[kw_with_cluster$word == "environment", ]
# make a df of cluster 66
cluster66 <- subset(kw_with_cluster, subset=kmeans100 == 66)
# Depression
# find out in which cluster the word depression is assigned so that I can check if all the forms of depression are in there.
kw_with_cluster[kw_with_cluster$word == "depression", ]
# make a df of cluster 8
cluster8 <- subset(kw_with_cluster, subset=kmeans100 == 8)
View(cluster8)
# -> only the word depression is in this cluster. Might be that these are the only depression words in the embedding need to check this. -> If I look at the lemma_embedding dataframe and type depre in the filter within the word column I get the following words: nondepressed, antidepressant depressed, depression, depressive. You would expect some of these words to fall within the same cluster.
kw_with_cluster[kw_with_cluster$word == "depressed", ]
kw_with_cluster[kw_with_cluster$word == "depressive", ]
kw_with_cluster[kw_with_cluster$word == "nondepressed", ]
kw_with_cluster[kw_with_cluster$word == "antidepressant", ]
# Kelly
# find out in which cluster the word Kelly is assigned so that I can check if all names are put together for example.
kw_with_cluster[kw_with_cluster$word == "kelly", ]
# make a df of cluster 87
cluster87 <- subset(kw_with_cluster, subset=kmeans100 == 87)
View(cluster87)
# cortisol
# find out in which cluster the word cortisol is assigned so that I can check if all names are put together for example.
kw_with_cluster[kw_with_cluster$word == "cortisol", ]
# make a df of cluster 56
cluster56 <- subset(kw_with_cluster, subset=kmeans100 == 56)
# empathy
# find out in which cluster the word empathy is assigned so that I can check if all names are put together for example.
kw_with_cluster[kw_with_cluster$word == "empathy", ]
# make a df of cluster 71
cluster71 <- subset(kw_with_cluster, subset = kmeans100 == 71)
find_similar_words("empathy",glove_embedding,25)
View(cluster71)
# fit the k-means clustering with 133 clusters
k_means_fit133 <- kmeans(glove_embedding, 133, iter.max = 30, nstart = 25)
# obtain the centroids
k_means_fit133$centers
# look at the size of the clusters
k_means_fit133$size
min(k_means_fit133$size)
max(k_means_fit133$size)
# The cost function in kmeans is the total sum of the squares
k_means_fit133$totss
# check results
k_means_fit133
# Create data frame in which the merge cluster assignment back to rows/word.
words_with_cluster133 <- as.data.frame(cbind(row.names(glove_embedding), k_means_fit133$cluster))
# add column names
names(words_with_cluster133) <- c("word", "kmeans133")
# check the cluster with only 3 and 6 observations
cluster133_32 <- subset(words_with_cluster133,
subset=kmeans133 == 32)
View(cluster133_32)
# Health
# find out in which cluster the word health is assigned
words_with_cluster133[words_with_cluster133$word == "health", ]
# make a df of cluster 50
cluster133_50 <- subset(words_with_cluster133,
subset=kmeans133 == 50)
View(cluster133_50)
# Depression
# find out in which cluster the word depression is assigned
words_with_cluster133[words_with_cluster133$word == "depression", ]
# make a df of cluster 2
cluster133_108 <- subset(words_with_cluster133,
subset=kmeans133 == 108)
View(cluster133_108)
max(k_means_fit133$size)
# look at the size of the clusters
k_means_fit133$size
# check the largest cluster with 216 observations
cluster133_76 <- subset(words_with_cluster133,
subset=kmeans133 == 76)
View(cluster133_76)
worcs::git_update()
View(kw_with_cluster)
View(lost_lemma)
View(lost_lemma)
View(df_incllemma)
setwd("~/Documents/Research_Assistant_Rgit/veni_sysrev/amy")
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
clean_df <- df_check %>%
anti_join(stop_words, by= c("lemma" = "word"))
# check number of unique words after removing stop words
length(unique(clean_df$lemma))
# double check if the word "and" is removed from the file
clean_df[clean_df$lemma == "and", c("token", "lemma", "doc_id")]
# double check if the word "and" is removed from the file
df_check[df_check$lemma == "and", c("token", "lemma", "doc_id")]
# double check if the word "and" is removed from the file
df_check[df_check$token == "and", c("token", "lemma", "doc_id")]
# double check if the word "and" is removed from the file
df_check[df_check$token == "the", c("token", "lemma", "doc_id")]
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# check number of unique words before applying more filters
length(unique(df_check$lemma))
View(df_check)
df_check <- df_check
# load the manuscript pre-processed file (without textrank algorithm and dict filter having been applied)
df_check <- readRDS("/Users/amyvanderham/Documents/Research_Assistant_Rgit/veni_sysrev/study2_df_kw.RData")
# REMOVING STOPWORDS.
# use stopwords from tidytext.
library(tidyverse)
library(tidytext)
clean_df <- df_check %>%
anti_join(stop_words, by= c("lemma" = "word"))
# 15274
df_check <- df_check
View(clean_df)
View(df_check)
worcs::git_update()
