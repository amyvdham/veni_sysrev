# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
# check the number of author keywords per doc
df_n_keywords <- df %>%
group_by(doc) %>%
summarise(n_keywords = n()) # snap nog niet helemaal hoe deze code weet dat die n() dan betrekking heeft op de column word maar gaat wel goed.
# Now that the dataset is cleaned created a df in which each document is a row and the column DEclean contains the author keywords of the specific document. Shape the tokens back to their original form
df_new <- df %>%
group_by(word) %>%
group_by(doc) %>%
summarise(DEclean = str_c(word, collapse = " "))
# second attempt word count
df_new$DEclean_length2 <- sapply(df_new$DEclean, function(x){str_count(x, " ")+1})
View(df_new)
df_new[doc == 3667]
df_new[df_new$doc == 3667]
df_new[df_new$doc == 3667, ]
df_new[df_new$doc == 3667, 2]
df[df$doc == 3667, 'DE']
recs[recs$doc == 3667, 'DE']
# Loading script containing functions
source("word_functions.R")
# prepare data
# this is were line 891 in manuscript file starts
recs <- read.csv("recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]
## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column.
df <- merge_df(recs, df, "word")
# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
# check the number of author keywords per doc
df_n_keywords <- df %>%
group_by(doc) %>%
summarise(n_keywords = n()) # snap nog niet helemaal hoe deze code weet dat die n() dan betrekking heeft op de column word maar gaat wel goed.
# Now that the dataset is cleaned created a df in which each document is a row and the column DEclean contains the author keywords of the specific document. Shape the tokens back to their original form
df_new <- df %>%
group_by(word) %>%
group_by(doc) %>%
summarise(DEclean = str_c(word, collapse = " "))
# second attempt word count
df_new$DEclean_length <- sapply(df_new$DEclean, function(x){str_count(x, " ")+1})
summary(df_new)
#
df_new <- data.table(df_new)
df_new[df_new$doc == 3667, 'DEclean']
# check if this is a result of data cleaning by looking at the original dataset
recs[recs$doc == 3667, 'DE']
# check if this is a result of data cleaning by looking at the original dataset
recs[recs$doc == 3667, 'DE']
recs[recs$doc == 1186, 'DE']
recs[recs$doc == 5235, 'DE']
recs[recs$doc == 5234, 'DE']
df_new[df_new$doc == 5234, 'DEclean']
# Vectorize the tokens, each token will receive a unique integer, the index of that token in a dictionary.
# Remember, we already restricted the corpus to 37.520 unique words.
tokenizer_w2v <- text_tokenizer() %>% fit_text_tokenizer(df_new)
# load required libraries used in the tutorial
library(tidyverse)
library(tidytext)
library(keras)
library(uwot)
# maximum number of words for a review
max_length <- 524
# Vectorize the tokens, each token will receive a unique integer, the index of that token in a dictionary.
# Remember, we already restricted the corpus to 37.520 unique words.
tokenizer_w2v <- text_tokenizer() %>% fit_text_tokenizer(df_new)
# and put these integers into a sequence
sequences_w2v <- texts_to_sequences(tokenizer_w2v, df_new)
worcs::git_update()
install.packages("tensorflow")
# Loading required libraries
library(stringr)
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(Matrix)
library(ggplot2)
library(dplyr)
# Required library for GloVe
library(text2vec)
# Loading script containing functions
source("word_functions.R")
# prepare data
# this is were line 891 in manuscript file starts
recs <- read.csv("recs_final.csv")
# convert object into data table
recs <- as.data.table(recs)
# add column called doc that contains the row number.
recs[, "doc" := 1:nrow(recs)]
## Extract individual words
# create list with (separated) author keywords of each article
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
# merge_df is a function from word_function.R. In short, the function unlist() is applied to the list object that has been put into the function (in this case df) and adds the output/result of this to the new column.
df <- merge_df(recs, df, "word")
# make sure that the values in the column word do not contain any capitals.
df[, word := tolower(word)]
## Clean
# delete all the rows that contain a missing value in the column word
df <- na.omit(df, cols = "word")
## Exclude words
# create object with the terms that should be excluded
exclude_terms <- readLines("exclude_terms.txt")
# object with all the row numbers of author keywords that should be excluded from the data frame
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
# create new data frame that excludes all the row numbers that have an author keyword that should be excluded
df <- df[!exclude_these, ]
# check the number of author keywords per doc
df_n_keywords <- df %>%
group_by(doc) %>%
summarise(n_keywords = n()) # snap nog niet helemaal hoe deze code weet dat die n() dan betrekking heeft op de column word maar gaat wel goed.
## NOTE: some keywords contain more than one word, e.g. risk factors or follow-up studies and youth smoking behavior. So to now the lenght (number of words) per doc this won't work
# Now that the dataset is cleaned created a df in which each document is a row and the column DEclean contains the author keywords of the specific document. Shape the tokens back to their original form
df_new <- df %>%
group_by(word) %>%
group_by(doc) %>%
summarise(DEclean = str_c(word, collapse = " "))
# second attempt word count
df_new$DEclean_length <- sapply(df_new$DEclean, function(x){str_count(x, " ")+1})
# this gives us the number of words in the column DEclean for each row. NOTE: words that are connected by a - are seen as one word, e.g. follow-up or affect-regulatory. Also, abbreviations are also counted as a word, e.g. INFORMATION AND COMMUNICATION TECHNOLOGY (ICT). (ICT) is counted in this example.
df_new <- data.table(df_new)
# Max length of author keywords
max(df_new$DEclean_length)
# investigate with the highest value of number of author keywords
df_new[df_new$doc == 3667, 'DEclean']
# check if this is a result of data cleaning by looking at the original data set
recs[recs$doc == 3667, 'DE']
# -> shows that this is not a result of data cleaning and it seems that this article does really have 524 author keywords.
# NOTE: document/case/row shown below contains < in author keywords. Does this need to be cleaned?
df_new[df_new$doc == 5234, 'DEclean']
# load required libraries used in the tutorial
library(tidyverse)
library(tidytext)
library(keras)
library(uwot)
# maximum number of words for a review
max_length <- 524
# Vectorize the tokens, each token will receive a unique integer, the index of that token in a dictionary.
# Remember, we already restricted the corpus to 37.520 unique words.
tokenizer_w2v <- text_tokenizer() %>% fit_text_tokenizer(df_new)
library("papaja")
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(knitr)
library(kableExtra)
library(cowplot)
run_everything <- TRUE
notingraph <- list()
keyw <- function(x){
x
}
cat_shapes <- c(Outcome = "square2", Indicator = "circle2", Cause = "quad2", Protective = "tri2")
legend_shapes <- c("circle2" = 21, "square2" = 22, "quad2" = 23, "tri2" = 24)[cat_shapes]
scale_shapes <- legend_shapes
names(scale_shapes) <- names(cat_shapes)[match(names(legend_shapes), cat_shapes)]
library(stringr)
library(udpipe)
library(igraph)
library(wordcloud)
library(Matrix)
library(ggplot2)
library(yaml)
source("word_functions.R")
source("circle2.R")
recs <- read.csv("recs_final.csv")
recs <- as.data.table(recs)
recs[, "doc" := 1:nrow(recs)]
study1details <- list(dim_recs = dim(recs))
# Extract individual words
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
df <- merge_df(recs, df, "word")
df[, word := tolower(word)]
# Clean
df <- na.omit(df, cols = "word")
number_docs_words <- c(docs = length(unique(df$doc)), words = length(unique(df$word)))
yaml::write_yaml(number_docs_words, "study1_number_docs_words.txt")
# Exclude words
exclude_terms <- readLines("exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
df <- df[!exclude_these, ]
# Categorize words
res_cat <- cat_words(df$word, dict, handle_dups = "all")
# Check coding issues
#res_cat$dup
#head(res_cat$unmatched)
df <- merge_df(df, res_cat$words, "word_coded")
saveRDS(df, "study1_df.RData")
## Look at POS tags?
if(run_everything){
recs <- read.csv("recs_final.csv")
recs <- as.data.table(recs)
if(!is.data.table(recs)){
browser()
}
recs[, "doc" := 1:nrow(recs)]
study1details <- list(dim_recs = dim(recs))
# Extract individual words
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
df <- merge_df(recs, df, "word")
df[, word := tolower(word)]
# Clean
df <- na.omit(df, cols = "word")
number_docs_words <- c(docs = length(unique(df$doc)), words = length(unique(df$word)))
yaml::write_yaml(number_docs_words, "study1_number_docs_words.txt")
# Exclude words
exclude_terms <- readLines("exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
df <- df[!exclude_these, ]
# Categorize words
res_cat <- cat_words(df$word, dict, handle_dups = "all")
# Check coding issues
#res_cat$dup
#head(res_cat$unmatched)
df <- merge_df(df, res_cat$words, "word_coded")
saveRDS(df, "study1_df.RData")
} else {
df <- readRDS("study1_df.RData")
number_docs_words <- yaml::read_yaml("study1_number_docs_words.txt")
}
## Look at POS tags?
if(run_everything){
recs <- read.csv("recs_final.csv")
recs <- as.data.table(recs)
if(!is.data.table(recs)){
browser()
}
recs[, "doc" := 1:nrow(recs)]
study1details <- list(dim_recs = dim(recs))
# Extract individual words
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
df <- merge_df(recs, df, "word")
df[, word := tolower(word)]
# Clean
df <- na.omit(df, cols = "word")
number_docs_words <- c(docs = length(unique(df$doc)), words = length(unique(df$word)))
yaml::write_yaml(number_docs_words, "study1_number_docs_words.txt")
# Exclude words
exclude_terms <- readLines("exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
df <- df[!exclude_these, ]
# Categorize words
res_cat <- cat_words(df$word, dict, handle_dups = "all")
# Check coding issues
#res_cat$dup
#head(res_cat$unmatched)
df <- merge_df(df, res_cat$words, "word_coded")
saveRDS(df, "study1_df.RData")
} else {
df <- readRDS("study1_df.RData")
number_docs_words <- yaml::read_yaml("study1_number_docs_words.txt")
}
# Frequency of word by doc
nounbydoc <- df[, list(freq = .N), by = list(doc_id = doc, term = word_coded)]
View(nounbydoc)
# Set frequency to 1; we're not interpreting word frequency, only occurrence
nounbydoc$freq <- 1
View(nounbydoc)
## Word frequencies
topterms <- colSums(dtm_top)
dtm <- udpipe::document_term_matrix(document_term_frequencies(nounbydoc))
term_freq <- table(colSums(dtm))
set.seed(5348)
dtm_top <- dtm[, select_words(dtm, .975)]
dtm_top <- dtm_top[rowSums(dtm_top) > 0, ]
if(run_everything){
term_freqtop <- table(colSums(dtm_top))
term_freq <- as.data.frame.table(term_freq)
term_freq$pruned <- term_freq$Var1 %in% names(term_freqtop)
write_yaml(term_freq, "study1_term_freq_dist.yml")
write_yaml(dim(dtm_top), "study1_dtm_top.yml")
# Wordcloud ---------------------------------------------------------------
## Word frequencies
topterms <- colSums(dtm_top)
baseline <- readRDS("baseline.RData")
word_freq <- data.frame(Word = names(topterms), Frequency = topterms, row.names = NULL)
write.csv(word_freq, "study1_word_freq.csv", row.names = FALSE)
df_plot <- word_freq
df_plot <- df_plot[order(df_plot$Frequency, decreasing = TRUE), ]
categ <- read.csv("study1_categorization.csv", stringsAsFactors = FALSE)
df_plot$cat <- categ$category[match(df_plot$Word, categ$name)]
df_plot$baseline <- as.character(df_plot$Word %in% baseline)
df_plot$faded <- df_plot$Word %in% baseline
# Tag words that are not in the cooccurrence graph
in_graph <- row.names(read.csv("s1_cooc.csv", row.names = 1))
notingraph <- !df_plot$Word %in% in_graph
df_plot$notingraph <- notingraph
italic_labels <- as.character(df_plot$Word)
italic_labels[notingraph] <- sapply(italic_labels[notingraph], function(x){
parse(text = paste0("italic('", x, "')"))
})
# Prettify words
df_plot$Word <- pretty_words(df_plot$Word)
df_plot$Word <- ordered(df_plot$Word, levels = df_plot$Word[order(df_plot$Frequency)])
cat_cols <- c(Outcome = "gray55", Indicator = "tomato", Cause = "darkgoldenrod3", Protective = "olivedrab4")
df_plot$cat <- ordered(df_plot$cat, levels = c("Outcome", "Indicator", "Cause", "Protective"))
write_yaml(df_plot$Word, "s1_words.yml")
p <- ggplot(df_plot, aes(y = Word, x = Frequency)) +
geom_segment(aes(x = 0, xend = Frequency,
y = Word, yend = Word, linetype = notingraph), colour = "grey50"
) +
geom_vline(xintercept = 0, colour = "grey50", linetype = 1) + xlab("Word frequency") +
geom_point(data = df_plot[df_plot$faded, ], aes(colour = cat, shape = cat), fill = "white", size = 1.5) +
geom_point(data = df_plot[!df_plot$faded, ], aes(colour = cat, fill = cat, shape = cat), size = 1.5) +
scale_colour_manual(values = c(Outcome = "gray55", Indicator = "tomato", Cause = "darkgoldenrod3", Protective = "olivedrab4"))+ #, guide = NULL
scale_shape_manual(values = scale_shapes)+#, guide = NULL
scale_fill_manual(values = c(Outcome = "gray55", Indicator = "tomato", Cause = "darkgoldenrod3", Protective = "olivedrab4")) +
scale_x_log10() +
#scale_y_discrete(labels = italic_labels) +
scale_linetype_manual(values = c("TRUE" = 3, "FALSE" = 1), guide = NULL) +
theme_bw() + theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(), axis.title.y = element_blank(),
legend.position = c(.70,.125),
legend.title = element_blank(),
axis.text.y = element_text(hjust=0, vjust = 0, size = 6))
saveRDS(p, "s1_varimp.RData")
svg("s1_varimp.svg", width = 7/2.54, height = 14/2.54)
eval(p)
dev.off()
ggsave("s1_varimp.png", p, device = "png", width = 7, height = 14, units = "cm")
p1 <- p + theme(legend.position = "none")
df_plot$Frequency <- sqrt(df_plot$Frequency)
## Visualise them with wordclouds
p <- quote({
set.seed(46)
wordcloud(words = df_plot$Word, freq = df_plot$Frequency, scale = c(2,.4), max.words = 150, rot.per = 0,  random.order = FALSE, colors = brewer.pal(8, "Dark2"))
})
svg("study1_wordcloud.svg")
eval(p)
dev.off()
png("study1_wordcloud.png")
eval(p)
dev.off()
}
## Word frequencies
topterms <- colSums(dtm_top)
baseline <- readRDS("baseline.RData")
word_freq <- data.frame(Word = names(topterms), Frequency = topterms, row.names = NULL)
View(word_freq)
write.csv(word_freq, "study1_word_freq.csv", row.names = FALSE)
df_plot <- word_freq
df_plot <- df_plot[order(df_plot$Frequency, decreasing = TRUE), ]
categ <- read.csv("study1_categorization.csv", stringsAsFactors = FALSE)
df_plot$cat <- categ$category[match(df_plot$Word, categ$name)]
df_plot$baseline <- as.character(df_plot$Word %in% baseline)
worcs::git_update(message = "running manuscript")
renv::status()
renv::snapshot()
renv::status()
library("papaja")
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(knitr)
library(kableExtra)
library(cowplot)
run_everything <- TRUE
notingraph <- list()
keyw <- function(x){
x
}
cat_shapes <- c(Outcome = "square2", Indicator = "circle2", Cause = "quad2", Protective = "tri2")
legend_shapes <- c("circle2" = 21, "square2" = 22, "quad2" = 23, "tri2" = 24)[cat_shapes]
scale_shapes <- legend_shapes
names(scale_shapes) <- names(cat_shapes)[match(names(legend_shapes), cat_shapes)]
library(stringr)
library(udpipe)
library(igraph)
library(wordcloud)
library(Matrix)
library(ggplot2)
library(yaml)
source("word_functions.R")
source("circle2.R")
#run_everything = FALSE
study1details <- read_yaml("study1_details.yml")
dict <- read_yaml("yaml_dict.txt")
## Look at POS tags?
if(run_everything){
recs <- read.csv("recs_final.csv")
recs <- as.data.table(recs)
if(!is.data.table(recs)){
browser()
}
recs[, "doc" := 1:nrow(recs)]
study1details <- list(dim_recs = dim(recs))
# Extract individual words
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
df <- merge_df(recs, df, "word")
df[, word := tolower(word)]
# Clean
df <- na.omit(df, cols = "word")
number_docs_words <- c(docs = length(unique(df$doc)), words = length(unique(df$word)))
yaml::write_yaml(number_docs_words, "study1_number_docs_words.txt")
# Exclude words
exclude_terms <- readLines("exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
df <- df[!exclude_these, ]
# Categorize words
res_cat <- cat_words(df$word, dict, handle_dups = "all")
# Check coding issues
#res_cat$dup
#head(res_cat$unmatched)
df <- merge_df(df, res_cat$words, "word_coded")
saveRDS(df, "study1_df.RData")
} else {
df <- readRDS("study1_df.RData")
number_docs_words <- yaml::read_yaml("study1_number_docs_words.txt")
}
# Frequency of word by doc
nounbydoc <- df[, list(freq = .N), by = list(doc_id = doc, term = word_coded)]
# Set frequency to 1; we're not interpreting word frequency, only occurrence
nounbydoc$freq <- 1
dtm <- udpipe::document_term_matrix(document_term_frequencies(nounbydoc))
term_freq <- table(colSums(dtm))
# Continue plotting word frequency ----------------------------------------
set.seed(5348)
dtm_top <- dtm[, select_words(dtm, .975)]
dtm_top <- dtm_top[rowSums(dtm_top) > 0, ]
if(run_everything){
term_freqtop <- table(colSums(dtm_top))
term_freq <- as.data.frame.table(term_freq)
term_freq$pruned <- term_freq$Var1 %in% names(term_freqtop)
write_yaml(term_freq, "study1_term_freq_dist.yml")
write_yaml(dim(dtm_top), "study1_dtm_top.yml")
# Wordcloud ---------------------------------------------------------------
## Word frequencies
topterms <- colSums(dtm_top)
baseline <- readRDS("baseline.RData")
word_freq <- data.frame(Word = names(topterms), Frequency = topterms, row.names = NULL)
write.csv(word_freq, "study1_word_freq.csv", row.names = FALSE)
df_plot <- word_freq
df_plot <- df_plot[order(df_plot$Frequency, decreasing = TRUE), ]
categ <- read.csv("study1_categorization.csv", stringsAsFactors = FALSE)
df_plot$cat <- categ$category[match(df_plot$Word, categ$name)]
df_plot$baseline <- as.character(df_plot$Word %in% baseline)
df_plot$faded <- df_plot$Word %in% baseline
# Tag words that are not in the cooccurrence graph
in_graph <- row.names(read.csv("s1_cooc.csv", row.names = 1))
notingraph <- !df_plot$Word %in% in_graph
df_plot$notingraph <- notingraph
italic_labels <- as.character(df_plot$Word)
italic_labels[notingraph] <- sapply(italic_labels[notingraph], function(x){
parse(text = paste0("italic('", x, "')"))
})
# Prettify words
df_plot$Word <- pretty_words(df_plot$Word)
df_plot$Word <- ordered(df_plot$Word, levels = df_plot$Word[order(df_plot$Frequency)])
cat_cols <- c(Outcome = "gray55", Indicator = "tomato", Cause = "darkgoldenrod3", Protective = "olivedrab4")
df_plot$cat <- ordered(df_plot$cat, levels = c("Outcome", "Indicator", "Cause", "Protective"))
write_yaml(df_plot$Word, "s1_words.yml")
p <- ggplot(df_plot, aes(y = Word, x = Frequency)) +
geom_segment(aes(x = 0, xend = Frequency,
y = Word, yend = Word, linetype = notingraph), colour = "grey50"
) +
geom_vline(xintercept = 0, colour = "grey50", linetype = 1) + xlab("Word frequency") +
geom_point(data = df_plot[df_plot$faded, ], aes(colour = cat, shape = cat), fill = "white", size = 1.5) +
geom_point(data = df_plot[!df_plot$faded, ], aes(colour = cat, fill = cat, shape = cat), size = 1.5) +
scale_colour_manual(values = c(Outcome = "gray55", Indicator = "tomato", Cause = "darkgoldenrod3", Protective = "olivedrab4"))+ #, guide = NULL
scale_shape_manual(values = scale_shapes)+#, guide = NULL
scale_fill_manual(values = c(Outcome = "gray55", Indicator = "tomato", Cause = "darkgoldenrod3", Protective = "olivedrab4")) +
scale_x_log10() +
#scale_y_discrete(labels = italic_labels) +
scale_linetype_manual(values = c("TRUE" = 3, "FALSE" = 1), guide = NULL) +
theme_bw() + theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(), axis.title.y = element_blank(),
legend.position = c(.70,.125),
legend.title = element_blank(),
axis.text.y = element_text(hjust=0, vjust = 0, size = 6))
saveRDS(p, "s1_varimp.RData")
svg("s1_varimp.svg", width = 7/2.54, height = 14/2.54)
eval(p)
dev.off()
ggsave("s1_varimp.png", p, device = "png", width = 7, height = 14, units = "cm")
p1 <- p + theme(legend.position = "none")
df_plot$Frequency <- sqrt(df_plot$Frequency)
## Visualise them with wordclouds
p <- quote({
set.seed(46)
wordcloud(words = df_plot$Word, freq = df_plot$Frequency, scale = c(2,.4), max.words = 150, rot.per = 0,  random.order = FALSE, colors = brewer.pal(8, "Dark2"))
})
svg("study1_wordcloud.svg")
eval(p)
dev.off()
png("study1_wordcloud.png")
eval(p)
dev.off()
}
View(word_freq)
View(term_freq)
View(nounbydoc)
worcs::git_update(message = "running manuscript word_freq up to date")
