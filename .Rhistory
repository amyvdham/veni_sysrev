dup_doi <- duplicated(recs$doi)
recs <- recs[!dup_doi, ]
}
recs$title <- recs$TI
potential_dups <- find_duplicates(recs, match_variable = "title", match_function = "stringdist", threshold = 5, to_lower = TRUE, remove_punctuation = TRUE)
unique_recs <- extract_unique_references(recs, matches = potential_dups)
sum(dup_doi)
(nrow(recs)-nrow(unique_recs))-sum(dup_doi))
(nrow(recs)-nrow(unique_recs))-sum(dup_doi)
dups[["dup_title"]]
6651-6595
dups[["dup_doi"]]
dups[["dup_title"]]
tmp <- read.csv("rayyan.csv")
tmp <- read.csv("rayyan_exports/articles.csv")
table(tmp$notes)
rayyan_res <- read.csv("rayyan_exports/articles.csv")
table(rayyan_res$notes)
install.packages("PRISMAstatement")
library(PRISMAstatement)
all(diff(rayyan_res$key) == 1)
nrow(df_screen) - nrow(rayyan_res)
nrow(unique_recs) - nrow(rayyan_res)
screen$record_id[!screen$record_id %in% rayyan_res$key]
names(screen
)
head(screen[, c("key", "record_id")])
head(screen[, c("key", "record_id")])
tail(screen[, c("key", "record_id")])
screen[!screen$key %in% rayyan_res$key, c("included")]
screen <- screen[screen$key %in% rayyan_res$key, ]
screen$included
screen$abstract[min(which(screen$included == 0))-1]
screen$abstract[min(which(screen$included == 0))-2]
min(which(screen$included == 0))-2
min(which(screen$included == 0))
screen$abstract
table(screen$included)
table(screen$included, screen$excluded_rayyan)
table(screen$included, screen$excluded_rayyan)
table(screen$included, screen$rayyan)
367+192
library(cowplot)
renv::install(cowplot, rebuild = T)
renv::install("cowplot", rebuild = T)
library(cowplot)
fig_svg <- cowplot::ggdraw()+cowplot::draw_image("study1_network1.svg")
plot(fig_svg)
fig_svg <- cowplot::ggdraw()+cowplot::draw_image("study1_network1.svg")
plot(fig_svg)
install.packages("magick")
fig_svg <- cowplot::ggdraw()+cowplot::draw_image("study1_network1.svg")
plot(fig_svg)
names(topterms)[!names(topterms) %in% unique(c(edg$term1, edg$term2)]
names(topterms)[!names(topterms) %in% unique(c(edg$term1, edg$term2))]
edg <- df_plot
names(topterms)[!names(topterms) %in% unique(c(edg$term1, edg$term2))]
names(topterms)
library("papaja")
library(bibliometrix)
library(revtools)
library(bib2df)
library(knitr)
library(kableExtra)
run_everything = FALSE
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
filenam <- "recs_6653.RData"
if(!file.exists(filenam)){
f <- list.files("./recs/18-3-2020/", full.names = TRUE)
f <- readFiles(f)
recs <- convert2df(f)
saveRDS(recs, filenam)
} else {
recs <- readRDS(filenam)
}
filenam <- "unique_recs.RData"
if(!file.exists(filenam)){
recs$doi <- recs$DI
recs$doi[is.na(recs$doi)] <- paste0("fake", 1:sum(is.na(recs$doi)))
if(any(duplicated(recs$doi))){
recs$doi[duplicated(recs$doi)|duplicated(recs$doi, fromLast = T)]
dup_doi <- duplicated(recs$doi)
recs <- recs[!dup_doi, ]
}
recs$title <- recs$TI
# locate and extract unique references
potential_dups <- find_duplicates(recs, match_variable = "title", match_function = "stringdist", threshold = 5, to_lower = TRUE, remove_punctuation = TRUE)
unique_recs <- extract_unique_references(recs, matches = potential_dups)
saveRDS(unique_recs, filenam)
write.csv(data.frame(dup_doi = sum(dup_doi), dup_title = (nrow(recs)-nrow(unique_recs))-sum(dup_doi)), "dups.csv", row.names = FALSE)
} else {
unique_recs <- readRDS(filenam)
dups <- read.csv("dups.csv")
}
unique_recs$id_num <- 1:nrow(unique_recs)
if(FALSE){
# df_screen <- unique_recs[, c("id_num", "TI", "AB")]
# names(df_screen) <- c("id_num", "title", "abstract")
# write.csv(df_screen, file = "asreview.csv", row.names = FALSE, fileEncoding = "UTF-8")
df_screen <- unique_recs[, c("id_num", "TI", "AU", "SO", "SN", "VL", "IS", "BP", "PY", "PU", "DI", "AB")]
names(df_screen) <- c("key", "title", "authors", "journal", "issn", "volume", "issue", "pages", "year", "publisher", "url", "abstract")
write.csv(df_screen, file = "rayyan.csv", row.names = FALSE)
}
rayyan_res <- read.csv("rayyan_exports/articles.csv")
rayyan_dups <- nrow(unique_recs) - nrow(rayyan_res)
screen <- read.csv("asreview_result_sysrevemotprob.csv")
screen <- screen[screen$key %in% rayyan_res$key, ]
screen$drop_these <- FALSE
screen$drop_these[which(screen$included == 1 | screen$rayyan == "False")] <- TRUE
if(!file.exists("recs_final.csv")){
recs_final <- unique_recs[screen$key[!screen$drop_these], ]
write.csv(recs_final, "recs_final.csv", row.names = FALSE)
} else {
recs_final <- read.csv("recs_final.csv")
}
library(tidySEM)
lo <- matrix(c("start", "dedup", "rayyan", "asreview", "included",
"", "dedup2", "excluded", "", ""), nrow = 5, ncol = 2)
nodes <- data.frame(
name = c("start", "dedup", "dedup2", "rayyan", "asreview", "included", "excluded"),
label = c(
paste0("Records identified through\nWeb of Science (n = ", nrow(recs), ")"),
paste0("After duplicates removed\n(n = ", nrow(unique_recs), ")"),
paste0("Duplicates\n(n = ", nrow(recs)-nrow(screen), ")"),
paste0("Articles screened for eligibility\nusing Rayyan (n = ", nrow(screen), ")"),
paste0("Articles screened for eligibility\nusing ASReview (n = ", nrow(screen), ")"),
paste0("Studies included in\nqualitative synthesis (n = ", nrow(recs_final), ")"),
paste0("Excluded studies (n = ", nrow(screen)-nrow(recs_final), ")"))
)
edges <- data.frame(
from = c("start", "dedup", "rayyan", "asreview",
"start", "rayyan", "rayyan", "asreview"),
to = c("dedup", "rayyan", "asreview", "included",
"dedup2", "dedup2", "excluded", "excluded"),
label = c("",   "", paste0("n = ", sum(screen$rayyan == "True")), paste0("n = ", sum(screen$included == 0, na.rm = TRUE)-sum(screen$rayyan == "True")),
paste0("n = ", sum(dups)),
paste0("n = ", rayyan_dups),
paste0("n = ", sum(screen$rayyan == "False")),
paste0("n = ", sum(screen$included == 1, na.rm = TRUE)-sum(screen$rayyan == "False"))
)
)
p <- graph_sem(layout = lo, nodes = nodes, edges = edges, angle = 1, rect_height = 1.2, rect_width = 1.3)
ggsave("prismachart.png", p, device = "png")
ggsave("prismachart.svg", p, device = "svg")
knitr::include_graphics("prismachart.png")
source("study1.R")
if(run_everything | !file.exists("Study1_lda_dims.txt")) source("LDA_analysis.R")
lda_dims <- read_yaml("Study1_lda_dims.txt")
knitr::include_graphics("study1_wordcloud.svg")
#knitr::include_graphics("study1_network1.svg")
library(cowplot)
fig_svg <- cowplot::ggdraw()+cowplot::draw_image("study1_network1.svg")
plot(fig_svg)
names(topterms)[!names(topterms) %in% unique(c(edg$term1, edg$term2))]
notingraph <- names(topterms)[!names(topterms) %in% unique(c(edg$term1, edg$term2))]
df <- categ[categ$name %in% notingraph, ]
notingraph <- names(topterms)[!names(topterms) %in% unique(c(edg$term1, edg$term2))]
notingraph <- categ[categ$name %in% notingraph, ]
notingraph <- tapply(pretty_words(notingraph$name), notingraph$category, function(i){paste(paste0("*", i, "*"), collapse = ", ")})
notingraph
notingraph["Cause"]
install.packages("pattern.nlp")
renv::install("bnosac/pattern.nlp")
library(pattern.nlp)
renv::install("bnosac/pattern.nlp", rebuild = TRUE)
install.packages("PythonInR")
renv::install("bnosac/pattern.nlp", rebuild = TRUE)
install.packages("pack")
renv::install("bnosac/pattern.nlp", rebuild = TRUE)
renv::install("pack", rebuild = TRUE)
renv::install("bnosac/pattern.nlp", rebuild = TRUE)
renv::install("findpython", rebuild = TRUE)
renv::install("bnosac/pattern.nlp", rebuild = TRUE)
library(pattern.nlp)
library(data.table)
#library(bibliometrix)
#library(yaml)
library(stringr)
#library(lattice)
#library(topicmodels)
library(udpipe)
library(igraph)
library(wordcloud)
library(Matrix)
library(yaml)
source("word_functions.R")
## Look at POS tags?
recs <- data.table(read.csv("recs_final.csv"))
recs[, "doc" := 1:nrow(recs)]
count.matches <- function(pat, vec) sapply(regmatches(vec, gregexpr(pat, vec)), length)
df <- read.table("c:/tmp/savedrecs.txt", sep = "\t", header = TRUE, quote = "", row.names = NULL, stringsAsFactors = FALSE, fileEncoding = "UTF-16")
recs[1, ]$AB
pattern_pos(x = recs[1, ]$AB, language = "english", core = TRUE)
library(data.table)
#library(bibliometrix)
#library(yaml)
library(stringr)
#library(lattice)
#library(topicmodels)
library(udpipe)
library(igraph)
library(wordcloud)
library(Matrix)
library(yaml)
source("word_functions.R")
## Look at POS tags?
recs <- data.table(read.csv("recs_final.csv"))
recs[, "doc" := 1:nrow(recs)]
# Try quanteda
#library(quanteda)
#corp <- corpus(recs, text_field = "DE")  # build a new corpus from the texts
# Try kwic
#kwic(corp, pattern = "drug")
# Extract individual words
df <- lapply(recs$DE, function(x){strsplit(x, split = "; ")[[1]]})
df <- merge_df(recs, df, "word")
df[, word := tolower(word)]
# Clean
df <- na.omit(df, cols = "word")
number_docs_words <- c(docs = length(unique(df$doc)), words = length(unique(df$word)))
# Exclude words
exclude_terms <- readLines("exclude_terms.txt")
exclude_these <- unique(unlist(lapply(exclude_terms, grep, x = df$word)))
df <- df[!exclude_these, ]
# Categorize words
dict <- read_yaml("yaml_dict.txt")
res_cat <- cat_words(df$word, dict, handle_dups = "all")
# Check coding issues
#res_cat$dup
#head(res_cat$unmatched)
df <- merge_df(df, res_cat$words, "word_coded")
length(unique(df$word))
length(unique(df$doc))
# Create plot data --------------------------------------------------------
nounbydoc <- df[, list(freq = .N), by = list(doc_id = doc, term = word)]
nounbydoc$freq <- 1
dtm <- udpipe::document_term_matrix(document_term_frequencies(nounbydoc))
tt <- as.data.frame(table(colSums(dtm)), stringsAsFactors = FALSE)
tt$Frequency <- as.numeric(tt$Var1)
tt$Words <- tt$Freq
tt$Dictionary <- "Original"
df_plot <- tt[, c("Frequency", "Words", "Dictionary")]
rm(nounbydoc, dtm, tt)
# End plot data -----------------------------------------------------------
# Remove duplicate words per doc
#df[, id := paste0(word_coded, doc)]
#df <- df[!duplicated(id), ]
#df[, id := NULL]
#head(df)
# Frequency of word by doc
nounbydoc <- df[, list(freq = .N), by = list(doc_id = doc, term = word_coded)]
# Set frequency to 1; we're not interpreting word frequency, only occurrence
nounbydoc$freq <- 1
# tmp <- nounbydoc %>%
#   bind_tf_idf(term, doc_id, freq)
# summary(tmp$tf_idf)
dtm <- udpipe::document_term_matrix(document_term_frequencies(nounbydoc))
topterms <- colSums(dtm)
topterms <- sort(topterms, decreasing = TRUE)
# Continue plotting word frequency ----------------------------------------
tt <- as.data.frame(table(topterms), stringsAsFactors = FALSE)
tt$Frequency <- as.numeric(tt$topterms)
tt$Words <- tt$Freq
tt$Dictionary <- "Recoded"
df_plot <- rbind(df_plot, tt[, c("Frequency", "Words", "Dictionary")])
p <- ggplot(df_plot, aes(x=Frequency, y = Words, linetype = Dictionary))+geom_point() + geom_path()+
theme_bw() + scale_y_log10()+scale_x_log10()
ggsave("study1_word_frequency.png", p, device = "png")
# Select most common terms ------------------------------------------------
topterms <- topterms[topterms > .005*nrow(recs)]
which_topterms <- head(topterms, 250)
which_topterms <- names(which_topterms)
dtm_top <- dtm[, which_topterms]
dtm_top <- dtm_top[rowSums(dtm_top) > 0, ]
dim(dtm_top)
# Wordcloud ---------------------------------------------------------------
## Word frequencies
word_freq <- data.frame(Word = names(topterms), Frequency = topterms, row.names = NULL)
df_plot <- word_freq
df_plot$Word <- pretty_words(df_plot$Word)
## Visualise them with wordclouds
p <- quote({
set.seed(46)
wordcloud(words = df_plot$Word, freq = df_plot$Frequency, scale = c(5,1), max.words = 150, rot.per = 0,  random.order = FALSE, colors = brewer.pal(8, "Dark2"))
})
svg("study1_wordcloud.svg")
eval(p)
dev.off()
png("study1_wordcloud.png")
eval(p)
dev.off()
# Co-occurrence -----------------------------------------------------------
#word_cooccurences <- pair_count(df_nouns, group="article.id", value="word.lemma", sort = TRUE)
create_cooc <- function(dtm){
dtm_binary <- dtm > 0
Matrix::t(dtm_binary) %*% dtm
}
cooc <- create_cooc(dtm_top)
word_cooccurences <- as_cooccurrence(cooc)
word_cooccurences <- word_cooccurences[!word_cooccurences$term1 == word_cooccurences$term2, ]
word_cooccurences <- word_cooccurences[order(word_cooccurences$cooc, decreasing = TRUE), ]
plot(1:length(word_cooccurences$cooc), word_cooccurences$cooc)
df_plot <- word_cooccurences[word_cooccurences$cooc > .005*nrow(recs), ] #.01*nrow(recs), ]
df_plot$id <- apply(df_plot[, c("term1", "term2")], 1, function(x)paste0(sort(x), collapse = ""))
df_plot <- df_plot[!duplicated(df_plot$id), ]
#df_plot <- df_plot[!(df_plot$term1 == "youth" | df_plot$term2 == "youth"),]
# #word_cooccurences$cooc <- (word_cooccurences$cooc-min(word_cooccurences$cooc))+.1
# set.seed(123456789)
# df_plot %>%
#   graph_from_data_frame() %>%
#   ggraph(layout = "circle") +
#   geom_edge_link(mapping = aes(edge_colour = cooc, edge_width = cooc)) +
#   #geom_node_text(color = "lightblue", size = 5) +
#   geom_node_label(aes(label = name), col = "darkgreen") +
#   ggtitle(sprintf("\n%s", "CETA treaty\nCo-occurrence of nouns")) +
#   theme_void()
# Create network ----------------------------------------------------------
edg <- df_plot
# edge_cols <- read.csv("edge_cols.csv", stringsAsFactors = FALSE)
# if(any(!edg$id %in% edge_cols$id)){
#   stop("Please classify missing edges.")
# }
# edge_cols$color <- "gray70"
# edge_cols$color[which(edge_cols$association)] <- "navy"
# edge_cols$color[which(!edge_cols$association)] <- "darkred"
# edg$color <- edge_cols$color[match(edg$id, edge_cols$id)]
#edg$col <- sapply(edg$color, function(i){
#  do.call(rgb, c(as.list(col2rgb(i)), list(alpha = 178, maxColorValue = 255)))
#})
edg$width = edg$cooc
vert <- data.frame(name = names(topterms), label = pretty_words(names(topterms)), size = topterms)
vert <- vert[vert$name %in% unique(c(edg$term1, edg$term2)), ]
categ <- read.csv("study1_categorization.csv", stringsAsFactors = FALSE)
if(any(!vert$name %in% categ$name)){
write.table(vert$name[!vert$name %in% categ$name], "clipboard", sep = "\n", row.names = FALSE, col.names= FALSE)
stop("Please re-categorize missing vertices.")
}
vert$Category <- categ$category[match(vert$name, categ$name)]
cat_cols <- c(Outcome = "gray50", Indicator = "tomato", Cause = "gold", Protective = "forestgreen")
cat_cols <- c(Outcome = "gray50", Indicator = "tomato", Cause = "gold", Protective = "olivedrab2")
vert$color <- cat_cols[vert$Category]
#min_size <- (strwidth(vert$name[max(nchar(vert$name))]) + strwidth("oo")) * 100
vert$size <- scales::rescale(log(vert$size), c(4, 12))
g <- graph_from_data_frame(edg, vertices = vert,
directed = FALSE)
#min_size <- (strwidth(vert$label[max(nchar(vert$label))]) + strwidth("oo"))*100
# edge thickness
E(g)$width <- scales::rescale(sqrt(E(g)$width), to = c(2, 8))
edge.start <- ends(g, es=E(g), names = FALSE)[,1]
# Color edges based on origin:
#E(g)$color <- V(g)$color[edge.start]
E(g)$lty <- c(1, 5)[(!edge.start == 1)+1]
# Other layouts:
#l <- layout_with_lgl(g, root = which(V(g)$name == "dysregulation"))
#l <- layout_with_fr(g)
#l <- layout_(g,with_dh(weight.edge.lengths = edge_density(g)/1000))
set.seed(1) #4 #2 #3
l1 <- l <- layout_with_fr(g)
set.seed(64)
l2 <- layout_in_circle(g)
# lay <- data.frame(name = V(g)$name, l1)
# write.csv(lay, "study1_layout.csv", row.names = FALSE)
# p <- quote({
#   # Set margins to 0
#   par(mar=c(0,0,0,0))
#   plot(g, edge.curved = 0, layout=l,
#        vertex.label.family = "sans",
#        vertex.label.cex = 0.8,
#        vertex.shape = "circle",
#        vertex.frame.color = 'gray40',
#        vertex.label.color = 'black',      # Color of node names
#        vertex.label.font = 1,         # Font of node names
#   )
#   legend(x=-1.1, y=1.1, names(cat_cols), pch=21, col="#777777", pt.bg=cat_cols, pt.cex=2, cex=.8, bty="n", ncol=1)
# })
# png("study1_network1.png")
# eval(p)
# dev.off()
#
# svg("study1_network1.svg")
# eval(p)
# dev.off()
#V(g)$size <- scales::rescale(V(g)$size, c(4, 12))
p <- quote({
# Set margins to 0
par(mar=c(0,0,0,0),
mfrow=c(1,2))
plot(g, edge.curved = 0, layout=l1,
vertex.label.family = "sans",
vertex.label.cex = 0.8,
vertex.shape = "circle",
vertex.frame.color = 'gray40',
vertex.label.color = 'black',      # Color of node names
vertex.label.font = 1,         # Font of node names
)
legend(x=-1.1, y=1.1, names(cat_cols), pch=21, col="#777777", pt.bg=cat_cols, pt.cex=2, cex=.8, bty="n", ncol=1)
plot(g, edge.curved = 0, layout=l2,
vertex.label.family = "sans",
vertex.label.cex = 0.8,
vertex.shape = "circle",
vertex.frame.color = 'gray40',
vertex.label.color = 'black',      # Color of node names
vertex.label.font = 1,         # Font of node names
)
})
# Save files
png("study1_network1.png", width = 960)
eval(p)
dev.off()
svg("study1_network1.svg", width = 14)
eval(p)
dev.off()
#
# library(qgraph)
#
# igraph::
# library(Matrix)
# library(qgraph)
# terms <- predict(topics, type = "terms", min_posterior = 0.08)
# terms <- unique(unlist(sapply(terms, names)))
# out <- dtm_top
# out <- cor(as.matrix(out))
# out <- nearPD(x=out, corr = TRUE)$mat
# out <- as.matrix(out)
#
# cor_mat <- dtm_cor(dtm_top)
# m <- EBICglasso(cor_mat, n = 429, threshold = TRUE, returnAllResults = T)
# m$optnet
# tmp <- as.data.frame.table(m$optnet)
# tmp[!tmp$Freq == 0, ]
# qgraph(tmp, layout="spring", labels = colnames(out), label.scale=FALSE,
#        label.cex=1, node.width=.5)
#
#
# df <- readRDS("savedrecs.RData")
# bibliometrix::biblioNetwork(recs, analysis = "co-occurrences", network = "author_keywords")
#
# tmp <- cocMatrix(recs, Field = "DE")
#
# tmp %>%
#   as_cooccurrence() %>%
#   graph_from_data_frame() %>%
#   ggraph(layout = "fr") +
#   geom_edge_link(mapping = aes(edge_colour = cooc)) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_text(aes(label = name), vjust = 1.8, col = "darkgreen") +
#   ggtitle(sprintf("\n%s", "CETA treaty\nCo-occurrence of nouns")) +
#   theme_void()
#
# gD <- igraph::simplify(igraph::graph.data.frame(allgenes, directed=FALSE))
# lou <- cluster_louvain(gD)
topterms
as.data.frame.table(topterms)
as.data.frame.table(topterms)
class(topterms)
word_freq
write.csv(word_freq, "study1_word_freq.csv", row.names = FALSE)
library(udpipe)
ud_model <- udpipe_download_model(language = "english")
data(brussels_reviews)
install.packages("cld3")
library(cld3)
languages <- cld3::detect_language(recs$AB[1:3])
languages
languages <- cld3::detect_language(recs$AB)
table(languages)
recs$AB[languages == "ig"][1]
recs$AB[languages == "ig"]
recs$AB[languages == "es"][1]
ud_model <- udpipe_load_model(ud_model$file_model)
x <- udpipe_annotate(ud_model, x = recs$AB[1:10], doc_id = recs$doc)
x <- udpipe_annotate(ud_model, x = recs$AB[1:10], doc_id = recs$doc[1:10])
x <- as.data.frame(x)
View(x)
start_time <- Sys.time()
nlp_res <- pattern_pos(x = recs$AB[1:10], language = "english", core = TRUE)
end_time <- Sys.time()
nlp_tim <- end_time - start_time
start_time <- Sys.time()
udp_res <- udpipe_annotate(ud_model, x = recs$AB[1:10], doc_id = recs$doc[1:10])
udp_res <- as.data.frame(udp_res)
end_time <- Sys.time()
udp_tim <- end_time - start_time
nlp_tim
udp_tim
recs$AB[1:10]
nlp_res
nlp_res <- pattern_pos(x = recs$AB[1:5], language = "english", core = TRUE)
nlp_res <- pattern_pos(x = recs$AB[1], language = "english", core = TRUE)
nlp_res
udp_res <- udpipe_annotate(ud_model, x = recs$AB[1], doc_id = recs$doc[1:10])
udp_res <- udpipe_annotate(ud_model, x = recs$AB[1], doc_id = recs$doc[1])
start_time <- Sys.time()
nlp_res <- pattern_pos(x = recs$AB[1], language = "english", core = TRUE)
end_time <- Sys.time()
nlp_tim <- end_time - start_time
start_time <- Sys.time()
udp_res <- udpipe_annotate(ud_model, x = recs$AB[1], doc_id = recs$doc[1])
udp_res <- as.data.frame(udp_res)
end_time <- Sys.time()
udp_tim <- end_time - start_time
nlp_tim
udp_tim
View(nlp_res)
View(udp_res)
450/569
.7*569
.75*569
.17*7
.17*6
